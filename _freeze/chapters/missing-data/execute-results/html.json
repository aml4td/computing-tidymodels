{
  "hash": "2a3512ab70aa2a046ee5c0f49c8de240",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/missing-data/\"\n---\n\n# Missing Data {#sec-missing-data}\n\nThis chapter outlines how to work with missing data when building prediction models.  \n\n\n\nA general discussion on missing data in R can be found in [R for Data Science (2e)](https://r4ds.hadley.nz/missing-values). [The Missing Book](https://tmb.njtierney.com/) is an excellent reference that supplements this chapter. \n\nThe data will be taken from [_A morphometric modeling approach to distinguishing among bobcat, coyote, and gray fox scats_](https://doi.org/10.2981/wlb.00105). The data set is designed to see how well experts can determine which of three species (bobcats, coyotes, and gray foxes) can be identified by their ~~poop~~ feces (a.k.a. scat). There are physical measurements as well as some laboratory tests that can be used as predictors. The species is the outcome. The data are in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=modeldata\">modeldata</a></span> package. \n\n## Requirements\n\nYou’ll need 3 packages (<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=naniar\">naniar</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=ranger\">ranger</a></span>, and <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidymodels\">tidymodels</a></span>) for this chapter. \nYou can install them via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreq_pkg <- c(\"naniar\", \"ranger\", \"tidymodels\")\n\n# Check to see if they are installed: \npkg_installed <- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list <- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\n```\n:::\n\n\nLet's load the meta package and manage some between-package function conflicts. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())\n```\n:::\n\n\nThe data are automatically attached when the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidymodels\">tidymodels</a></span> package is loaded. The data frame is named `scat`. Let's split the data into training and testing, create some resamples (to be discussed in chapter TODO), and a data frame of predictor values. For both data splitting steps, we'll stratify by the species since the frequencies of each are not balanced. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(383)\nscat_split <- initial_split(scat, strata = Species)\nscat_tr <- training(scat_split)\nscat_te <- testing(scat_split)\n\nscat_rs <- vfold_cv(scat_tr, repeats = 5, strata = Species)\n\nscat_tr_preds <- scat_tr %>% select(-Species)\n```\n:::\n\n\nHere is the breakdown of the species per data partition: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nscat_tr %>% count(Species)\n#> # A tibble: 3 × 2\n#>   Species      n\n#>   <fct>    <int>\n#> 1 bobcat      42\n#> 2 coyote      21\n#> 3 gray_fox    18\nscat_te %>% count(Species)\n#> # A tibble: 3 × 2\n#>   Species      n\n#>   <fct>    <int>\n#> 1 bobcat      15\n#> 2 coyote       7\n#> 3 gray_fox     7\n```\n:::\n\n\nWe'll spend some time on visualization and summary techniques that are helpful when performing exploratory data analysis. \n\n## Investigating Missing Data {#sec-missing-eda}\n\nThe <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=naniar\">naniar</a></span> package is an excellent tool for missing data. Let's load it and then get a summary of our training set variables. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(naniar)\n\nmiss_var_summary(scat_tr_preds) %>% print(n = Inf)\n#> # A tibble: 18 × 3\n#>    variable  n_miss pct_miss\n#>    <chr>      <int>    <num>\n#>  1 Taper         14   17.28 \n#>  2 TI            14   17.28 \n#>  3 Diameter       4    4.938\n#>  4 d13C           2    2.469\n#>  5 d15N           2    2.469\n#>  6 CN             2    2.469\n#>  7 Mass           1    1.235\n#>  8 Month          0    0    \n#>  9 Year           0    0    \n#> 10 Site           0    0    \n#> 11 Location       0    0    \n#> 12 Age            0    0    \n#> 13 Number         0    0    \n#> 14 Length         0    0    \n#> 15 ropey          0    0    \n#> 16 segmented      0    0    \n#> 17 flat           0    0    \n#> 18 scrape         0    0\n```\n:::\n\n\nThe `miss_var_summary()` function summarizes the missing data in the predictors. The `n_miss` column is the number of missing values, and the `p_miss` column is the proportion of missing values. \n\nFor convenience, let’s make character vectors of column names for predictors with and without missing values. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmiss_cols <- c(\"Taper\", \"TI\", \"Diameter\", \"Mass\", \"d13C\", \"d15N\", \"CN\")\nnon_miss_cols <- c(\"Month\", \"Year\", \"Site\", \"Location\", \"Age\", \"Number\", \"Length\", \n                   \"ropey\", \"segmented\", \"flat\", \"scrape\")\n```\n:::\n\n\nWe can make an _upset plot_, which visualizes frequently occurring subsets in the high-dimensional Venn diagram where each predictor is encoded as missing/not missing: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(naniar)\ngg_miss_upset(scat_tr_preds, nsets = 10)\n```\n\n::: {.cell-output-display}\n![](../figures/upset-1.svg){fig-align='center' width=95%}\n:::\n:::\n\n\nFrom this, we might notice that there might be two different mechanisms causing missing data. First, the laboratory values for predictors (`d13C`, `d15N`, and `CN`) are only missing with one another. This suggests that some laboratory errors may be the cause of their missingness. The second set of predictors that are missing at once are all related to physical properties measured on-site. If the diameter and taper predictors cannot be ascertained, it might be because the scat sample might not have been... solid enough to measure. These assertions, if accurate, help us understand the type of missingness involved and, by extension, how to handle them. \n\nThe steps we would take to address missingness in the predictors is a preprocessing step; the tidymodels approach is to handle these in a recipes object. Recipes are more thoroughly introduced in @sec-recipe-intro. For now, we’ll show their usage and defer the broader information on how recipes work (and why you might want to use them) to the next chapter. \n\n<a href=\"https://aml4td.org/chapters/missing-data.html\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n## Filtering \n\nA recipe consists of an initialized object and a sequence of one or more \"steps\" that define specific actions/computations that should be done to the data prior to modeling. \n\nThe initialization consists of a call to `recipe::recipe()`. The most common interface used there is a formula. This declares which column is the outcome and which are predictors. For example: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nscat_rec <- recipe(Species ~ ., data = scat_tr)\n```\n:::\n\n\nAt this point, the recipe catalogs each column’s name, data type, and role (i.e., predictor or outcome). From there, we can add step functions to specify what should be done to what columns.  \n\nTwo recipes steps can be used for filtering missing data: `recipes::step_naomit()` and `recipes::step_filter_missing()`. The former removes rows of the training set, and the latter removes predictors if they have too many missing values. \n\n### Row Filtering {#sec-removing-missing-rows}\n\nLet's add `step_naomit()` to the recipe and declare the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=dplyr\">dplyr</a></span> selector `dplyr::everything()` should be used to capture which columns should be checked for missing rows: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nna_omit_rec <- \n  scat_rec %>% \n  step_naomit(everything())\n```\n:::\n\n\nTo estimate the recipe (manually), we can use `recipes::prep()` to process the training set and use these values to decide which rows to omit: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nna_omit_rec <- \n  scat_rec %>% \n  step_naomit(everything()) %>% \n  prep()\n```\n:::\n\n\nThe `recipes::bake()` function can be used to apply the recipe to a data set. Before processing, there are 81 scat samples in the training set. How many remain after applying the recipe?\n\nTo do this, we can use the `bake()` function but supply `new_data = NULL`. This is a shortcut: when preparing the recipe, we must execute all the steps on the entire training set. By default, recipes save the preprocessed version of the training set in the recipe object. There's no need to re-process the data. \n\nThe results is that we loose 16 scat samples due to missingness: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nall_complete <- bake(na_omit_rec, new_data = NULL)\n\nnrow(all_complete)\n#> [1] 65\n```\n:::\n\n\n_However_, `step_naomit()` is a bit irregular compared to other recipe steps. It is designed to [skip execution on every other data set](https://recipes.tidymodels.org/articles/Skipping.html). This is an important (and appropriate) choice for this method. \n\nIf we were to apply the recipe to the test set, it would _not_ exclude the missing rows: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbake(na_omit_rec, new_data = scat_te) %>% nrow()\n#> [1] 29\n\nnrow(scat_te)\n#> [1] 29\n\n# but there are missing values:\nsum(!complete.cases(scat_te))\n#> [1] 3\n```\n:::\n\n\n<a href=\"https://aml4td.org/chapters/missing-data.html#sec-removing-missing-data\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n### Column Filtering {#sec-removing-missing-cols}\n\nThe sample size of this data set is not large; removing rows might be more problematic than removing columns with a lot of missingness. We can use the `step_filter_missing()` step to do this. We decide on a threshold representing our \"line of dignity\" regarding how much missingness is acceptable. That can be specified as a proportion of missing data and is passed to the `threshold` argument. \n\nHere's an example where we determine that more than 10% missingness is too much. Based on our results from the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=naniar\">naniar</a></span> package above, this should eliminate two predictors (`Taper` and `TI`). \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfilter_features_rec <- \n  scat_rec %>% \n  step_filter_missing(everything(), threshold = 0.10) %>% \n  prep()\n\nncol(scat_tr)\n#> [1] 19\nbake(filter_features_rec, new_data = NULL) %>% ncol()\n#> [1] 17\n\n# use the tody method to determine which were removed: \ntidy(filter_features_rec, number = 1)\n#> # A tibble: 2 × 2\n#>   terms id                  \n#>   <chr> <chr>               \n#> 1 Taper filter_missing_jh0Cw\n#> 2 TI    filter_missing_jh0Cw\n```\n:::\n\n\n<a href=\"https://aml4td.org/chapters/missing-data.html#sec-removing-missing-data\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n## Imputation\n\n\n\nThe <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=recipes\">recipes</a></span> package has several steps for imputing predictors: `recipes::step_impute_bag()`, `recipes::step_impute_knn()`, `recipes::step_impute_linear()`, `recipes::step_impute_lower()`, `recipes::step_impute_mean()`, `recipes::step_impute_median()`, `recipes::step_impute_mode()`, `recipes::step_impute_roll()`. \n\n### Linear Regression {#sec-imputation-linear}\n\nLet’s consider using linear regression to predict the rows missing their value of `Taper`. The imputation steps allow you to select which column to impute and which predictors to use as predictors in the imputation model.\n\nIf we were to predictor `Taper` as a function of `Age`, `Length`, `Number`, and `Location`, the code would be: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlin_impute_rec <- \n  scat_rec %>% \n  step_impute_linear(Taper, impute_with = imp_vars(Age, Length, Number, Location)) %>% \n  prep() # <- This estimates the regression\n\n# Imputing the test set: \nlin_impute_rec %>% \n  bake(new_data = scat_te, Taper) %>% \n  filter(is.na(Taper))\n#> # A tibble: 0 × 1\n#> # ℹ 1 variable: Taper <dbl>\n```\n:::\n\n\nThe `tidy()` methods can extract the model object. We'll use `tidyr::enframe()` to get the coefficients: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_res <- tidy(lin_impute_rec, number = 1) \nlm_res\n#> # A tibble: 1 × 3\n#>   terms model  id                 \n#>   <chr> <list> <chr>              \n#> 1 Taper <lm>   impute_linear_dtBVd\n\nenframe(coef(lm_res$model[[1]]))\n#> # A tibble: 6 × 2\n#>   name               value\n#>   <chr>              <dbl>\n#> 1 (Intercept)      38.34  \n#> 2 Age              -1.603 \n#> 3 Length            0.2712\n#> 4 Number           -2.480 \n#> 5 Locationmiddle   -5.366 \n#> 6 Locationoff_edge  5.216\n```\n:::\n\n\nWe might also want to impute the predictor based on its mean value but would like it to be different based on some other grouping column. We can accomplish this by using a single categorical predictor in the formula (such as `Location`): \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngroup_impute_rec <- \n  scat_rec %>% \n  step_mutate(Taper_missing = is.na(Taper)) %>% \n  step_impute_linear(Taper, impute_with = imp_vars(Location)) %>% \n  prep()\n\ngroup_impute_rec %>% \n  bake(new_data = scat_tr, Taper, Taper_missing, Location) %>% \n  filter(Taper_missing) %>% \n  count(Taper, Location)\n#> # A tibble: 3 × 3\n#>   Taper Location     n\n#>   <dbl> <fct>    <int>\n#> 1 24.18 middle      10\n#> 2 29.87 edge         2\n#> 3 30.52 off_edge     2\n```\n:::\n\n\n<a href=\"https://aml4td.org/chapters/missing-data.html#sec-imputation-linear\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n### Nearest-Neighbor Imputation {#sec-imputation-knn}\n\nThe syntax for imputation steps is very consistent, so the only change that would be made to move from linear imputation to a nonlinear, nearest-neighbor method would be to change the name. \n\nThe number of neighbors defaults to five. We can change that using the `neighbors` option:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_impute_rec <- \n  scat_rec %>% \n  step_impute_knn(\n    all_of(miss_cols), \n    impute_with = imp_vars(Age, Length, Number, Location),\n    neighbors = 5) %>% \n  prep()\n\nimputed_train <- \n  knn_impute_rec %>% \n  bake(new_data = scat_tr)\n\nmean(complete.cases(imputed_train))\n#> [1] 1\n```\n:::\n\n\nNote that this step uses [Gower distance]() to define the neighbors. This method does _not_ require the predictors to be numeric or in the same units; they can be left as-is. Also, the function keeps the imputed data in the same format. A categorical predictor being imputed will remain a categorical predictor. \n\n<a href=\"https://aml4td.org/chapters/missing-data.html#sec-imputation-knn\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n### Tuning the Preprocessors {#sec-imputation-within-tuning}\n\nThe syntax to tune parameters will be described in depth in @sec-overfitting. Let’s briefly show that preprocessing parameters can also be tuned. \n\nMany tree-based models can naturally handle missingness. Random forest models compute a large number of tree-based models and combine them into an ensemble model. Unfortunately, most implementations of random forests require complete data. \n\nLet’s use our neighbor-based imputation method but tune the number of neighbors. At the same time, we can tune the random forest $n_{min}$ parameter using a space-filling grid. \n\nTo do this, we give the `neighbors` argument of `step_impute_knn()` a value of `tune()`. This marks it for optimization. tidymodels knows a lot about these parameters and can make informed decisions about the range and scale of the tuning parameters. With the `tune::tune_grid()` function, using `grid = 15` will automatically create a two-factor grid of candidate models to evaluate. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_impute_rec <- \n  scat_rec %>% \n  step_impute_knn(\n    all_of(miss_cols), \n    impute_with = imp_vars(Age, Length, Number, Location),\n    neighbors = tune()) \n\nrf_spec <- \n  rand_forest(min_n = tune(), trees = 1000) %>% \n  set_mode(\"classification\")\n\nknn_rf_wflow <- workflow(knn_impute_rec, rf_spec)\n\nknn_rf_res <- \n  knn_rf_wflow %>% \n  tune_grid(\n    scat_rs,\n    grid = 15\n  )\n```\n:::\n\n\nLooking at the results below, we can see that the number of neighbors does not seem to affect performance (measured via a Brier score). However, for these data, the random forests $n_{min}$ parameter does have a profound effect on model performance. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshow_best(knn_rf_res, metric = \"brier_class\")\n#> # A tibble: 5 × 8\n#>   min_n neighbors .metric     .estimator   mean     n  std_err .config              \n#>   <int>     <int> <chr>       <chr>       <dbl> <int>    <dbl> <chr>                \n#> 1     2         6 brier_class multiclass 0.1843    50 0.008136 Preprocessor01_Model1\n#> 2     4         2 brier_class multiclass 0.1855    50 0.007938 Preprocessor02_Model1\n#> 3    10         9 brier_class multiclass 0.1900    50 0.007496 Preprocessor04_Model1\n#> 4     7         4 brier_class multiclass 0.1904    50 0.007951 Preprocessor03_Model1\n#> 5    12         1 brier_class multiclass 0.1937    50 0.007449 Preprocessor05_Model1\n\nautoplot(knn_rf_res, metric = \"brier_class\")\n```\n\n::: {.cell-output-display}\n![](../figures/impute-knn-tune-res-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}