{
  "hash": "64eac2c6e035fabf9ae372210d42cbc9",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/resampling/\"\n---\n\n# Measuring Performance with Resampling {#sec-resampling}\n\nThis chapter outlines how to create objects to facilitate resampling. At the end, @sec-resampled-models illustrates how to use the resampling objects to produce good estimates of performance for a model. \n\n\n\n## Requirements\n\nWe will use the `ames`, `concrete`, and `Chicago` data sets from the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=modeldata\">modeldata</a></span> package (which is automatically loaded below) to illustrate some of the methods. The other data, `Orthodont`, can be obtained in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=nlme\">nlme</a></span> package which comes with each R installation.  \n\nYou’ll need 8 packages (<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=Cubist\">Cubist</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=future.mirai\">future.mirai</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=parallelly\">parallelly</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=probably\">probably</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=rules\">rules</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=spatialsample\">spatialsample</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidymodels\">tidymodels</a></span>, and <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidysdm\">tidysdm</a></span>) for this chapter. \nYou can install them via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreq_pkg <- c(\"Cubist\", \"future.mirai\", \"parallelly\", \"probably\", \"rules\", \"spatialsample\", \n             \"tidymodels\", \"tidysdm\")\n\n# Check to see if they are installed: \npkg_installed <- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list <- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\n```\n:::\n\n\nLet's load the meta package and manage some between-package function conflicts. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())\n```\n:::\n\n\n## Basic Methods and Data Structures {#sec-resampling-basics}\n\nThe <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=rsample\">rsample</a></span> package provides many functions to facilitate resampling. For now, we’ll assume an initial split into a training and test set has been made (but see @sec-validation-sets below for three-way splits). \n\nLet’s use the concrete data for illustration: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(82)\nconcrete_split <- initial_split(concrete)\nconcrete_tr <- training(concrete_split)\nconcrete_te <- testing(concrete_split)\n```\n:::\n\n\nAll resampling methods use the training set as the substrate for creating resamples. We’ll demonstrate the main tools and functions using basic bootstrap resampling and then discuss the other resampling methods. \n\nThe function that we’ll use to demonstrate is `rsample::bootstraps()`. Its main option, `times`, describes how many resamples to create. Let’s make five resamples: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(380)\nconcrete_rs <- bootstraps(concrete, times = 5)\n```\n:::\n\n\nThis creates a tibble with five rows and two columns: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconcrete_rs\n#> # Bootstrap sampling \n#> # A tibble: 5 × 2\n#>   splits             id        \n#>   <list>             <chr>     \n#> 1 <split [1030/355]> Bootstrap1\n#> 2 <split [1030/408]> Bootstrap2\n#> 3 <split [1030/369]> Bootstrap3\n#> 4 <split [1030/375]> Bootstrap4\n#> 5 <split [1030/378]> Bootstrap5\n```\n:::\n\n\nWe'll investigate the `splits` column in a bit. The `id` column gives each row a name (corresponding to a resample). Some resampling methods, such as repeated cross-validation, can have additional identification columns. \n\nThe object has two additional classes `bootstraps` and `rset` that differentiate it from a standard tibble. \n\nThis collection of splits is called an `\"rset\"` and has specific rules. The object above is defined as a set of five bootstrap samples. If we delete a row, it breaks that definition, and the class drops back down to a basic tibble: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconcrete_rs[-1,]\n#> # A tibble: 4 × 2\n#>   splits             id        \n#>   <list>             <chr>     \n#> 1 <split [1030/408]> Bootstrap2\n#> 2 <split [1030/369]> Bootstrap3\n#> 3 <split [1030/375]> Bootstrap4\n#> 4 <split [1030/378]> Bootstrap5\n```\n:::\n\n\nThe \"A tibble: 4 × 2\" title is different from the original title of \"Bootstrap sampling.\" This, and the class values, give away the difference. You can add columns without violating the definition. \n\n### Split Objects {#sec-rsplits}\n\nNote the `splits` column in the output above. This is a _list column_ in the data frame. It contains an `rsplit` object that tells us which rows of the training set go into our analysis and assessment sets. As a reminder: \n\n- The **analysis set** (of size $n_{fit}$) estimates quantities associated with preprocessing, model training, and postprocessing. \n- The **assessment set** ($n_{pred}$) is only used for prediction so that we can compute measures of model effectiveness (e.g., RMSE, classification accuracy, etc).  \n\nWhen we see output \"`<split [1030/408]>`\" this indicates a binary split where the analysis set has 1030 rows, and the assessment set has 408 rows. \n\nTo get the analysis and assessment sets, there are two eponymous functions. For a specific split: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nex_split <- concrete_rs$splits[[1]]\nex_split\n#> <Analysis/Assess/Total>\n#> <1030/355/1030>\n\nanalysis(ex_split)   %>% dim()\n#> [1] 1030    9\nassessment(ex_split) %>% dim()\n#> [1] 355   9\n```\n:::\n\n\nIf we want to get the specific row indices of the training set: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nex_ind <- as.integer(ex_split, data = \"assessment\")\nhead(ex_ind)\n#> [1]  2  5  6 14 15 17\nlength(ex_ind)\n#> [1] 355\n```\n:::\n\n\nYou shouldn't really have to interact with these objects at all. \n\n## Basic Resampling Tools {#sec-resampling-todo}\n\n<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=rsample\">rsample</a></span> contains several functions that resample data whose rows are thought to be statistically independent of one another. Almost all of these functions contain options for stratified resampling. \n\nWe'll show an example with basic 10-fold cross-validation below in @sec-resampled-models.\n\nLet's examine each flavor of resampling mentioned in the book chapter. \n\n## Validation Sets {#sec-validation-sets}\n\nTo create a validation set, the first split should use `rsample::initial_validation_split()`: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(426)\nconcrete_split <- initial_validation_split(concrete, prop = c(.8, .1))\nconcrete_tr <- training(concrete_split)\nconcrete_vl <- validation(concrete_split)\nconcrete_te <- testing(concrete_split)\n```\n:::\n\n\nTo make an `rset` object that can be used with most of tidymodel’s resampling machinery, we can use the `rsample::validation_set()` function to produce one (taking the initial three-way split as input): \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconcrete_rs <- validation_set(concrete_split)\nconcrete_rs\n#> # A tibble: 1 × 2\n#>   splits            id        \n#>   <list>            <chr>     \n#> 1 <split [824/103]> validation\n```\n:::\n\n\nAt this point, we can use `concrete_rs` as if it were any other `rset` object. \n\n<a href=\"https://aml4td.org/chapters/resampling.html#sec-validation\" >{{< fa solid rotate-left size=small >}}</a>\n\n## Monte Carlo Cross-Validation {#sec-cv-mc}\n\nThe relevant function here is `mc_cv()` with two main arguments:\n\n - `times` is the number of resamples\n - `prop` is the proportion of the data that is allocated to the analysis set. \n\nFor example: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(380)\nmc_cv(concrete_tr, times = 3, prop = 9 / 10)\n#> # Monte Carlo cross-validation (0.9/0.1) with 3 resamples \n#> # A tibble: 3 × 2\n#>   splits           id       \n#>   <list>           <chr>    \n#> 1 <split [741/83]> Resample1\n#> 2 <split [741/83]> Resample2\n#> 3 <split [741/83]> Resample3\n\n# or \n\nmc_cv(concrete_tr, times = 2, prop = 2 /  3)\n#> # Monte Carlo cross-validation (0.67/0.33) with 2 resamples \n#> # A tibble: 2 × 2\n#>   splits            id       \n#>   <list>            <chr>    \n#> 1 <split [549/275]> Resample1\n#> 2 <split [549/275]> Resample2\n```\n:::\n\n\n<a href=\"https://aml4td.org/chapters/resampling.html#sec-cv-mc\" >{{< fa solid rotate-left size=small >}}</a>\n\n## V-Fold Cross-Validation {#sec-cv}\n\nBasic V-fold cross-validation is performed using `vfold_cv()`.  The `v` argument defines the number of folds and defaults to `v = 10`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(380)\nconcrete_rs <- vfold_cv(concrete_tr)\nconcrete_rs\n#> #  10-fold cross-validation \n#> # A tibble: 10 × 2\n#>   splits           id    \n#>   <list>           <chr> \n#> 1 <split [741/83]> Fold01\n#> 2 <split [741/83]> Fold02\n#> 3 <split [741/83]> Fold03\n#> 4 <split [741/83]> Fold04\n#> 5 <split [742/82]> Fold05\n#> 6 <split [742/82]> Fold06\n#> # ℹ 4 more rows\n```\n:::\n\n\nAs with the other tools, the `strata` argument can balance the outcome distributions across folds. It takes a single column as input. \n\nAnother argument of note is `repeats`, which describes how many sets of V resamples should be created. This generated an additional column called `id2`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(380)\nconcrete_rs <- vfold_cv(concrete_tr, repeats = 2)\nconcrete_rs\n#> #  10-fold cross-validation repeated 2 times \n#> # A tibble: 20 × 3\n#>   splits           id      id2   \n#>   <list>           <chr>   <chr> \n#> 1 <split [741/83]> Repeat1 Fold01\n#> 2 <split [741/83]> Repeat1 Fold02\n#> 3 <split [741/83]> Repeat1 Fold03\n#> 4 <split [741/83]> Repeat1 Fold04\n#> 5 <split [742/82]> Repeat1 Fold05\n#> 6 <split [742/82]> Repeat1 Fold06\n#> # ℹ 14 more rows\n```\n:::\n\n\n<a href=\"https://aml4td.org/chapters/resampling.html#sec-cv\" >{{< fa solid rotate-left size=small >}}</a>\n\n## The Bootstrap {#sec-bootstrap}\n\nFirst, there are special occasions when the regular set of bootstrap samples needs to be supplemented with an additional resample that can be used to measure the resubstitution rate (predicting the analysis set after fitting the data on the same analysis set). The function that produces this extra row is called `apparent()`, which is the same name as the function argument: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(380)\nconcrete_rs <- bootstraps(concrete, times = 5, apparent = TRUE)\n```\n:::\n\n\nNote that the `id` column reflects this, and the split label shows that the analysis and assessment sets are the same size. \n\nThe tidymodels resampling functions are aware of the potential presence of the \"apparent\" sample and will _not_ include it at inappropriate times. For example, if we resample a model with and without using `apparent = TRUE`, we’ll get the same results as long as we use the same random number seed to make each set of resamples. \n\nSecondly, there is a `strata` argument for this function. That enables different bootstrap samples to be taken within each stratum, which are combined into the final resampling set. _Technically_, this isn’t a bootstrap sample, but it is probably close enough to be useful. \n\n<a href=\"https://aml4td.org/chapters/resampling.html#sec-bootstrap\" >{{< fa solid rotate-left size=small >}}</a>\n\n## Time Series Data {#sec-time-series-resampling}\n\nUsually, the most recent data are used to evaluate performance for time series data. The function `initial_time_split()` can be used to make the initial split. We’ll use the `Chicago` data to demonstrate: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- nrow(Chicago)\n# To get 30 days of data\nprop_30 <- (n - 30) / n\n\nchi_split <- initial_time_split(Chicago, prop = prop_30)\nchi_split\n#> <Training/Testing/Total>\n#> <5668/30/5698>\n\nchi_tr <- training(chi_split)\nchi_te <- testing(chi_split)\n```\n:::\n\n\nLet's say that we want \n\n - 5,000 days in our analysis set,\n - 30 day assessment sets\n - shift the 30-day window 30 days ahead\n\nThe data set has a column with the `Date` class. We can use this to partition the data in case there is an unequal number of data points in our 30-day period. The `sliding_period()` can be used with a date or date/time input, while `sliding_index()` can be used for equally spaced data. \n\nLet’s use `sliding_period()` for this example, annotate the argument logic in comments, and then compute a few columns for the start/stop dates for the analysis and assessment sets (for illustration): \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nchi_rs <- \n  sliding_period(\n    chi_tr,\n    index = date,\n    period = \"day\",   # Could be \"year\", \"quarter\", \"month\", \"week\", or \"day\"\n    lookback = 5000,  # 5000 days in the analysis sets\n    assess_start = 1, # Start the assessment set 1 day after analysis set\n    assess_stop = 30, # Stop the assessment set 20 days after analysis set\n    step = 30         # Jump ahead 30 days between resamples; no assessment overlap in assessments\n  ) %>% \n  mutate(\n    fit_start =  map_vec(splits, ~ min(analysis(.x)$date)),\n    fit_stop =   map_vec(splits, ~ max(analysis(.x)$date)),\n    perf_start = map_vec(splits, ~ min(assessment(.x)$date)),\n    perf_stop =  map_vec(splits, ~ max(assessment(.x)$date))\n  )\nchi_rs\n#> # Sliding period resampling \n#> # A tibble: 22 × 6\n#>   splits            id      fit_start  fit_stop   perf_start perf_stop \n#>   <list>            <chr>   <date>     <date>     <date>     <date>    \n#> 1 <split [5001/30]> Slice01 2001-01-22 2014-10-01 2014-10-02 2014-10-31\n#> 2 <split [5001/30]> Slice02 2001-02-21 2014-10-31 2014-11-01 2014-11-30\n#> 3 <split [5001/30]> Slice03 2001-03-23 2014-11-30 2014-12-01 2014-12-30\n#> 4 <split [5001/30]> Slice04 2001-04-22 2014-12-30 2014-12-31 2015-01-29\n#> 5 <split [5001/30]> Slice05 2001-05-22 2015-01-29 2015-01-30 2015-02-28\n#> 6 <split [5001/30]> Slice06 2001-06-21 2015-02-28 2015-03-01 2015-03-30\n#> # ℹ 16 more rows\n```\n:::\n\n\nThe first analysis set starts on 2001-01-22 and ends 5,000 days later on 2014-10-01. The next day (2014-10-02), the analysis set includes 30 days and stops on 2014-10-31. \n\nFor the second resample, the analysis and assessment sets both start 30 days later. \n\nHere is a visualization of the date periods defined by the resampling scheme that illustrates why the method is sometimes called rolling origin forecast resampling. The figure also shows that the assessment sets are very small compared to the analysis sets.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nchi_rs %>% \n  ggplot(aes(y = id)) + \n  geom_segment(aes(x = fit_start,  xend = fit_stop,  yend = id), col = \"grey\", linewidth = 1) +\n  geom_segment(aes(x = perf_start, xend = perf_stop, yend = id), col = \"red\", linewidth = 3) +\n  labs(y = NULL, x = \"Date\")\n```\n\n::: {.cell-output-display}\n![](../figures/rolling-analysis-1.svg){fig-align='center' width=80%}\n:::\n:::\n\n\nOne variation of this approach is to cumulately increase the analysis set by keeping the starting date the same (inside of sliding/rolling). For this, we can make the \"lookback\" infinite but use the `skip` argument to remove the large number of resamples that contain fewer than 5,000 days in the analysis sets: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nchi_rs <- \n  sliding_period(\n    chi_tr,\n    index = date,\n    period = \"day\", \n    lookback = Inf,   # Use all data before assessment\n    assess_start = 1,\n    assess_stop = 30,\n    step = 30,\n    skip = 5000       # Drop first 5000 results so assessment starts at same time \n  ) %>% \n  mutate(\n    fit_start =  map_vec(splits, ~ min(analysis(.x)$date)),\n    fit_stop =   map_vec(splits, ~ max(analysis(.x)$date)),\n    perf_start = map_vec(splits, ~ min(assessment(.x)$date)),\n    perf_stop =  map_vec(splits, ~ max(assessment(.x)$date))\n  )\nchi_rs\n#> # Sliding period resampling \n#> # A tibble: 22 × 6\n#>   splits            id      fit_start  fit_stop   perf_start perf_stop \n#>   <list>            <chr>   <date>     <date>     <date>     <date>    \n#> 1 <split [5001/30]> Slice01 2001-01-22 2014-10-01 2014-10-02 2014-10-31\n#> 2 <split [5031/30]> Slice02 2001-01-22 2014-10-31 2014-11-01 2014-11-30\n#> 3 <split [5061/30]> Slice03 2001-01-22 2014-11-30 2014-12-01 2014-12-30\n#> 4 <split [5091/30]> Slice04 2001-01-22 2014-12-30 2014-12-31 2015-01-29\n#> 5 <split [5121/30]> Slice05 2001-01-22 2015-01-29 2015-01-30 2015-02-28\n#> 6 <split [5151/30]> Slice06 2001-01-22 2015-02-28 2015-03-01 2015-03-30\n#> # ℹ 16 more rows\n```\n:::\n\n\nNote that the values in the `fit_stop` column are the same. Visually: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nchi_rs %>% \n  ggplot(aes(y = id)) + \n  geom_segment(aes(x = fit_start,  xend = fit_stop,  yend = id), col = \"grey\", linewidth = 1) +\n  geom_segment(aes(x = perf_start, xend = perf_stop, yend = id), col = \"red\", linewidth = 3) +\n  labs(y = NULL, x = \"Date\") \n```\n\n::: {.cell-output-display}\n![](../figures/cumulative-analysis-1.svg){fig-align='center' width=80%}\n:::\n:::\n\n\n<a href=\"https://aml4td.org/chapters/resampling.html#sec-time-series-resampling\" >{{< fa solid rotate-left size=small >}}</a>\n\n## Spatial Data {#sec-spatial-resampling}\n\nWe split the Ames data into a training and testing set back in @sec-spatial-splitting using this code: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(sf)\n#> Linking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\nlibrary(spatialsample)\nlibrary(tidysdm)\n\names_sf <-\n  ames %>%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n\nset.seed(318)\names_block_buff_split <-\n  spatial_initial_split(\n    ames_sf, \n    prop = 0.2, \n    strategy = spatial_block_cv,\n    method = \"continuous\",\n    n = 25, \n    square = FALSE,\n    buffer = 250)\n\names_tr <- training(ames_block_buff_split)\names_te <- testing(ames_block_buff_split)\n```\n:::\n\n\nThe options for resampling are basically the same as the initial split. For example, with block resampling, we create a grid on the training set and allocate specific grids to specific assessment sets. Buffering can also be used for each resample. The `spatial_block_cv()`  function in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=spatialsample\">spatialsample</a></span> package and has a `v` argument for the number of resamples: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(652)\names_rs <-\n  spatial_block_cv(\n    ames_tr, \n    v = 10,\n    method = \"continuous\",\n    n = 25, \n    square = FALSE,\n    buffer = 250)\names_rs\n#> #  10-fold spatial block cross-validation \n#> # A tibble: 10 × 2\n#>   splits             id    \n#>   <list>             <chr> \n#> 1 <split [1047/167]> Fold01\n#> 2 <split [1098/182]> Fold02\n#> 3 <split [1225/147]> Fold03\n#> 4 <split [1130/174]> Fold04\n#> 5 <split [1123/155]> Fold05\n#> 6 <split [1235/95]>  Fold06\n#> # ℹ 4 more rows\n```\n:::\n\n\nThere is an overall `autoplot()` method that can be used to show the grid:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(ames_rs, cex = 1 / 3, show_grid = TRUE)\n```\n\n::: {.cell-output-display}\n![](../figures/ames-split-block-rs-all-1.svg){fig-align='center' width=90%}\n:::\n:::\n\n\nWe can also `autoplot()` individual splits to see the analysis and assessment set. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(ames_rs$splits[[1]], cex = 1 / 2)\n```\n\n::: {.cell-output-display}\n![](../figures/ames-split-block-rs-single-1.svg){fig-align='center' width=70%}\n:::\n:::\n\n\n<a href=\"https://aml4td.org/chapters/resampling.html#sec-spatial-resampling\" >{{< fa solid rotate-left size=small >}}</a>\n\n## Grouped or Multi-Level Data {#sec-multilevel-resampling}\n\nReturning to the orthodontal data from @sec-multilevel-splitting, we use the initial split: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(Orthodont, package = \"nlme\")\n\nset.seed(93)\north_split <- group_initial_split(Orthodont, group = Subject, prop = 2 / 3)\north_tr <- training(orth_split)\north_te <- testing(orth_split)\n```\n:::\n\n\nThere are several resampling functions for these data in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=rsample\">rsample</a></span> package, including: `group_vfold_cv()`,  `group_bootstraps()`, and `group_mc_cv()`. For example: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(vctrs)\n\n# Subjects in the training set: \nvec_unique_count(orth_tr$Subject)\n#> [1] 18\n\nset.seed(714)\north_rs <- \n  group_vfold_cv(orth_tr, group = Subject, v = 10) %>% \n  mutate(num_subjects = map_int(splits, ~ vec_unique_count(assessment(.x)$Subject)))\n\north_rs\n#> # Group 10-fold cross-validation \n#> # A tibble: 10 × 3\n#>   splits         id         num_subjects\n#>   <list>         <chr>             <int>\n#> 1 <split [64/8]> Resample01            2\n#> 2 <split [64/8]> Resample02            2\n#> 3 <split [64/8]> Resample03            2\n#> 4 <split [64/8]> Resample04            2\n#> 5 <split [64/8]> Resample05            2\n#> 6 <split [64/8]> Resample06            2\n#> # ℹ 4 more rows\n```\n:::\n\n\nTo leave a single subject out for each resample, we could have set `v` to be 18. \n\n<a href=\"https://aml4td.org/chapters/resampling.html#sec-multilevel-resampling\" >{{< fa solid rotate-left size=small >}}</a>\n\n## Estimating Performance {#sec-resampled-models}\n\nNow that we can create different types of resamples for our training set, how do we actually resample a model to get accurate performance statistics? \n\ntidymodels contains high-level functions for this purpose, so there is typically no need to loop over rows of the resampling objects to get the data sets, train the model, etc. \n\nThe `fit_resamples()` function can do all of this for you. It takes a model (or workflow) in conjunction with a resampling object as inputs. \n\nTo demonstrate, let’s re-use the concrete data and create an object for a simple 10-fold cross-validation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(426)\nconcrete_split <- initial_split(concrete, prop = 3 / 4)\nconcrete_tr <- training(concrete_split)\nconcrete_te <- testing(concrete_split)\n\nconcrete_rs <- vfold_cv(concrete_tr)\n```\n:::\n\n\nLet’s use a Cubist model for the data. It creates a set of rules from the data (derived from a regression tree) and, for each rule, creates a corresponding linear regression for the training set points covered by the rule. In the end, a sample is predicted, perhaps using multiple rules, and the average of the linear regression models is used as the prediction. \n\nUsually, we use a boosting-like process called _model committees _to create an ensemble of rule sets. Instead, we will make a single rule set. We’ll need to load the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=rules\">rules</a></span> package to load this type of model into the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=parsnip\">parsnip</a></span> model database. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rules)\nrules_spec <- cubist_rules(committees = 1)\n```\n:::\n\n\nTo specify the model in `fit_resamples()`, there are two options: \n\n- The first two arguments can be a model specification and a preprocessor (in that order). The preprocessor could be a recipe or a standard R formula. \n- The first argument can be a workflow. \n\nAfter the model specification, the `resamples` argument takes the resamping object. \n\nFrom here, we can run `fit_resamples()`. Note that the Cubist model does not use any random numbers. If it did, we would probably want to set the random number seed before using `fit_resamples()`. \n\nOur code:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconcrete_res <- fit_resamples(rules_spec, compressive_strength ~ ., resamples = concrete_rs)\nconcrete_res\n#> # Resampling results\n#> # 10-fold cross-validation \n#> # A tibble: 10 × 4\n#>   splits           id     .metrics         .notes          \n#>   <list>           <chr>  <list>           <list>          \n#> 1 <split [694/78]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]>\n#> 2 <split [694/78]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]>\n#> 3 <split [695/77]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]>\n#> 4 <split [695/77]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]>\n#> 5 <split [695/77]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]>\n#> 6 <split [695/77]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]>\n#> # ℹ 4 more rows\n```\n:::\n\n\nThis looks a lot like our resampling object. There are some new columns. `.metrics` contains data frames with performance statistics for the particular resample. The `.notes` column contains any warnings or error messages that the model produced; none were produced by these 10 model fits. Note that, if there are errors, `fit_resamples()` does not stop computations.\n  \nHow can we get our performance estimates? To aggregate the data in this object, there is a set of `collect_*()` functions. The first is `collect_metrics()`. By default, it returns the averages of the resampled estimates:   \n  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(concrete_res)\n#> # A tibble: 2 × 6\n#>   .metric .estimator   mean     n std_err .config             \n#>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n#> 1 rmse    standard   6.503     10 0.2396  Preprocessor1_Model1\n#> 2 rsq     standard   0.8517    10 0.01170 Preprocessor1_Model1\n```\n:::\n\n\nNote the `n` and `std_err` columns. To get the per-resample estimates: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(concrete_res, summarize = FALSE)\n#> # A tibble: 20 × 5\n#>   id     .metric .estimator .estimate .config             \n#>   <chr>  <chr>   <chr>          <dbl> <chr>               \n#> 1 Fold01 rmse    standard      6.764  Preprocessor1_Model1\n#> 2 Fold01 rsq     standard      0.8462 Preprocessor1_Model1\n#> 3 Fold02 rmse    standard      6.925  Preprocessor1_Model1\n#> 4 Fold02 rsq     standard      0.8044 Preprocessor1_Model1\n#> 5 Fold03 rmse    standard      6.652  Preprocessor1_Model1\n#> 6 Fold03 rsq     standard      0.8298 Preprocessor1_Model1\n#> # ℹ 14 more rows\n```\n:::\n\n\nIf there were issues with the computations, `collect_notes(concrete_res)` would print a catalog of messages. \n\nNext, let’s look at a few customizations for `fit_resamples()`. \n\n### Parallel Processing {#sec-parallel-resamples}\n\nWe fit 10 different Cubist models to 10 slightly different data sets. None of these computations depend on one another. This is the case of an \"embarrassingly parallel\" computing issue. We can increase our computational efficiency by running training the models on multiple \"worker\" processes on our computer(s). \n\nThe `future` package can run the resamples in parallel. The `plan()` function sets the parallel processing engine. There are a few plans that can be used: \n\n - The most common approach is \"multisession\". This uses a parallel socket cluster (akak \"psock cluster\") on your local computer. It is available for all operating systems. \n - Another option (not available on Windows) is \"multicore\". The forks the current R session into different worker processes. \n - The \"cluster\" option is most useful for having worker processes on different machines. \n - The \"sequential\" plan is regular, non-parallel computing. \n \nThere are several other packages with plans: <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=future.batchtools\">future.batchtools</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=future.callr\">future.callr</a></span>, and <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=future.mirai\">future.mirai</a></span>. \n\nOnce we know a plan, we run the following code (once) before running operations that can be done in parallel: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(future)\nparallelly::availableCores()\nplan(multisession)\n```\n:::\n\n\nAlternatively, the more recent mirai \"engine\" for parallel processing can also be used for additional efficiency. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(future.mirai)\n#> Loading required package: future\nplan(mirai_multisession)\n```\n:::\n\n\nThis will generally increase the efficiency of the resampling process. \n\n### Other Options {#sec-function-arguments}\n\nLet's look at some other options. First, note that the results did not include the trained models or the out-of-sample predicted results. This is the default because there is no way of knowing how much memory will be required to keep these values. \n\nWe’ll talk about accessing the fitted models in the next sections. \n\nTo save the predictions, we can use an R convention of a \"control function.\" These functions are reserved for specifying ancillary aspects of the computations. For `fit_resamples()` the control function is called `control_resamples()`, and it has an option called `save_pred`. When set to `TRUE`, the out-of-sample predictions are retained. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nctrl <- control_resamples(save_pred = TRUE)\n```\n:::\n\n\nLet's also estimate different metrics. As mentioned back in @sec-model-development-whole-game, a _metric set_ is used to specify statistics for model efficacy. `fit_resamples()` has an option called `metrics` that we can use to pass in a metric set.\n\nLet's re-run our model with these two changes:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreg_mtr <- metric_set(rmse, rsq, ccc, mae)\n\nconcrete_opts_res <- fit_resamples(\n  rules_spec,\n  compressive_strength ~ .,\n  resamples = concrete_rs,\n  metrics = reg_mtr,\n  control = ctrl\n)\nconcrete_opts_res\n#> # Resampling results\n#> # 10-fold cross-validation \n#> # A tibble: 10 × 5\n#>   splits           id     .metrics         .notes           .predictions     \n#>   <list>           <chr>  <list>           <list>           <list>           \n#> 1 <split [694/78]> Fold01 <tibble [4 × 4]> <tibble [0 × 3]> <tibble [78 × 4]>\n#> 2 <split [694/78]> Fold02 <tibble [4 × 4]> <tibble [0 × 3]> <tibble [78 × 4]>\n#> 3 <split [695/77]> Fold03 <tibble [4 × 4]> <tibble [0 × 3]> <tibble [77 × 4]>\n#> 4 <split [695/77]> Fold04 <tibble [4 × 4]> <tibble [0 × 3]> <tibble [77 × 4]>\n#> 5 <split [695/77]> Fold05 <tibble [4 × 4]> <tibble [0 × 3]> <tibble [77 × 4]>\n#> 6 <split [695/77]> Fold06 <tibble [4 × 4]> <tibble [0 × 3]> <tibble [77 × 4]>\n#> # ℹ 4 more rows\n```\n:::\n\n\nThe expanded set of metrics: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(concrete_opts_res)\n#> # A tibble: 4 × 6\n#>   .metric .estimator   mean     n  std_err .config             \n#>   <chr>   <chr>       <dbl> <int>    <dbl> <chr>               \n#> 1 ccc     standard   0.9198    10 0.006753 Preprocessor1_Model1\n#> 2 mae     standard   4.621     10 0.1657   Preprocessor1_Model1\n#> 3 rmse    standard   6.503     10 0.2396   Preprocessor1_Model1\n#> 4 rsq     standard   0.8517    10 0.01170  Preprocessor1_Model1\n```\n:::\n\n\nNotice that there is now a column named `.predictions`. The number of rows in the tibbles matches the sizes of the assessment sets shown in the `splits` column; these are the held-out predicted values. \n\nTo obtain these values, there are `collect_predictions()` and `augment()`: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nheldout_pred <- collect_predictions(concrete_opts_res)\nheldout_pred\n#> # A tibble: 772 × 5\n#>   .pred id      .row compressive_strength .config             \n#>   <dbl> <chr>  <int>                <dbl> <chr>               \n#> 1 40.56 Fold01     8                40.76 Preprocessor1_Model1\n#> 2 29.34 Fold01    14                29.22 Preprocessor1_Model1\n#> 3 32.65 Fold01    27                37.36 Preprocessor1_Model1\n#> 4 24.10 Fold01    38                20.73 Preprocessor1_Model1\n#> 5 61.61 Fold01    53                55.16 Preprocessor1_Model1\n#> 6 12.30 Fold01    71                 9.74 Preprocessor1_Model1\n#> # ℹ 766 more rows\n\n# Same but merges them with the original training data\naugment(concrete_opts_res)\n#> # A tibble: 772 × 11\n#>    .pred    .resid cement blast_furnace_slag fly_ash water superplasticizer\n#>    <dbl>     <dbl>  <dbl>              <dbl>   <dbl> <dbl>            <dbl>\n#> 1 45.54  -12.69     168                 42.1   163.8 121.8              5.7\n#> 2 54.82   -1.131    190                190       0   228                0  \n#> 3 32.77    0.07400  160                188     146   203               11  \n#> 4  9.688   2.862    190.3                0     125.2 166.6              9.9\n#> 5 47.36  -10.09     218.9                0     124.1 158.5             11.3\n#> 6 16.93   -9.526    168.9               42.2   124.3 158.3             10.8\n#> # ℹ 766 more rows\n#> # ℹ 4 more variables: coarse_aggregate <dbl>, fine_aggregate <dbl>, age <int>,\n#> #   compressive_strength <dbl>\n```\n:::\n\n\nFrom here, we can do exploratory data analysis to understand where our model can be improved. \n\nOne other option: the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=probably\">probably</a></span> package has a simple interface for obtaining plots of the observed and predicted values (i.e., a regression \"calibration\" plot): \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(probably)\ncal_plot_regression(concrete_opts_res)\n```\n\n::: {.cell-output-display}\n![](../figures/reg-cal-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\nNot too bad but there are fairly large outliers that seem to occur more with mixtures corresponding to larger observed outcomes. \n\n### Extracting Results {#sec-extraction}\n\nHow can we get the 10 trained models? The control function has an option for `extract` that takes a user-defined function. The argument to this function (say `x`) is the fitted workflow. If you want the whole model, you can return `x`. Otherwise, we can run computations on the model and return whatever elements or statistics associated with the model that we are interested in. \n\nFor example, a `tidy()` method for Cubist models will save information on the rules, the regression function for each rule, and various statistics. To get this, we need to pull the Cubist model out of the workflow and then run the tidy method on it. Here is an example of that: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nextract_rules <- function(x) {\n  x %>% \n    extract_fit_engine() %>%\n    tidy()\n}\n```\n:::\n\n\nNow we update our control object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nctrl <- control_resamples(save_pred = TRUE, extract = extract_rules)\n```\n:::\n\n\nand pass it into `fit_resamples()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconcrete_ext_res <- fit_resamples(rules_spec,\n                                  compressive_strength ~ .,\n                                  resamples = concrete_rs,\n                                  control = ctrl)\nconcrete_ext_res\n#> # Resampling results\n#> # 10-fold cross-validation \n#> # A tibble: 10 × 6\n#>   splits           id     .metrics         .notes           .extracts .predictions\n#>   <list>           <chr>  <list>           <list>           <list>    <list>      \n#> 1 <split [694/78]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>  <tibble>    \n#> 2 <split [694/78]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>  <tibble>    \n#> 3 <split [695/77]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>  <tibble>    \n#> 4 <split [695/77]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>  <tibble>    \n#> 5 <split [695/77]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>  <tibble>    \n#> 6 <split [695/77]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>  <tibble>    \n#> # ℹ 4 more rows\n```\n:::\n\n\nThe new output has an `.extracts` column. The rows contain a tibble that has the resampling information and another tibble, also with the name `.extracts`. We can pull that column out via: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrule_extract <- collect_extracts(concrete_ext_res)\nrule_extract\n#> # A tibble: 10 × 3\n#>   id     .extracts         .config             \n#>   <chr>  <list>            <chr>               \n#> 1 Fold01 <tibble [10 × 5]> Preprocessor1_Model1\n#> 2 Fold02 <tibble [13 × 5]> Preprocessor1_Model1\n#> 3 Fold03 <tibble [13 × 5]> Preprocessor1_Model1\n#> 4 Fold04 <tibble [11 × 5]> Preprocessor1_Model1\n#> 5 Fold05 <tibble [13 × 5]> Preprocessor1_Model1\n#> 6 Fold06 <tibble [13 × 5]> Preprocessor1_Model1\n#> # ℹ 4 more rows\n```\n:::\n\n\nWhat is in a specific result? \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrule_extract$.extracts[[1]]\n#> # A tibble: 10 × 5\n#>   committee rule_num rule                                         estimate statistic\n#>       <int>    <int> <chr>                                        <list>   <list>   \n#> 1         1        1 ( cement <= 218.89999 ) & ( blast_furnace_s… <tibble> <tibble> \n#> 2         1        2 ( cement <= 218.89999 ) & ( blast_furnace_s… <tibble> <tibble> \n#> 3         1        3 ( water > 183.8 ) & ( cement > 218.89999 ) … <tibble> <tibble> \n#> 4         1        4 ( age <= 7 ) & ( water <= 183.8 ) & ( blast… <tibble> <tibble> \n#> 5         1        5 ( age <= 28 ) & ( age > 7 ) & ( cement > 21… <tibble> <tibble> \n#> 6         1        6 ( age > 28 ) & ( superplasticizer <= 7.8000… <tibble> <tibble> \n#> # ℹ 4 more rows\n```\n:::\n\n\nTo \"flatten out\" these results, we’ll `unnest()` the column of extracted results:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrule_extract <- \n  collect_extracts(concrete_ext_res) %>% \n  unnest(col = .extracts)\nrule_extract\n#> # A tibble: 124 × 7\n#>   id     committee rule_num rule                          estimate statistic .config\n#>   <chr>      <int>    <int> <chr>                         <list>   <list>    <chr>  \n#> 1 Fold01         1        1 ( cement <= 218.89999 ) & ( … <tibble> <tibble>  Prepro…\n#> 2 Fold01         1        2 ( cement <= 218.89999 ) & ( … <tibble> <tibble>  Prepro…\n#> 3 Fold01         1        3 ( water > 183.8 ) & ( cement… <tibble> <tibble>  Prepro…\n#> 4 Fold01         1        4 ( age <= 7 ) & ( water <= 18… <tibble> <tibble>  Prepro…\n#> 5 Fold01         1        5 ( age <= 28 ) & ( age > 7 ) … <tibble> <tibble>  Prepro…\n#> 6 Fold01         1        6 ( age > 28 ) & ( superplasti… <tibble> <tibble>  Prepro…\n#> # ℹ 118 more rows\n```\n:::\n\n\nWhat could we do with these results? Let’s look at what is in the `statistics` tibble: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrule_extract$statistic[[1]]\n#> # A tibble: 1 × 6\n#>   num_conditions coverage  mean   min   max error\n#>            <dbl>    <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1              3       74 22.71   7.4 55.51 4.062\n```\n:::\n\n\nIt might be interesting to know how complex each rule was. For example, the rule\n\n\n::: {.cell layout-align=\"center\"}\n\n```\n#> (water <= 174.8) & (cement > 255.5) & (blast_furnace_slag <= \n#>     139.89999) & (age > 7) & (age <= 56) & (fly_ash <= 126.5)\n```\n:::\n\n\nhas 6 conditions.\n\nWe can use a `mutate()` and a `map_int()` to pull out the frequency of terms included in the rules (captured by the `num_conditions` column). \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconditions <- \n  rule_extract %>% \n  mutate(conditions = map_int(statistic, ~ .x$num_conditions)) %>% \n  select(id, rule_num, conditions)\n```\n:::\n\n\nHere is the distribution of the number of logical conditions that make up the rules: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconditions %>% \n  count(conditions)\n#> # A tibble: 6 × 2\n#>   conditions     n\n#>        <int> <int>\n#> 1          1     7\n#> 2          2    13\n#> 3          3    53\n#> 4          4    36\n#> 5          5    14\n#> 6          6     1\n```\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}