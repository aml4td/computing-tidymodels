{
  "hash": "b581e98f26b0ee5cc4b71e922312feba",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/iterative/\"\n---\n\n# Iterative Search {#sec-iterative-search}\n\n\n\nThis [book chapter](https://tidymodels.aml4td.org/chapters/iterative-search.html) discusses several search procedures for finding optimal (or at least acceptable) tuning parameter values. \n\n## Requirements\n\nThis chapter requires 6 packages (<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=finetune\">finetune</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=future.mirai\">future.mirai</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=GA\">GA</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=probably\">probably</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidymodels\">tidymodels</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=xgboost\">xgboost</a></span>). To install: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreq_pkg <- c(\"finetune\", \"GA\", \"probably\", \"tidymodels\", \"xgboost\")\n\n# Check to see if they are installed: \npkg_installed <- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list <- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\n```\n:::\n\n\n\n\nLet's load the packages and set some preferences:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(GA)\nlibrary(tidymodels)\nlibrary(finetune)\nlibrary(probably)\nlibrary(future.mirai)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\nplan(mirai_multisession)\n```\n:::\n\n\nTo reduce the complexity of the example, we’ll use a simulated classification data set containing numeric predictors. We’ll simulate 1,000 samples using a simulation system, the details of which can be found in the [<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=modeldata\">modeldata</a></span> documentation](https://modeldata.tidymodels.org/reference/sim_classification.html#details). The data set has linear, nonlinear, and interacting features, and the classes are fairly balanced. We’ll use a 3:1 split for training and testing as well as 10-fold cross-validation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(2783)\nsim_dat <- sim_classification(1000)\n\nset.seed(101)\nsim_split <- initial_split(sim_dat)\nsim_train <- training(sim_split)\nsim_test <- testing(sim_split)\nsim_rs <- vfold_cv(sim_train)\n```\n:::\n\n\nWe’ll tune a boosted classification model using the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=xgboost\">xgboost</a></span> package, described in a later chapter. We tune multiple parameters and set an additional parameter, `validation`, to be zero. This is used when early stopping, which we will not use:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbst_spec <-\n  boost_tree(\n    mtry = tune(),\n    tree_depth = tune(),\n    trees = tune(),\n    learn_rate = tune(),\n    min_n = tune(),\n    loss_reduction = tune(),\n    sample_size = tune()\n  ) %>%\n  set_engine('xgboost', validation = 0) %>%\n  set_mode('classification')\n```\n:::\n\n\nTree-based models require little to no preprocessing so we will use a simple R formula to define the roles of variables: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbst_wflow <- workflow(class ~ ., bst_spec)\n```\n:::\n\n\nFrom the workflow, we create a _parameters_ object and set the ranges for two parameters. `mtry` requires an upper bound to be set since it depends on the number of model terms in the data set. We’ll need parameter information since most iterative methods need to know the possible ranges as well as the type of parameter (e.g., integer, character, etc.) and/or any transformations of the values. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbst_param <-\n  bst_wflow %>%\n  extract_parameter_set_dials() %>%\n  update(\n    mtry = mtry(c(3, 15)),\n    trees = trees(c(10, 500))\n  )\n```\n:::\n\n\nWe can now fit and/or tune models. We’ll declare what metrics should be collected and then create a small space-filling design that is used as the starting point for simulated annealing and Bayesian optimization. We _could_ let the system make these initial values for us, but we’ll create them now so that we can reuse the results and have a common starting place. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncls_mtr <- metric_set(brier_class, roc_auc)\n\ninit_grid <- grid_space_filling(bst_param, size = 6)\n\nset.seed(21)\ninitial_res <-\n  bst_wflow %>%\n  tune_grid(\n    resamples = sim_rs,\n    grid = init_grid,\n    metrics = cls_mtr,\n    control = control_grid(save_pred = TRUE)\n  )\n```\n:::\n\n\nFrom these six candidates, the smallest Brier score was 0.117, a mediocre value: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshow_best(initial_res, metric = \"brier_class\") %>% \n  select(-.estimator, -.config, -.metric) %>% \n  relocate(mean)\n#> # A tibble: 5 × 10\n#>     mean  mtry trees min_n tree_depth learn_rate loss_reduction sample_size     n  std_err\n#>    <dbl> <int> <int> <int>      <int>      <dbl>          <dbl>       <dbl> <int>    <dbl>\n#> 1 0.1172    12   402    17          1   0.1           1    e-10        0.46    10 0.005269\n#> 2 0.1201    15   304    24          9   0.03162       3.162e+ 1        1       10 0.004319\n#> 3 0.1504     5   206    32         15   0.3162        1.995e- 8        0.64    10 0.006183\n#> 4 0.1786     3   108     2          3   0.01          7.943e- 4        0.82    10 0.002977\n#> 5 0.2441     7   500     9         12   0.003162      1.585e- 1        0.1     10 0.001748\n```\n:::\n\n\nWe will show how to use three iterative search methods. \n\n## Simulated Annealing  {#sec-sim-anneal}\n\nThe finetune package contains `finetune::tune_sim_anneal()` that can incrementally search the parameter space in a non-greedy way. Its syntax is very similar to `tune_grid()` with two additional arguments of note: \n\n- `initial`: Either: \n\t- An integer that declares how many points in a space-filling design should be created and evaluated before proceeding. \n\t- An object from a previous run of `tune_grid()` or one of the other `tune_*()` functions. \n- `iter`: An integer for the maximum search iterations. \n\nAlso of note is `control_sim_anneal()`, which helps save additional results and controls logging, and if restarts or early stopping should be used. \n\nOne important note: the first metric in the metric set guides the optimization. All of the other metric values are recorded for each iteration but only one is used to improve the model fit.  \n\nHere's some example code: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(381)\nsa_res <-\n  bst_wflow %>%\n  tune_sim_anneal(\n    resamples = sim_rs,\n    param_info = bst_param,\n    metrics = cls_mtr,\n    # Additional options:\n    initial = initial_res,\n    iter = 50,\n    # Prevent early stopping, save out-of-sample predictions, \n    # and log the process to the console: \n    control = control_sim_anneal(\n      no_improve = Inf,\n      verbose_iter = TRUE,\n      save_pred = TRUE\n    )\n  )\n#> Optimizing brier_class\n#> Initial best: 0.11724\n#> 1 ◯ accept suboptimal  brier_class=0.12584 (+/-0.005567)\n#> 2 ♥ new best           brier_class=0.11088 (+/-0.006034)\n#> 3 ◯ accept suboptimal  brier_class=0.12186 (+/-0.006986)\n#> 4 + better suboptimal  brier_class=0.11355 (+/-0.0077)\n#> 5 + better suboptimal  brier_class=0.11204 (+/-0.005044)\n#> 6 ─ discard suboptimal brier_class=0.18772 (+/-0.006055)\n#> 7 ─ discard suboptimal brier_class=0.14709 (+/-0.004402)\n#> 8 ─ discard suboptimal brier_class=0.15341 (+/-0.006266)\n#> 9 ♥ new best           brier_class=0.10271 (+/-0.005368)\n#> 10 ♥ new best           brier_class=0.098399 (+/-0.005912)\n#> 11 ─ discard suboptimal brier_class=0.11041 (+/-0.006706)\n#> 12 ♥ new best           brier_class=0.089953 (+/-0.004929)\n#> 13 ─ discard suboptimal brier_class=0.094707 (+/-0.005409)\n#> 14 ─ discard suboptimal brier_class=0.10833 (+/-0.006363)\n#> 15 ♥ new best           brier_class=0.088551 (+/-0.005869)\n#> 16 ◯ accept suboptimal  brier_class=0.092395 (+/-0.006208)\n#> 17 + better suboptimal  brier_class=0.091379 (+/-0.006153)\n#> 18 ♥ new best           brier_class=0.080126 (+/-0.005732)\n#> 19 ♥ new best           brier_class=0.078878 (+/-0.005375)\n#> 20 ─ discard suboptimal brier_class=0.088738 (+/-0.00475)\n#> 21 ─ discard suboptimal brier_class=0.088662 (+/-0.004205)\n#> 22 ◯ accept suboptimal  brier_class=0.079888 (+/-0.005168)\n#> 23 ─ discard suboptimal brier_class=0.083144 (+/-0.004481)\n#> 24 ◯ accept suboptimal  brier_class=0.080998 (+/-0.005145)\n#> 25 ─ discard suboptimal brier_class=0.089208 (+/-0.00424)\n#> 26 ─ discard suboptimal brier_class=0.083758 (+/-0.004778)\n#> 27 ✖ restart from best  brier_class=0.080229 (+/-0.005623)\n#> 28 ─ discard suboptimal brier_class=0.07948 (+/-0.005617)\n#> 29 ─ discard suboptimal brier_class=0.088518 (+/-0.004385)\n#> 30 ◯ accept suboptimal  brier_class=0.080155 (+/-0.004802)\n#> 31 ─ discard suboptimal brier_class=0.085765 (+/-0.004087)\n#> 32 ─ discard suboptimal brier_class=0.09274 (+/-0.004826)\n#> 33 ◯ accept suboptimal  brier_class=0.082697 (+/-0.004478)\n#> 34 ─ discard suboptimal brier_class=0.1074 (+/-0.004515)\n#> 35 ✖ restart from best  brier_class=0.095289 (+/-0.004477)\n#> 36 ─ discard suboptimal brier_class=0.08412 (+/-0.005034)\n#> 37 ─ discard suboptimal brier_class=0.082724 (+/-0.004939)\n#> 38 ─ discard suboptimal brier_class=0.081199 (+/-0.006138)\n#> 39 ─ discard suboptimal brier_class=0.084084 (+/-0.005246)\n#> 40 ─ discard suboptimal brier_class=0.082772 (+/-0.005205)\n#> 41 ─ discard suboptimal brier_class=0.082996 (+/-0.004645)\n#> 42 ─ discard suboptimal brier_class=0.081317 (+/-0.004638)\n#> 43 ✖ restart from best  brier_class=0.084592 (+/-0.004923)\n#> 44 ─ discard suboptimal brier_class=0.085653 (+/-0.004442)\n#> 45 ─ discard suboptimal brier_class=0.085816 (+/-0.004273)\n#> 46 ─ discard suboptimal brier_class=0.085154 (+/-0.004607)\n#> 47 ◯ accept suboptimal  brier_class=0.084356 (+/-0.004371)\n#> 48 ─ discard suboptimal brier_class=0.10018 (+/-0.004477)\n#> 49 + better suboptimal  brier_class=0.080555 (+/-0.005406)\n#> 50 ─ discard suboptimal brier_class=0.084699 (+/-0.004644)\n```\n:::\n\n\nThe Brier score has been reduced from the initial value of 0.117 to a new best of 0.079. We'll estimate:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshow_best(sa_res, metric = \"brier_class\") %>% \n  select(-.estimator, -.config, -.metric) %>% \n  relocate(mean)\n#> # A tibble: 5 × 11\n#>      mean  mtry trees min_n tree_depth learn_rate loss_reduction sample_size     n\n#>     <dbl> <int> <int> <int>      <int>      <dbl>          <dbl>       <dbl> <int>\n#> 1 0.07888    12   243     2          6    0.02247   0.000001299       0.3711    10\n#> 2 0.07948    11   243     3          8    0.02518   0.0000002770      0.4750    10\n#> 3 0.07989    13   228     2          5    0.01815   0.000009236       0.3659    10\n#> 4 0.08013    11   291     2          6    0.04033   0.000001875       0.4259    10\n#> 5 0.08015    14   278     2          4    0.01628   0.000005399       0.3251    10\n#> # ℹ 2 more variables: std_err <dbl>, .iter <int>\n```\n:::\n\n\nThere are several ways to use `autoplot()` to investigate the results. The default methods plots the metric(s) versus the parameters. Here is it for just the Brier score: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(sa_res, metric = \"brier_class\")\n```\n\n::: {.cell-output-display}\n![](../figures/sa-profile-1.svg){fig-align='center' width=65%}\n:::\n:::\n\n\nNext, we can see how the parameter values change over the search by adding `type = \"parameters\"`: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(sa_res, metric = \"brier_class\", type = \"parameters\")\n```\n\n::: {.cell-output-display}\n![](../figures/sa-history-1.svg){fig-align='center' width=65%}\n:::\n:::\n\n\nFinally, a plot of performance metrics can be used via `type = \"performance\"`: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(sa_res, metric = \"brier_class\", type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](../figures/sa-performance-1.svg){fig-align='center' width=65%}\n:::\n:::\n\n\nIf we had used `control_sim_anneal(save_worflow = TRUE)`, we could use `fit_best()` to determine the candidate with the best metric value and then fit that model to the training set. \n\n<a href=\"https://aml4td.org/chapters/iterative-search.html#sec-sim-anneal\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n## Genetic Algorithms  {#sec-genetic-algo}\n\ntidymodels has no API or function for optimizing models using genetic algorithms. However, there is unsupported code (below) for doing this as long as the tuning parameters are all numeric. We’ll use the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=GA\">GA</a></span> package for the computations, and this will require: \n\n- The upper and lower bounds of the parameters\n- Code to transform the parameter values (if needed)\n- A means to resample/evaluate a model on an out-of-sample data set. \n- A method to compute a single performance metric such that larger values are more desirable.\n\nTo get started, let’s work with the parameter object named `bst_param`. We can use `purrr::map_dbl()` to get vectors of the minimum and maximum values. These should be in the transformed space (if needed):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmin_vals <- map_dbl(bst_param$object, ~ .x$range[[1]])\nmax_vals <- map_dbl(bst_param$object, ~ .x$range[[2]])\n```\n:::\n\n\nThe remainder of the tasks should occur within the GA's processing. This function shows code with comments to help understand: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nyardstick_fitness <- function(values, wflow, param_info, metrics, ...) {\n  # Quietly load required packages if run in parallel\n  shhh <- purrr::quietly(require)\n  loaded <- lapply(c(\"tidymodels\", required_pkgs(wflow)), shhh)\n\n  info <- as_tibble(metrics)\n\n  # Check to see if there are any qualitative parameters and stop if so.\n  qual_check <- map_lgl(param_info$object, ~ inherits(.x, \"qual_param\"))\n  if (any(qual_check)) {\n    cli::cli_abort(\n      \"The function only works for quantitative tuning parameters.\"\n    )\n  }\n\n  # Back-transform parameters if they use a transformation (inputs are in\n  # transformed scales)\n  values <- purrr::map2_dbl(\n    values,\n    param_info$object,\n    ~ dials::value_inverse(.y, .x)\n  )\n\n  # Convert integer parameters to integers\n  is_int <- map_lgl(param_info$object, ~ .x$type == \"integer\")\n  int_param <- param_info$id[is_int]\n  for (i in int_param) {\n    ind <- which(param_info$id == i)\n    values[[ind]] <- floor(values[[ind]])\n  }\n\n  # Convert from vector to a tibble\n  values <- matrix(values, nrow = 1)\n  colnames(values) <- param_info$id\n  values <- as_tibble(values)\n\n  # We could run _populations_ within a generation in parallel. If we do,\n  # let's make sure to turn off parallelization of resamples here:\n  # ctrl <- control_grid(allow_par = FALSE)\n  \n  ctrl <- control_grid()\n\n  # Resample / validate metrics\n  res <- tune_grid(\n    wflow,\n    metrics = metrics,\n    param_info = param_info,\n    grid = values,\n    control = ctrl,\n    ...\n  )\n\n  # Fitness is to be maximized so change direction if needed\n  best_res <- show_best(res, metric = info$metric[1])\n  if (info$direction[1] == \"minimize\") {\n    obj_value <- -best_res$mean\n  } else {\n    obj_value <- best_res$mean\n  }\n  obj_value\n}\n```\n:::\n\n\nNow, let's initialize the search using a space-filling design (with 10 candidates per population): \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop_size <- 10\ngrid_ga <- grid_space_filling(bst_param, size = pop_size, original = FALSE)\n\n# We apply the GA operators on the transformed scale of the parameters (if any).\n# For this example, two use a log-transform: \ngrid_ga$learn_rate <- log10(grid_ga$learn_rate)\ngrid_ga$loss_reduction <- log10(grid_ga$loss_reduction)\n```\n:::\n\n\nNow we can run `GA::ga()` to begin the process: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(158)\nga_res <-\n  ga(\n    # ga() options:\n    type = \"real-valued\",\n    fitness = yardstick_fitness,\n    lower = min_vals,\n    upper = max_vals,\n    popSize = pop_size,\n    suggestions = as.matrix(grid_ga),\n    maxiter = 25,\n    # Save the best solutions at each iteration\n    keepBest = TRUE,\n    seed = 39,\n    # Here we can signal to run _populations_ within a generation in parallel\n    parallel = FALSE,\n    # Now options to pass to the `...` in yardstick_fitness()\n    wflow = bst_wflow,\n    param_info = bst_param,\n    metrics = cls_mtr,\n    resamples = sim_rs\n  )\n```\n:::\n\n\nHere is a plot of the best results per population and the mean result (both are Brier scores): \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Negate the fitness value since the Brier score should be minimized.\n-attr(ga_res,\"summary\") %>% \n  as_tibble() %>% \n  mutate(generation = row_number()) %>% \n  select(best = max, mean = mean, generation) %>% \n  pivot_longer(c(best, mean), names_to = \"summary\", values_to = \"fitness\") %>% \n  ggplot(aes(generation, fitness, col = summary, pch = summary)) + \n  geom_point() + \n  labs(x = \"Generation\", y = \"Brier Score (CV)\")\n```\n\n::: {.cell-output-display}\n![](../figures/ga-profile-1.svg){fig-align='center' width=65%}\n:::\n:::\n\n\nThe best results are in a slot called `solution`. Let's remap that to the original parameter values: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nga_best <- \n  # There could be multiple solutions for the same fitness; we take the first. \n  ga_res@solution[1,] %>% \n  # Back-transform\n  map2(bst_param$object, ~ value_inverse(.y, .x)) %>% \n  as_tibble() %>% \n  set_names(bst_param$id) %>% \n  # Attach fitness and coerce to integer if needed.\n  mutate(\n    mtry = floor(mtry),\n    trees = floor(trees),\n    min_n = floor(min_n),\n    tree_depth = floor(tree_depth),\n    brier = -ga_res@fitnessValue\n  ) %>% \n  relocate(brier)\n\nga_best\n#> # A tibble: 1 × 8\n#>     brier  mtry trees min_n tree_depth learn_rate loss_reduction sample_size\n#>     <dbl> <dbl> <dbl> <dbl>      <dbl>      <dbl>          <dbl>       <dbl>\n#> 1 0.07793     9   410     2          6    0.01235   0.0000001197      0.6159\n```\n:::\n\n\n<a href=\"https://aml4td.org/chapters/iterative-search.html#sec-genetic-algo\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n## Bayesian Optimization {#sec-bayes-opt}\n\nNumerous packages use Bayesian optimization: \n\n- [<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=BayesGP\">BayesGP</a></span>](https://cran.r-project.org/package=BayesGP)\n- [<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=BayesGPfit\">BayesGPfit</a></span>](https://cran.r-project.org/package=BayesGPfit)\n- [<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=FastGP\">FastGP</a></span>](https://cran.r-project.org/package=FastGP)\n- [<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=GauPro\">GauPro</a></span>](https://github.com/CollinErickson/GauPro)\n- [<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=GPBayes\">GPBayes</a></span>](https://github.com/pulongma/GPBayes)\n- [<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=GPfit\">GPfit</a></span>](https://cran.r-project.org/package=GPfit)\n\nand many others. The book [_Gaussian process modeling, design and optimization for the applied sciences_](https://bookdown.org/rbg/surrogates/) also contains descriptions of many other GO packages. \n\nCurrently, tidymodels uses <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=GPfit\">GPfit</a></span>.\n\nThe <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tune\">tune</a></span> package contains `tune_bayes()` for Bayesian optimization. The syntax is identical to what we've already seen with `tune_sim_anneal()`. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(221)\nbo_res <- bst_wflow %>%\n  tune_bayes(\n    resamples = sim_rs,\n    param_info = bst_param,\n    metrics = cls_mtr,\n    # These options work as before: \n    initial = initial_res,\n    iter = 50,\n    control = control_bayes(\n      no_improve = Inf,\n      verbose_iter = TRUE,\n      save_pred = TRUE,\n    )\n  )\n#> ! There are 7 tuning parameters and 6 grid points were requested.\n#> • There are more tuning parameters than there are initial points. This is likely to\n#>   cause numerical issues in the first few search iterations.\n#> Optimizing brier_class using the expected improvement\n#> \n#> ── Iteration 1 ──────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.1172 (@iter 0)\n#> i Gaussian process model\n#> ! The Gaussian process model is being fit using 7 features but only has 6 data points\n#>   to do so. This may cause errors or a poor model fit.\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=5, trees=489, min_n=26, tree_depth=14, learn_rate=0.0582,\n#>   loss_reduction=9.86e-09, sample_size=0.114\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.2499 (+/-0.000157)\n#> \n#> ── Iteration 2 ──────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.1172 (@iter 0)\n#> i Gaussian process model\n#> ! The Gaussian process model is being fit using 7 features but only has 7 data points\n#>   to do so. This may cause errors or a poor model fit.\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=3, trees=96, min_n=39, tree_depth=14, learn_rate=0.0768,\n#>   loss_reduction=2.53e-07, sample_size=0.513\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.2189 (+/-0.00405)\n#> \n#> ── Iteration 3 ──────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.1172 (@iter 0)\n#> i Gaussian process model\n#> ! The Gaussian process model is being fit using 7 features but only has 8 data points\n#>   to do so. This may cause errors or a poor model fit.\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=5, trees=286, min_n=7, tree_depth=1, learn_rate=0.0278, loss_reduction=4.26e-05,\n#>   sample_size=0.432\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:\tbrier_class=0.1153 (+/-0.00444)\n#> \n#> ── Iteration 4 ──────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.1153 (@iter 3)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=7, trees=338, min_n=24, tree_depth=1, learn_rate=0.00178, loss_reduction=0.108,\n#>   sample_size=0.702\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.2166 (+/-0.00133)\n#> \n#> ── Iteration 5 ──────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.1153 (@iter 3)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=13, trees=384, min_n=28, tree_depth=11, learn_rate=0.254,\n#>   loss_reduction=0.000924, sample_size=0.936\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.1163 (+/-0.00601)\n#> \n#> ── Iteration 6 ──────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.1153 (@iter 3)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=7, trees=208, min_n=30, tree_depth=15, learn_rate=0.0206, loss_reduction=3.24,\n#>   sample_size=0.81\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.1245 (+/-0.00436)\n#> \n#> ── Iteration 7 ──────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.1153 (@iter 3)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=15, trees=229, min_n=6, tree_depth=15, learn_rate=0.266, loss_reduction=2.85,\n#>   sample_size=0.982\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:\tbrier_class=0.08313 (+/-0.00674)\n#> \n#> ── Iteration 8 ──────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.08313 (@iter 7)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=9, trees=296, min_n=12, tree_depth=15, learn_rate=0.176,\n#>   loss_reduction=9.48e-08, sample_size=0.999\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.09177 (+/-0.00532)\n#> \n#> ── Iteration 9 ──────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.08313 (@iter 7)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=10, trees=199, min_n=3, tree_depth=14, learn_rate=0.0622, loss_reduction=0.125,\n#>   sample_size=0.959\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:\tbrier_class=0.0824 (+/-0.00654)\n#> \n#> ── Iteration 10 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.0824 (@iter 9)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=3, trees=48, min_n=6, tree_depth=15, learn_rate=0.0012, loss_reduction=9.11e-10,\n#>   sample_size=0.934\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.2413 (+/-0.000398)\n#> \n#> ── Iteration 11 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.0824 (@iter 9)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=12, trees=255, min_n=4, tree_depth=8, learn_rate=0.164, loss_reduction=0.00354,\n#>   sample_size=0.352\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.0903 (+/-0.00684)\n#> \n#> ── Iteration 12 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.0824 (@iter 9)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=14, trees=205, min_n=3, tree_depth=14, learn_rate=0.0699,\n#>   loss_reduction=7.86e-05, sample_size=0.977\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:\tbrier_class=0.08224 (+/-0.00684)\n#> \n#> ── Iteration 13 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.08224 (@iter 12)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=8, trees=357, min_n=5, tree_depth=5, learn_rate=0.308, loss_reduction=4.44e-10,\n#>   sample_size=0.385\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.1102 (+/-0.00737)\n#> \n#> ── Iteration 14 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.08224 (@iter 12)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=15, trees=220, min_n=9, tree_depth=2, learn_rate=0.113, loss_reduction=5.26,\n#>   sample_size=0.405\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.09564 (+/-0.00507)\n#> \n#> ── Iteration 15 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.08224 (@iter 12)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=7, trees=249, min_n=4, tree_depth=8, learn_rate=0.134, loss_reduction=1.86,\n#>   sample_size=0.854\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:\tbrier_class=0.07948 (+/-0.00598)\n#> \n#> ── Iteration 16 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=11, trees=249, min_n=3, tree_depth=6, learn_rate=0.0989,\n#>   loss_reduction=2.55e-10, sample_size=0.896\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08295 (+/-0.00608)\n#> \n#> ── Iteration 17 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=11, trees=244, min_n=4, tree_depth=6, learn_rate=0.0662, loss_reduction=13.2,\n#>   sample_size=0.976\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08762 (+/-0.00476)\n#> \n#> ── Iteration 18 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=12, trees=238, min_n=3, tree_depth=1, learn_rate=0.166, loss_reduction=9.55e-07,\n#>   sample_size=0.966\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08199 (+/-0.00546)\n#> \n#> ── Iteration 19 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=6, trees=239, min_n=5, tree_depth=14, learn_rate=0.188, loss_reduction=3.6e-09,\n#>   sample_size=0.975\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08804 (+/-0.00716)\n#> \n#> ── Iteration 20 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=10, trees=205, min_n=3, tree_depth=2, learn_rate=0.118, loss_reduction=0.37,\n#>   sample_size=0.845\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08471 (+/-0.00557)\n#> \n#> ── Iteration 21 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=13, trees=326, min_n=2, tree_depth=14, learn_rate=0.107, loss_reduction=0.348,\n#>   sample_size=0.822\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08266 (+/-0.00683)\n#> \n#> ── Iteration 22 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=11, trees=285, min_n=4, tree_depth=2, learn_rate=0.164, loss_reduction=7.6,\n#>   sample_size=0.966\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08542 (+/-0.0049)\n#> \n#> ── Iteration 23 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=7, trees=251, min_n=3, tree_depth=2, learn_rate=0.0766, loss_reduction=7.89e-09,\n#>   sample_size=0.919\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08063 (+/-0.00555)\n#> \n#> ── Iteration 24 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=8, trees=213, min_n=2, tree_depth=14, learn_rate=0.128, loss_reduction=2.15e-10,\n#>   sample_size=0.831\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08654 (+/-0.00632)\n#> \n#> ── Iteration 25 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=15, trees=357, min_n=4, tree_depth=8, learn_rate=0.177, loss_reduction=0.00166,\n#>   sample_size=0.901\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08988 (+/-0.00743)\n#> \n#> ── Iteration 26 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=15, trees=203, min_n=2, tree_depth=14, learn_rate=0.183,\n#>   loss_reduction=3.94e-08, sample_size=0.922\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08778 (+/-0.00755)\n#> \n#> ── Iteration 27 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=3, trees=342, min_n=4, tree_depth=2, learn_rate=0.0774, loss_reduction=1.98,\n#>   sample_size=0.748\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08631 (+/-0.00513)\n#> \n#> ── Iteration 28 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=11, trees=327, min_n=7, tree_depth=14, learn_rate=0.103,\n#>   loss_reduction=5.45e-10, sample_size=0.979\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08946 (+/-0.00573)\n#> \n#> ── Iteration 29 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=14, trees=266, min_n=2, tree_depth=6, learn_rate=0.136, loss_reduction=0.000997,\n#>   sample_size=0.853\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08579 (+/-0.00728)\n#> \n#> ── Iteration 30 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=6, trees=317, min_n=9, tree_depth=5, learn_rate=0.13, loss_reduction=1.5,\n#>   sample_size=0.914\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08558 (+/-0.00593)\n#> \n#> ── Iteration 31 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=15, trees=356, min_n=21, tree_depth=7, learn_rate=0.145, loss_reduction=0.122,\n#>   sample_size=0.222\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.2315 (+/-0.0046)\n#> \n#> ── Iteration 32 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=3, trees=29, min_n=5, tree_depth=13, learn_rate=0.13, loss_reduction=18.1,\n#>   sample_size=0.672\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.1201 (+/-0.00443)\n#> \n#> ── Iteration 33 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=4, trees=394, min_n=15, tree_depth=8, learn_rate=0.0998,\n#>   loss_reduction=5.21e-10, sample_size=0.804\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.0944 (+/-0.00564)\n#> \n#> ── Iteration 34 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=13, trees=460, min_n=3, tree_depth=13, learn_rate=0.0819, loss_reduction=0.113,\n#>   sample_size=0.464\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08671 (+/-0.00593)\n#> \n#> ── Iteration 35 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=12, trees=290, min_n=4, tree_depth=15, learn_rate=0.137, loss_reduction=14,\n#>   sample_size=0.542\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.09203 (+/-0.00477)\n#> \n#> ── Iteration 36 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=14, trees=99, min_n=15, tree_depth=12, learn_rate=0.11, loss_reduction=2.86e-09,\n#>   sample_size=0.967\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08659 (+/-0.00509)\n#> \n#> ── Iteration 37 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=9, trees=268, min_n=3, tree_depth=6, learn_rate=0.109, loss_reduction=0.297,\n#>   sample_size=0.437\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08782 (+/-0.00678)\n#> \n#> ── Iteration 38 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=13, trees=34, min_n=2, tree_depth=1, learn_rate=0.143, loss_reduction=0.0201,\n#>   sample_size=0.995\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.1141 (+/-0.00432)\n#> \n#> ── Iteration 39 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=10, trees=225, min_n=13, tree_depth=11, learn_rate=0.0965,\n#>   loss_reduction=3.44e-06, sample_size=0.925\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08963 (+/-0.00498)\n#> \n#> ── Iteration 40 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=9, trees=17, min_n=2, tree_depth=13, learn_rate=0.146, loss_reduction=0.026,\n#>   sample_size=0.359\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.09176 (+/-0.00485)\n#> \n#> ── Iteration 41 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=13, trees=157, min_n=17, tree_depth=11, learn_rate=0.314,\n#>   loss_reduction=6.95e-08, sample_size=0.957\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.09725 (+/-0.00662)\n#> \n#> ── Iteration 42 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=7, trees=313, min_n=2, tree_depth=6, learn_rate=0.127, loss_reduction=0.109,\n#>   sample_size=0.813\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.09222 (+/-0.00767)\n#> \n#> ── Iteration 43 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=12, trees=493, min_n=12, tree_depth=13, learn_rate=0.0622,\n#>   loss_reduction=0.000261, sample_size=0.637\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.094 (+/-0.00595)\n#> \n#> ── Iteration 44 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=7, trees=18, min_n=32, tree_depth=13, learn_rate=0.089, loss_reduction=8.1e-09,\n#>   sample_size=0.995\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.1396 (+/-0.00449)\n#> \n#> ── Iteration 45 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=7, trees=187, min_n=9, tree_depth=15, learn_rate=0.114, loss_reduction=0.00238,\n#>   sample_size=0.868\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08754 (+/-0.00559)\n#> \n#> ── Iteration 46 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=10, trees=11, min_n=3, tree_depth=4, learn_rate=0.197, loss_reduction=3.75e-10,\n#>   sample_size=0.137\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.1147 (+/-0.00564)\n#> \n#> ── Iteration 47 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=9, trees=432, min_n=6, tree_depth=9, learn_rate=0.0675, loss_reduction=6.32e-09,\n#>   sample_size=0.521\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08992 (+/-0.00517)\n#> \n#> ── Iteration 48 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=15, trees=443, min_n=4, tree_depth=15, learn_rate=0.0621, loss_reduction=0.682,\n#>   sample_size=0.838\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.08458 (+/-0.0066)\n#> \n#> ── Iteration 49 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=8, trees=482, min_n=9, tree_depth=2, learn_rate=0.0937, loss_reduction=0.00332,\n#>   sample_size=0.534\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.09804 (+/-0.00584)\n#> \n#> ── Iteration 50 ─────────────────────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tbrier_class=0.07948 (@iter 15)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i mtry=15, trees=71, min_n=21, tree_depth=3, learn_rate=0.064, loss_reduction=1.1e-10,\n#>   sample_size=0.858\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tbrier_class=0.1003 (+/-0.00459)\n```\n:::\n\n\nThe same helper functions are used to interrogate the results and to create diagnostic plots: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshow_best(bo_res, metric = \"brier_class\") %>% \n  select(-.estimator, -.config, -.metric) %>% \n  relocate(mean)\n#> # A tibble: 5 × 11\n#>      mean  mtry trees min_n tree_depth learn_rate loss_reduction sample_size     n\n#>     <dbl> <int> <int> <int>      <int>      <dbl>          <dbl>       <dbl> <int>\n#> 1 0.07948     7   249     4          8    0.1343        1.859e+0      0.8543    10\n#> 2 0.08063     7   251     3          2    0.07663       7.895e-9      0.9194    10\n#> 3 0.08199    12   238     3          1    0.1658        9.551e-7      0.9661    10\n#> 4 0.08224    14   205     3         14    0.06994       7.861e-5      0.9767    10\n#> 5 0.08240    10   199     3         14    0.06220       1.247e-1      0.9593    10\n#> # ℹ 2 more variables: std_err <dbl>, .iter <int>\n```\n:::\n\n\nThese results are about the same as those of the SA search. We can plot the data and see that some parameters (number of trees, learning rate, minimum node size, and the sampling proportion) appear to converge to specific values: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(bo_res, metric = \"brier_class\")\n```\n\n::: {.cell-output-display}\n![](../figures/bo-profile-1.svg){fig-align='center' width=65%}\n:::\n:::\n\n\nHere we see that the learning rate and the minumum node size reach a steady-state: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(bo_res, metric = \"brier_class\", type = \"parameters\")\n```\n\n::: {.cell-output-display}\n![](../figures/bo-history-1.svg){fig-align='center' width=65%}\n:::\n:::\n\n\nA plot of the overall progress: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(bo_res, metric = \"brier_class\", type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](../figures/bo-performance-1.svg){fig-align='center' width=65%}\n:::\n:::\n\n\nOther packages use Bayesian optimization: \n\n- [<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=mlr3mbo\">mlr3mbo</a></span>](https://mlr3mbo.mlr-org.com/)\n- [<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=mlrMBO\">mlrMBO</a></span>](https://mlrmbo.mlr-org.com/)\n- [<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=ParBayesianOptimization\">ParBayesianOptimization</a></span>](https://github.com/AnotherSamWilson/ParBayesianOptimization)\n- [<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=rBayesianOptimization\">rBayesianOptimization</a></span>](https://github.com/yanyachen/rBayesianOptimization)\n\n<a href=\"https://aml4td.org/chapters/iterative-search.html#sec-bayes-opt\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}