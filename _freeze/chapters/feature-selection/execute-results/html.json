{
  "hash": "428d0b7206a1b1544f96b3cf8f32ab13",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/feature-selection/\"\n---\n\n# Feature Selection {#sec-feature-selection}\n\n\n\nThe book’s [_Feature Selection_](https://aml4td.org/chapters/feature-selection.html) chapter focuses on different methods to reduce the number of predictors used by the model\n\n## Requirements\n\nWe will use the following package in this chapter: You’ll need 9 packages (<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=bestNormalize\">bestNormalize</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=future.mirai\">future.mirai</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=janitor\">janitor</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=kernlab\">kernlab</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=partykit\">partykit</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=QSARdata\">QSARdata</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=ranger\">ranger</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=rpart\">rpart</a></span>, and <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidymodels\">tidymodels</a></span>) for this chapter. \nYou can install them via:. To install them:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreq_pkg <- c(\"bestNormalize\", \"future.mirai\", \"janitor\", \"kernlab\", \"partykit\", \n             \"QSARdata\", \"ranger\", \"rpart\", \"tidymodels\")\n\n# Check to see if they are installed: \npkg_installed <- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list <- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\n\n# For coliono, install from GitHub\npak::pak(\"stevenpawley/colino\")\n```\n:::\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nWe'll demonstrate these tools using the [phospholipidosis](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22Drug-induced+phospholipidosis%22&btnG=) (PLD) data set from the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=QSARdata\">QSARdata</a></span> package. These data are an example of a data set used in drug discovery to help predict when compounds have unwanted side effects (e.g., toxicity).\n\nWe'll consider two predictor sets. One consists of a series of descriptors of molecules (such as size or weight) and a set of predictors that are binary indicators for important sub-structures in the equations that make up the molecules. The response has two classes (\"clean\" and toxic outcomes).\n\nEven before initial splitting, there are more predictors (1345) than data points (324), making feature selection an essential task for these data. \n\nLet's load the meta package, manage some between-package function conflicts, and initialize parallel processing:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(colino)\nlibrary(bestNormalize)\nlibrary(future.mirai)\nlibrary(janitor)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\nplan(mirai_multisession)\n```\n:::\n\n\nThere are a few different data frames for these data. We'll merge two and clean up and shorten the variable names, use a 4:1 split of the data, and then initialize multiple repeats of 10-fold cross-validation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(PLD, package = \"QSARdata\")\n\ndrug_data <- \n  # Merge the outcome data with a subset of possible predictors.\n  PLD_Outcome %>% \n  full_join(PLD_VolSurfPlus, by = \"Molecule\") %>% \n  full_join(PLD_PipelinePilot_FP %>% select(1, contains(\"FCFP\")), \n            by = \"Molecule\") %>% \n  select(-Molecule) %>% \n  clean_names() %>% \n  as_tibble() %>% \n  # Make shorter names:\n  rename_with(~ gsub(\"vol_surf_plus_\", \"\", .x), everything()) %>% \n  rename_with(~ gsub(\"ppfp_fcfp_\", \"fp_\", .x), everything())\n\nset.seed(106)\ndrug_split <- initial_split(drug_data, prop = 0.8, strata = class)\ndrug_train <- training(drug_split)\ndrug_test <- testing(drug_split)\ndrug_rs <- vfold_cv(drug_train, repeats = 10, strata = class)\n```\n:::\n\n\nThere is a class imbalance for these data: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndim(drug_train)\n#> [1]  259 1346\ndrug_train %>% count(class)\n#> # A tibble: 2 × 2\n#>   class          n\n#>   <fct>      <int>\n#> 1 inducer       99\n#> 2 noninducer   160\n```\n:::\n\n\nThe level \"inducer\" indicates that the molecule has been proven to cause phospholipidosis.\n\n## Unsupervised Selection {#sec-unsupervised-selection}\n\nIn tidymodels, most preprocessing methods for feature selection are recipe steps. \n\nThere are several unsupervised feature filters in <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=recipes\">recipes</a></span>: \n\n- `step_zv()`: Removes predictors with a single value. \n- `step_nzv(freq_cut = double(1), unique_cut = double(1))`: Removes predictors that have a few unique values that are out of the mainstream.\n- `step_corr(threshold = double(1))`: Reduces the pairwise correlations between predictors. \n- `step_lincomb()`: Eliminates strict linear dependencies between predictors. \n- `step_filter_missing(threshold = double(1))`: Remove predictors that have too many missing values.\n\nThe <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=textrecipes\">textrecipes</a></span> package also has a few unsupervised methods for screening tokens (i.e., words): \n\n- `step_tokenfilter()`: filter tokens based on term frequency. \n- `step_stopwords()`: Removes top words (e.g., \"and\", \"or\", etc.)\n- `step_pos_filter()`: Part of speech filtering of tokens.\n \nIt is suggested that these steps occur early in a recipe, perhaps after any imputation methods.\n \nWe suggest adding these to a recipe early, after any imputation methods. \n\nFor the drug toxicity data, we can visualize the amount of predictor correlations by first computing the correlation matrix: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncor_mat <- \n  drug_train %>% \n  select(-class) %>% \n  cor()\n#> Warning in cor(.): the standard deviation is zero\n```\n:::\n\n\nNote the warning: some columns have a single unique value. Let's look at this across the entire set of predictors via a function in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=vctrs\">vctrs</a></span> package: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnum_unique <- map_int(drug_train %>% select(-class), vctrs::vec_unique_count)\nnames(num_unique)[num_unique == 1]\n#>  [1] \"fp_0167\" \"fp_0172\" \"fp_0174\" \"fp_0178\" \"fp_0181\" \"fp_0182\" \"fp_0183\" \"fp_0191\"\n#>  [9] \"fp_0193\" \"fp_0195\" \"fp_0198\" \"fp_0200\" \"fp_0204\" \"fp_0205\" \"fp_0206\" \"fp_0212\"\n```\n:::\n\n\nWe’ll start a recipe with the step that will eliminate these: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndrug_rec <- \n  recipe(class ~ ., data = drug_train) %>% \n  step_zv(all_predictors())\n```\n:::\n\n\nReturning to correlations, let's plot the distribution of pairwise correlation between predictors: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npairwise_cor <- cor_mat[upper.tri(cor_mat)]\n\nsum(abs(pairwise_cor) >= 3 / 4, na.rm = TRUE)\n#> [1] 3922\n\ntibble(correlation = pairwise_cor) %>% \n  filter(!is.na(correlation)) %>% \n  ggplot(aes(x = correlation)) + \n  geom_histogram(binwidth = 0.1, col = \"white\")\n```\n\n::: {.cell-output-display}\n![](../figures/cor-dist-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\nThis isn't too bad, but if we wanted to reduce the extreme pairwise correlations, we could use: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndrug_rec %>% \n  step_corr(all_predictors(), threshold = 0.75)\n#> \n#> ── Recipe ───────────────────────────────────────────────────────────────────────────\n#> \n#> ── Inputs\n#> Number of variables by role\n#> outcome:      1\n#> predictor: 1345\n#> \n#> ── Operations\n#> • Zero variance filter on: all_predictors()\n#> • Correlation filter on: all_predictors()\n```\n:::\n\n\nor search for an optimal cutoff using `threshold = tune()` (as we will below). \n\n<a href=\"https://aml4td.org/chapters/feature-selection.html#sec-unsupervised-selection\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n## Automatic Selection {#sec-automatic-selection}\n\nThe text mentioned that there are types of models that automatically select predictors. Tree-based models typically fall into this category. \n\nTo demonstrate, let’s fit a Classification and Regression Tree ([CART](https://www.nature.com/articles/nmeth.4370)) to the training set and see how many predictors are removed. \n\nBefore doing so, let’s _turn off_ a feature of this model. CART computes special alternate splits during training (\"surrogate\" and \"competing\" splits) to aid with things like missing value imputation. We’ll use the built-in feature importance measure to see how many predictors were used. Unfortunately, those measures will include splits not actually used by the model, so we prohibit these from being listed using `rpart.control()`. \n\nWe can pass that to the model fit when we set the engine: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncart_ctrl <- rpart::rpart.control(maxcompete = 0, maxsurrogate = 0)\n\ncart_spec <- \n  decision_tree(mode = \"classification\") %>% \n  set_engine(\"rpart\", control = !!cart_ctrl)\n```\n:::\n\n\n**Note** the use of `!!` (\"bang-bang\") when adding `cart_ctrl` as an engine option. If we had just used `control = cart_ctrl`, it tells R to look for a reference to object \"`cart_ctrl`\", which resides in the global environment. Ordinarily, that works fine. However, if we use parallel processing, that reference is not available to the worker processes, and an error will occur. \n\nUsing the bang-bang operator, we replace the _reference_ to \"`cart_ctrl`\" with the actual value of that object. It splices the actual data into the model specification so parallel workers can find it. \nHere’s the model fit: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncart_drug_fit <- cart_spec %>% fit(class ~ ., data = drug_train)\ncart_drug_fit\n#> parsnip model object\n#> \n#> n= 259 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>  1) root 259 99 noninducer (0.38224 0.61776)  \n#>    2) l1lg_s< -2.105 103 30 inducer (0.70874 0.29126)  \n#>      4) logp_c_hex>=1.023 70 10 inducer (0.85714 0.14286)  \n#>        8) cw7< 0.01674 41  1 inducer (0.97561 0.02439) *\n#>        9) cw7>=0.01674 29  9 inducer (0.68966 0.31034)  \n#>         18) w6>=33.5 13  0 inducer (1.00000 0.00000) *\n#>         19) w6< 33.5 16  7 noninducer (0.43750 0.56250) *\n#>      5) logp_c_hex< 1.023 33 13 noninducer (0.39394 0.60606)  \n#>       10) fp_0048>=0.5 9  0 inducer (1.00000 0.00000) *\n#>       11) fp_0048< 0.5 24  4 noninducer (0.16667 0.83333)  \n#>         22) d8>=4.5 7  3 inducer (0.57143 0.42857) *\n#>         23) d8< 4.5 17  0 noninducer (0.00000 1.00000) *\n#>    3) l1lg_s>=-2.105 156 26 noninducer (0.16667 0.83333)  \n#>      6) cw7< 0.002635 9  3 inducer (0.66667 0.33333) *\n#>      7) cw7>=0.002635 147 20 noninducer (0.13605 0.86395)  \n#>       14) vd>=0.4691 8  3 inducer (0.62500 0.37500) *\n#>       15) vd< 0.4691 139 15 noninducer (0.10791 0.89209) *\n```\n:::\n\n\nOf the 1345 predictors, only 7 were actually part of the prediction equations. The <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=partykit\">partykit</a></span> package has a nice plot method to visualize the tree:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(partykit)\n\ncart_drug_party <- \n  cart_drug_fit %>% \n  extract_fit_engine() %>% \n  as.party()\nplot(cart_drug_party)\n```\n\n::: {.cell-output-display}\n![](../figures/cart-plot-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\nAs previously mentioned, trees produced by the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=rpart\">rpart</a></span> package have an internal importance score. To return this, let’s write a small function to pull the `rpart` object out, extract the importance scores, and then return a data frame with that data: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_active_features <- function(x) {\n  require(tidymodels)\n  x %>% \n    extract_fit_engine() %>% \n    pluck(\"variable.importance\") %>% \n    enframe() %>% \n    setNames(c(\"predictor\", \"importance\"))\n}\n\nget_active_features(cart_drug_fit) \n#> # A tibble: 7 × 2\n#>   predictor  importance\n#>   <chr>           <dbl>\n#> 1 l1lg_s         36.46 \n#> 2 logp_c_hex      9.624\n#> 3 fp_0048         9.091\n#> 4 cw7             7.553\n#> 5 w6              4.539\n#> 6 vd              4.045\n#> # ℹ 1 more row\n```\n:::\n\n\nThis shows us the 7 predictors used, along with their relative effect on the model. \n\nThese results show what happens with the training set, but would a predictor like `l1lg_s` be consistently selected? \n\nTo determine this, we can resample the model and save the importance scores for each of the 100 analysis sets. Let’s take the `get_active_features()` function and add it to a different `control` function that will be executed during resampling: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nctrl <- control_resamples(extract = get_active_features)\ncart_drug_res <- \n  cart_spec %>% \n  fit_resamples(\n    class ~ ., \n    resamples = drug_rs, \n    control = ctrl\n  )\n```\n:::\n\n\nOur results will have an extra column called `.extract` that contains the results for the resample. Since we didn’t tune this model, `.extract` contains a simple tibble with the results: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncart_drug_res$.extracts[[1]]\n#> # A tibble: 1 × 2\n#>   .extracts        .config             \n#>   <list>           <chr>               \n#> 1 <tibble [7 × 2]> Preprocessor1_Model1\n\ncart_drug_res$.extracts[[1]]$.extracts\n#> [[1]]\n#> # A tibble: 7 × 2\n#>   predictor  importance\n#>   <chr>           <dbl>\n#> 1 vd             35.40 \n#> 2 logp_c_hex      9.001\n#> 3 l1lg_s          7.934\n#> 4 aus74           3.75 \n#> 5 cw1             3.438\n#> 6 fp_0027         3.048\n#> # ℹ 1 more row\n```\n:::\n\n\nWe can extract the results from all the resamples, unnest, and count the number of times each predictor was selected: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampled_selection <- \n  cart_drug_res %>% \n  collect_extracts() %>% \n  unnest(.extracts) %>% \n  count(predictor) %>%\n  arrange(desc(n))\n\nresampled_selection %>% slice_head(n = 5)\n#> # A tibble: 5 × 2\n#>   predictor      n\n#>   <chr>      <int>\n#> 1 l1lg_s        86\n#> 2 vd            58\n#> 3 logp_c_hex    57\n#> 4 cw7           44\n#> 5 iw4           43\n```\n:::\n\n\nA visualization illustrates that a small number of predictors were reliably selected:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampled_selection %>% \n  ggplot(aes(n)) +\n  geom_histogram(binwidth = 2, col = \"white\") +\n  labs(x = \"# Times Selected (of 100)\")\n```\n\n::: {.cell-output-display}\n![](../figures/cart-freq-1.svg){fig-align='center' width=70%}\n:::\n:::\n\n\nWe can also see the model's performance characteristics: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(cart_drug_res)\n#> # A tibble: 3 × 6\n#>   .metric     .estimator   mean     n  std_err .config             \n#>   <chr>       <chr>       <dbl> <int>    <dbl> <chr>               \n#> 1 accuracy    binary     0.7406   100 0.007049 Preprocessor1_Model1\n#> 2 brier_class binary     0.2001   100 0.005307 Preprocessor1_Model1\n#> 3 roc_auc     binary     0.7559   100 0.008105 Preprocessor1_Model1\n```\n:::\n\n\nOne additional note about using tree-based models to automatically select predictors. Many tree ensembles create a collection of individual tree models. For ensembles to work well, this collection should have a diverse set of trees (rather than those with the same splits). To encourage diversity, many tree models have an `mtry` parameter. This parameter is an integer for the number of predictors in the data set that should be randomly selected when making a split. For example, if `mtry = 3`, a different random selection of three predictors would be the only ones considered for each split in the tree. This facilitates diversity but also forces irrelevant predictors to be included in the model. \n\nHowever, this also means that many tree ensembles will have prediction functions that include predictors that have no effect. If we take the same strategy as above, we will vastly overestimate the number of predictors that affect the model. \n\nFor this reason, we might consider setting `mtry` to use the complete predictor set during splitting _if we are trying to select predictors_. While this might slightly decrease the model’s performance, the false positive rate of finding \"important predictors\" will be significantly reduced. \n\n<a href=\"https://aml4td.org/chapters/feature-selection.html#sec-automatic-selection\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n## Wrapper Methods  {#sec-wrappers}\n\ntidymodels does not contain any wrapper methods, primarily due to their computational costs. \n\nSeveral other packages do, most notably <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=caret\">caret</a></span>. For more information on what that package can do, see the feature selection chapters of the documentation: \n\n - [_Feature Selection Overview_](https://topepo.github.io/caret/feature-selection-overview.html)\n - [_Feature Selection using Univariate Filters_](https://topepo.github.io/caret/feature-selection-using-univariate-filters.html)\n - [_Recursive Feature Elimination_](https://topepo.github.io/caret/recursive-feature-elimination.html)\n - [_Feature Selection using Genetic Algorithms_](https://topepo.github.io/caret/feature-selection-using-genetic-algorithms.html)\n - [_Feature Selection using Simulated Annealing_](https://topepo.github.io/caret/feature-selection-using-simulated-annealing.html)\n\nR code from the Feature Engineering and Selection book can also be found at [`https://github.com/topepo/FES`](https://github.com/topepo/FES).\n\n<a href=\"https://aml4td.org/chapters/feature-selection.html#sec-wrappers\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n## Filter Methods {#sec-filters}\n\nCurrently, the majority of supervised filters live in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=colino\">colino</a></span> package (although this will change in the Autumn of 2025). Those steps include: \n\n- `step_select_aov()`: filter categorical predictors using the ANOVA F-test.\n- `step_select_boruta()`: feature selection step using the Boruta algorithm [(pdf)](https://www.jmlr.org/papers/volume3/stoppiglia03a/stoppiglia03a.pdf).\n- `step_select_carscore()`: feature selection step using [CAR scores](https://arxiv.org/abs/1007.5516).\n- `step_select_fcbf()`: fast correlation-based filter.\n- `step_select_forests()`: feature selection step using random forest feature importance scores.\n- `step_select_infgain()`: information gain feature selection step.\n- `step_select_linear()`: feature selection step using the magnitude of a linear models' coefficients.\n- `step_select_mrmr()`: apply minimum redundancy maximum relevance feature selection (MRMR).\n- `step_select_relief()`: feature selection step using the Relief algorithm.\n- `step_select_roc()`: filter numeric predictors using ROC curve.\n- `step_select_tree()`: feature selection step using a decision tree importance scores.\n- `step_select_vip()`: feature selection step using a model's feature importance scores or coefficients.\n- `step_select_xtab()`: filter categorical predictors using contingency tables.\n\nThese steps contain tuning parameters that control how many predictors to retain: \n\n - `top_n` specifies the number to retain while\n - `threshold` describes the cut-point for the metric being used to filter\n\nLet’s add a supervised filter based on the popular random forest importance scores to demonstrate. The step requires a model declaration via a <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=parsnip\">parsnip</a></span> specification. We’ll choose random forest model and optimize the number of top predictors that should be retained and then given to the model. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbase_model <- \n  rand_forest(trees = 1000, mode = \"classification\") %>%\n  set_engine(\"ranger\", importance = \"permutation\")\n\ndrug_rec <- \n  recipe(class ~ ., data = drug_train) %>% \n  step_zv(all_predictors(), id = \"zv\") %>% \n  step_corr(all_numeric_predictors(), threshold = tune(), id = \"cor\")  %>%\n  step_select_vip(\n    all_numeric_predictors(),\n    outcome = \"class\",\n    model = base_model,\n    top_p = tune(),\n    id = \"vip\"\n  ) %>%\n  step_orderNorm(all_numeric_predictors())\n```\n:::\n\n\nNote that we also add a correlation filter and optimize the exclusion threshold. This helps the random forest model since the inclusion of highly correlated predictors [can dilute the importance](https://bookdown.org/max/FES/recursive-feature-elimination.html#fig:greedy-rf-imp) of the set of related predictors. \n\nWe’ll fit a support vector machine model to these data, so the recipe concludes with a step that will normalize the features to have the same distribution (even the binary values). \n\nNow we can specify the supervised model, tag two parameters for optimization, and then add the model and recipe to a workflow: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_spec <- \n  svm_rbf(cost = tune(), rbf_sigma = tune()) %>% \n  set_mode(\"classification\")\n\nvip_svm_wflow <- workflow(drug_rec, svm_spec)\n```\n:::\n\n\nLet’s add specific ranges for the supervised filter parameter since its upper range depends on the data dimensions. We’re not sure how many predictors will pass the unsupervised filter steps, but we’ll guess that we should include, at most, 100 predictors. If we overestimate this number, `step_select_vip()` will adjust the range to the upper limit. \n\nWe'll also adjust the range of the correlation filter to make it more aggressively remove highly correlated predictors: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvip_svm_param <- \n  vip_svm_wflow %>% \n  extract_parameter_set_dials() %>% \n  update(\n  top_p = top_p(c(1L, 100L)), \n  threshold = threshold(c(0.50, 0.99))\n  )\n```\n:::\n\n\n_Finally_, let’s tune the model via Bayesian optimization and use the Brier score to guide the process to the best values of the tuning parameters: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nctrl <- control_bayes(no_improve = Inf, parallel_over = \"everything\")\n\nvip_svm_res <- \n  vip_svm_wflow %>% \n  tune_bayes(\n    resamples = drug_rs,\n    metrics = metric_set(brier_class, roc_auc),\n    initial = 10L,\n    iter = 25L,\n    control = ctrl,\n    param_info = vip_svm_param\n  )\n```\n:::\n\n\nA visualization of the process shows that the search does reduce the Brier score during the search: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(vip_svm_res, metric = \"brier_class\", type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](../figures/vip-performance-1.svg){fig-align='center' width=80%}\n:::\n:::\n\n\nWhen we plot the parameter choices over iterations, we see that each tuning parameter converges to specific ranges. The number of predictors retained fluctuates, and a few choices could be used (say, between 5 and 15 predictors). \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(vip_svm_res, type = \"parameters\")\n```\n\n::: {.cell-output-display}\n![](../figures/vip-param-1.svg){fig-align='center' width=80%}\n:::\n:::\n\n\nA plot of the parameter values versus the Brier score tells a similar story:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(vip_svm_res, metric = \"brier_class\")\n```\n\n::: {.cell-output-display}\n![](../figures/vip-profile-1.svg){fig-align='center' width=80%}\n:::\n:::\n\n\nThe numerically best results are:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshow_best(vip_svm_res, metric = \"brier_class\")\n#> # A tibble: 5 × 11\n#>    cost rbf_sigma threshold top_p .metric   .estimator   mean     n  std_err .config\n#>   <dbl>     <dbl>     <dbl> <int> <chr>     <chr>       <dbl> <int>    <dbl> <chr>  \n#> 1 30.67 0.0005070    0.9665    35 brier_cl… binary     0.1331   100 0.003508 Iter10 \n#> 2 29.89 0.001532     0.9811    75 brier_cl… binary     0.1349   100 0.003652 Iter11 \n#> 3 27.73 0.0002266    0.8979    50 brier_cl… binary     0.1357   100 0.003505 Iter24 \n#> 4 23.13 0.0003069    0.8237    38 brier_cl… binary     0.1359   100 0.003820 Iter15 \n#> 5 23.82 0.0008604    0.9667    80 brier_cl… binary     0.1382   100 0.003489 Iter12 \n#> # ℹ 1 more variable: .iter <int>\n\nbest_param <- select_best(vip_svm_res, metric = \"brier_class\")\n```\n:::\n\n\nLet's update our workflow with the best parameters, then fit the final model on the entire training set: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(124)\nfinal_model <- \n  vip_svm_wflow %>% \n  finalize_workflow(best_param) %>% \n  fit(drug_train)\nfinal_model\n#> ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: svm_rbf()\n#> \n#> ── Preprocessor ─────────────────────────────────────────────────────────────────────\n#> 4 Recipe Steps\n#> \n#> • step_zv()\n#> • step_corr()\n#> • step_select_vip()\n#> • step_orderNorm()\n#> \n#> ── Model ────────────────────────────────────────────────────────────────────────────\n#> Support Vector Machine object of class \"ksvm\" \n#> \n#> SV type: C-svc  (classification) \n#>  parameter : cost C = 30.6666439770923 \n#> \n#> Gaussian Radial Basis kernel function. \n#>  Hyperparameter : sigma =  0.000507017083942551 \n#> \n#> Number of Support Vectors : 126 \n#> \n#> Objective Function Value : -3333 \n#> Training error : 0.142857 \n#> Probability model included.\n```\n:::\n\n\nHow many predictors were removed, and how many made it to the final model? We can write a function to use the `tidy()` method on the recipe steps to assess what was eliminated. The \"mold\" for the workflow can also tell us how many predictors were passed to the SVM model: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_filter_info <- function(x) {\n  fit_rec <- extract_recipe(x)\n  # The tidy methods show the predictors that were eliminated:\n  corr_rm <- nrow(tidy(fit_rec, id = \"cor\"))\n  zv_rm <- nrow(tidy(fit_rec, id = \"zv\"))\n  vip_rm <- nrow(tidy(fit_rec, id = \"vip\"))\n  \n  # The mold has a 'predictors' element that describes the\n  # columns which are given to the model: \n  kept <- \n    x %>% \n    extract_mold() %>% \n    pluck(\"predictors\") %>% \n    ncol()\n  # We'll save them as a tibble:\n  tibble(corr_rm, zv_rm, vip_rm, kept)\n}\n\nget_filter_info(final_model)\n#> # A tibble: 1 × 4\n#>   corr_rm zv_rm vip_rm  kept\n#>     <int> <int>  <int> <int>\n#> 1     682    16    612    35\n```\n:::\n\n\nThe correlation filter removes a large number of predictors, which is not surprising for this type of data set. \n\nHow does the model work on the test set of 65 molecules?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntest_pred <- augment(final_model, drug_test)\ntest_pred %>% brier_class(class, .pred_inducer)\n#> # A tibble: 1 × 3\n#>   .metric     .estimator .estimate\n#>   <chr>       <chr>          <dbl>\n#> 1 brier_class binary        0.1178\ntest_pred %>% roc_auc(class, .pred_inducer)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.899\n```\n:::\n\n\n<a href=\"https://aml4td.org/chapters/feature-selection.html#sec-filters\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}