{
  "hash": "9173a4ec595b68e6cbe9b0160fd9a2e7",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/embeddings/\"\n---\n\n\n\n\n\n# Embeddings {#sec-embeddings}\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Requirements\n\nYou’ll need 13 packages (<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=bestNormalize\">bestNormalize</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=dimRed\">dimRed</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=embed\">embed</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=fastICA\">fastICA</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=igraph\">igraph</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=mixOmics\">mixOmics</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=modeldatatoo\">modeldatatoo</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=patchwork\">patchwork</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=RANN\">RANN</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=RSpectra\">RSpectra</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidymodels\">tidymodels</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=uwot\">uwot</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=viridis\">viridis</a></span>) for this chapter. The <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=mixOmics\">mixOmics</a></span> is a Bioconductor package and is not on CRAN. For the others, we can install them as usual but we'll get <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=mixOmics\">mixOmics</a></span> from GitHub:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreq_pkg <- c(\"bestNormalize\", \"dimRed\", \"embed\", \"fastICA\", \"igraph\", \n             \"mixOmics\", \"modeldatatoo\", \"patchwork\", \"RANN\", \"RSpectra\", \n             \"tidymodels\", \"uwot\", \"viridis\")\n\n# Check to see if they are installed: \npkg_installed <- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list <- names(pkg_installed)[!pkg_installed]\n  \n  # mixOmics is not on CRAN\n  cran_install_list <- install_list[install_list != \"mixOmics\"]\n  if ( length(cran_install_list) > 0 ) {\n    pak::pak(cran_install_list)\n  }\n  \n  # Get mixOmics from github\n  if ( \"mixOmics\" %in% install_list ) {\n    pak::pak(\"mixOmicsTeam/mixOmics\")\n  }\n}\n```\n:::\n\n\n\n\n\nLet's load the meta package and manage some between-package function conflicts. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(viridis)\nlibrary(embed) # for umap\nlibrary(patchwork)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\n```\n:::\n\n\n\n\n\n## Example: Predicting Barley Amounts  {#sec-barley}\n\nThe data are contained in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=modeldatatoo\">modeldatatoo</a></span> package. Let's load the data, remove two outcome columns that will not be analyzed here, and conduct a three-way split of the data: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(modeldatatoo)\n\nchimiometrie_2019 <-\n  data_chimiometrie_2019()  %>%\n  select(-soy_oil, -lucerne)\n\nset.seed(101)\nbarley_split <-\n  initial_validation_split(chimiometrie_2019,\n                           prop = c(0.7, 0.15),\n                           strata = barley)\nbarley_train <- training(barley_split)\nbarley_val   <- validation(barley_split)\nbarley_test  <- testing(barley_split)\n```\n:::\n\n\n\n\n\nThe column names for the predictors are `wvlgth_001` through `wvlgth_550`. \n\nThe primary recipe used for almost all of the embedding methods is:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bestNormalize) # for ORD transformation\n\nbarley_rec <-\n  recipe(barley ~ ., data = barley_train) %>%\n  step_orderNorm(all_numeric_predictors()) %>%\n  # Pre-compute to save time later\n  prep()\n\nbarley_rec\n#> \n#> ── Recipe ───────────────────────────────────────────────────────────────────────────\n#> \n#> ── Inputs\n#> Number of variables by role\n#> outcome:     1\n#> predictor: 550\n#> \n#> ── Training information\n#> Training data contained 4839 data points and no incomplete rows.\n#> \n#> ── Operations\n#> • orderNorm transformation on: wvlgth_001, wvlgth_002, wvlgth_003, ... | Trained\n```\n:::\n\n\n\n\n\n\nMost of the embedding methods can be computed with a common interface if you use a recipe. The recipe step functions are mostly in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=recipes\">recipes</a></span> package although some live in “side packages” such as the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=embed\">embed</a></span> package. We’ll be clear about which package is needed for each. \n\n## Linear Transformations  {#sec-linear-embed}\n\nWe'll look at the three methods described in the text. \n\n### Principal Component Analysis\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbarley_pca_rec <-\n  barley_rec %>%\n  step_pca(all_numeric_predictors(), num_comp = 2, id = \"pca\") %>% \n  prep()\n\nbarley_pca_rec\n#> \n#> ── Recipe ───────────────────────────────────────────────────────────────────────────\n#> \n#> ── Inputs\n#> Number of variables by role\n#> outcome:     1\n#> predictor: 550\n#> \n#> ── Training information\n#> Training data contained 4839 data points and no incomplete rows.\n#> \n#> ── Operations\n#> • orderNorm transformation on: wvlgth_001, wvlgth_002, wvlgth_003, ... | Trained\n#> • PCA extraction with: wvlgth_001, wvlgth_002, wvlgth_003, ... | Trained\n```\n:::\n\n\n\n\n\nTo interrogate the results more, the `tidy()` method can extract elements of the computations. For example, you can return how variance each component captures using the argument `type = \"variance\"`. Note that, when the PCA recipe step was added, we used the option `id = “pca”`. This is not required, but it makes it easier to specify what step that the `tidy()` method should consider: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npca_scree <- tidy(barley_pca_rec, id = \"pca\", type = \"variance\")\npca_scree\n#> # A tibble: 2,200 × 4\n#>   terms       value component id   \n#>   <chr>       <dbl>     <int> <chr>\n#> 1 variance 507.5            1 pca  \n#> 2 variance  35.84           2 pca  \n#> 3 variance   3.395          3 pca  \n#> 4 variance   1.511          4 pca  \n#> 5 variance   0.6940         5 pca  \n#> 6 variance   0.4265         6 pca  \n#> # ℹ 2,194 more rows\n\npca_scree %>% count(terms)\n#> # A tibble: 4 × 2\n#>   terms                           n\n#>   <chr>                       <int>\n#> 1 cumulative percent variance   550\n#> 2 cumulative variance           550\n#> 3 percent variance              550\n#> 4 variance                      550\n```\n:::\n\n\n\n\n\nNote that there are 550 entries for each since there are 550 predictor columns. \n\nThe default option for the `tidy()` method with PCA is to return the estimated loadings. This can help untangle which predictors are influencing the PCA components the most (or least). \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npca_loadings <- tidy(barley_pca_rec, id = \"pca\")\npca_loadings\n#> # A tibble: 302,500 × 4\n#>   terms         value component id   \n#>   <chr>         <dbl> <chr>     <chr>\n#> 1 wvlgth_001 -0.01696 PC1       pca  \n#> 2 wvlgth_002 -0.01704 PC1       pca  \n#> 3 wvlgth_003 -0.01713 PC1       pca  \n#> 4 wvlgth_004 -0.01723 PC1       pca  \n#> 5 wvlgth_005 -0.01734 PC1       pca  \n#> 6 wvlgth_006 -0.01748 PC1       pca  \n#> # ℹ 302,494 more rows\n```\n:::\n\n\n\n\n\nThere are `550^2 = 302500` possible loadings. \n\nTo get the component values for new data, such as the validation set, the `bake()` method can be used. Using `new_data = NULL` returns the training set points:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbarley_pca_rec %>% \n  bake(new_data = NULL, starts_with(\"PC\"))\n#> # A tibble: 4,839 × 2\n#>       PC1   PC2\n#>     <dbl> <dbl>\n#> 1  2.546  6.650\n#> 2  0.1477 6.821\n#> 3 -3.638  5.551\n#> 4 -5.344  5.264\n#> 5 -5.064  4.263\n#> 6  8.857  8.426\n#> # ℹ 4,833 more rows\n```\n:::\n\n\n\n\n\nSince we used `num_comp = 2`, there are 2 new features that are generated. \n\nWe can also pass new data in, such as the validation set: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npca_score_plot <- \n  barley_pca_rec %>% \n  bake(new_data = barley_val) %>% \n  ggplot(aes(PC1, PC2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\")\n\npca_score_plot\n```\n\n::: {.cell-output-display}\n![](../figures/fig-pca-scores-1.svg){#fig-pca-scores fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n\nNote the difference in the axis ranges. If we are considering how much the PCA components explain the original predictors (i.e., not the outcome), it can be very helpful to keep the axis scales common: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npca_score_plot + coord_obs_pred()\n```\n\n::: {.cell-output-display}\n![](../figures/fig-pca-scores-equal-1.svg){#fig-pca-scores-equal fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n\nThis helps avoid over-interpreting proportionally small patterns in the later components. \n\nThe functions `embed::step_pca_sparse()` and `embed::step_pca_sparse_bayes()` have sparse/regularized estimation methods for PCA. Each has an argument called `predictor_prop()` that attempts to control how much sparsity should be used. `predictor_prop = 0` should approximate regular PCA and values near 1.0 would produce very few non-zero loadings. \n\n### Independent Component Analysis\n\nAn ICA recipe step can also be found in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=recipes\">recipes</a></span> package. The syntax is virtually identical: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(538)\nbarley_ica_rec <-\n  recipe(barley ~ ., data = barley_train) %>% \n  step_ica(all_numeric_predictors(), num_comp = 2, id = \"ica\") %>% \n  prep()\n```\n:::\n\n\n\n\n\nSimilarly, the `tidy()` method returns the ICA loadings: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(barley_ica_rec, id = \"ica\")\n#> # A tibble: 1,100 × 4\n#>   terms      component    value id   \n#>   <chr>      <chr>        <dbl> <chr>\n#> 1 wvlgth_001 IC1       -0.02197 ica  \n#> 2 wvlgth_001 IC2        0.9340  ica  \n#> 3 wvlgth_002 IC1       -0.02196 ica  \n#> 4 wvlgth_002 IC2        0.9307  ica  \n#> 5 wvlgth_003 IC1       -0.02196 ica  \n#> 6 wvlgth_003 IC2        0.9270  ica  \n#> # ℹ 1,094 more rows\n```\n:::\n\n\n\n\n\nMost other dimension reduction techniques (but not PCA and PLS) depend on random numbers. We’ll set them when needed, but it is worth pointing out that you are likely to get different results each time you run them. \n\nFor example, when two ICA components are used, the results are not the same but close when using a different random number seed. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(955)\nica_redo <- \n  recipe(barley ~ ., data = barley_train) %>% \n  step_ica(all_numeric_predictors(), num_comp = 2, id = \"ica\") %>% \n  prep()\n\nica_redo %>% tidy(id = \"ica\")\n#> # A tibble: 1,100 × 4\n#>   terms      component    value id   \n#>   <chr>      <chr>        <dbl> <chr>\n#> 1 wvlgth_001 IC1        0.9341  ica  \n#> 2 wvlgth_001 IC2       -0.01989 ica  \n#> 3 wvlgth_002 IC1        0.9307  ica  \n#> 4 wvlgth_002 IC2       -0.01989 ica  \n#> 5 wvlgth_003 IC1        0.9270  ica  \n#> 6 wvlgth_003 IC2       -0.01990 ica  \n#> # ℹ 1,094 more rows\n```\n:::\n\n\n\n\n\nThe individual loads are different, and components one and two are swapped between invokations with different seeds: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nica_1 <- \n  barley_ica_rec %>% \n  bake(new_data = barley_val) %>% \n  ggplot(aes(IC1, IC2, col = barley)) + \n  geom_point(alpha = 1 / 4, show.legend = FALSE) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred() +\n  labs(title = \"seed = 538\")\n\nica_2 <- \n  ica_redo %>% \n  bake(new_data = barley_val) %>% \n  ggplot(aes(IC1, IC2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred() +\n  labs(title = \"seed = 955\")\n\nica_1 + ica_2\n```\n\n::: {.cell-output-display}\n![](../figures/fig-ica-scores-1.svg){#fig-ica-scores fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\nThis might not cause a difference in performance when the features are used in a predictive model but if the model uses slopes and intercepts, the values would not be different each time it is run. \n\n### Partial Least Squares {#numeric-pls}\n\nThe syntax for PLS is also very similar. However, it is a supervised method so we need to specify the column containing the outcome (the outcome column is not needed after model training). The code below uses `dplyr::vars()` to declare the column name but a simple character string can also be used. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbarley_pls_rec <-\n  barley_rec %>%\n  step_pls(all_numeric_predictors(), outcome = vars(barley), num_comp = 2,\n           id = \"pls\") %>% \n  prep()\n\n# Loadings: \ntidy(barley_pls_rec, id = \"pls\")\n#> # A tibble: 1,100 × 4\n#>   terms         value component id   \n#>   <chr>         <dbl> <chr>     <chr>\n#> 1 wvlgth_001 -0.05632 PLS1      pls  \n#> 2 wvlgth_001 -0.1572  PLS2      pls  \n#> 3 wvlgth_002 -0.05637 PLS1      pls  \n#> 4 wvlgth_002 -0.1571  PLS2      pls  \n#> 5 wvlgth_003 -0.05642 PLS1      pls  \n#> 6 wvlgth_003 -0.1570  PLS2      pls  \n#> # ℹ 1,094 more rows\n```\n:::\n\n\n\n\n\n## Multidimensional Scaling {#sec-mds}\n\ntidymodels contains recipe steps for Isomap and UMAP. The latter is accecable via the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=embed\">embed</a></span> package. \n\n### Isomap  {#sec-isomap}\n\nAgain, the syntax is very similar to the previous unsupervised methods. The main two tuning parameters are `num_terms` and `neighbors `. We should also set the seed before execution. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(221)\nbarley_isomap_rec <-\n  barley_rec %>%\n  step_isomap(all_numeric_predictors(), neighbors = 10, num_terms = 2) %>% \n  prep()\n```\n:::\n\n\n\n\n\nWe can project this preprocessing model onto new data: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbarley_isomap_rec %>% \n  bake(new_data = barley_val) %>% \n  ggplot(aes(Isomap1, Isomap2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred()\n```\n\n::: {.cell-output-display}\n![](../figures/fig-isomap-scores-1.svg){#fig-isomap-scores fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n\n### UMAP {#sec-umap}\n\n`step_umap()`, in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=embed\">embed</a></span> package, has a number of tuning parameters: `neighbors`, `num_comp`, `min_dist`, `learn_rate`, `epochs`, `initial` (initialization method, e.g. “pca”), and the optional `target_weight`. \n\nFor an unsupervised embedding: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(724)\nbarley_umap_rec <-\n  barley_rec %>%\n  step_umap(all_numeric_predictors(), neighbors = 10, num_comp = 2) %>% \n  prep()\n```\n:::\n\n\n\n\n\nProjection on new data is the same: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbarley_umap_rec %>% \n  bake(new_data = barley_val) %>% \n  ggplot(aes(UMAP1, UMAP2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred()\n```\n\n::: {.cell-output-display}\n![](../figures/fig-umap-scores-1.svg){#fig-umap-scores fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n\nFor a _supervised_ embedding, the `target_weight` argument is used. A value of zero is unsupervised and values near 1.0 are completely supervised. As with PLS, the argument for the outcome column is called `outcome` and can be a string of an unquoted name wrapped in `vars()`. \n\n## Centroid-Based Methods  {#sec-centroids}\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}