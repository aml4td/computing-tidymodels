{
  "hash": "4eeab3a9380a1897e2b49a2562e701c1",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/embeddings/\"\n---\n\n\n\n\n\n# Embeddings {#sec-embeddings}\n\n\n\n\n\n\n\n\n\n\n\n## Requirements\n\nYou’ll need 13 packages (<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=bestNormalize\">bestNormalize</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=dimRed\">dimRed</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=embed\">embed</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=fastICA\">fastICA</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=igraph\">igraph</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=mixOmics\">mixOmics</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=modeldatatoo\">modeldatatoo</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=patchwork\">patchwork</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=RANN\">RANN</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=RSpectra\">RSpectra</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidymodels\">tidymodels</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=uwot\">uwot</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=viridis\">viridis</a></span>) for this chapter. The <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=mixOmics\">mixOmics</a></span> is a Bioconductor package and is not on CRAN. For the others, we can install them as usual but we'll get <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=mixOmics\">mixOmics</a></span> from GitHub:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreq_pkg <- c(\"bestNormalize\", \"dimRed\", \"embed\", \"fastICA\", \"igraph\", \n             \"mixOmics\", \"modeldatatoo\", \"patchwork\", \"RANN\", \"RSpectra\", \n             \"tidymodels\", \"uwot\", \"viridis\")\n\n# Check to see if they are installed: \npkg_installed <- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list <- names(pkg_installed)[!pkg_installed]\n  \n  # mixOmics is not on CRAN\n  cran_install_list <- install_list[install_list != \"mixOmics\"]\n  if ( length(cran_install_list) > 0 ) {\n    pak::pak(cran_install_list)\n  }\n  \n  # Get mixOmics from github\n  if ( \"mixOmics\" %in% install_list ) {\n    pak::pak(\"mixOmicsTeam/mixOmics\")\n  }\n}\n```\n:::\n\n\n\n\n\nLet's load the meta package and manage some between-package function conflicts. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(viridis)\nlibrary(embed) # for umap\nlibrary(patchwork)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\n```\n:::\n\n\n\n\n\n## Example: Predicting Barley Amounts  {#sec-barley}\n\nThe data are in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=modeldatatoo\">modeldatatoo</a></span> package. Let's load the data, remove two outcome columns that will not be analyzed here, and conduct a three-way split of the data: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/setup_chemometrics.R\")\n```\n:::\n\n\n\n\n\nThe column names for the predictors are `wvlgth_001` through `wvlgth_550`. \n\nThe primary recipe used for almost all of the embedding methods is:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bestNormalize) # for ORD transformation\n\nbarley_rec <-\n  recipe(barley ~ ., data = barley_train) %>%\n  step_orderNorm(all_numeric_predictors()) %>%\n  # Pre-compute to save time later\n  prep()\n\nbarley_rec\n#> \n#> ── Recipe ───────────────────────────────────────────────────────────────────────────\n#> \n#> ── Inputs\n#> Number of variables by role\n#> outcome:     1\n#> predictor: 550\n#> \n#> ── Training information\n#> Training data contained 4839 data points and no incomplete rows.\n#> \n#> ── Operations\n#> • orderNorm transformation on: wvlgth_001, wvlgth_002, wvlgth_003, ... | Trained\n```\n:::\n\n\n\n\n\nIf you use a recipe, most of the embedding methods can be computed with a common interface. The recipe step functions are mostly in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=recipes\">recipes</a></span> package, although some live in \"side packages,\" such as the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=embed\">embed</a></span> package. We’ll be clear about which package is needed for each. \n\n<a href=\"https://aml4td.org/chapters/embeddings.html#sec-barley\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n## Linear Transformations  {#sec-linear-embed}\n\nWe'll look at the three _linear_ methods described in the text. \n\n### Principal Component Analysis {#sec-pca}\n\nUnsurprisingly, the recipe step needed here is called `step_pca()`. We’ll add an `id` argument to more easily reference the step of interest. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbarley_pca_rec <-\n  barley_rec %>%\n  step_pca(all_numeric_predictors(), num_comp = 2, id = \"pca\") %>% \n  prep()\n\nbarley_pca_rec\n#> \n#> ── Recipe ───────────────────────────────────────────────────────────────────────────\n#> \n#> ── Inputs\n#> Number of variables by role\n#> outcome:     1\n#> predictor: 550\n#> \n#> ── Training information\n#> Training data contained 4839 data points and no incomplete rows.\n#> \n#> ── Operations\n#> • orderNorm transformation on: wvlgth_001, wvlgth_002, wvlgth_003, ... | Trained\n#> • PCA extraction with: wvlgth_001, wvlgth_002, wvlgth_003, ... | Trained\n```\n:::\n\n\n\n\n\nTo further investigate the results, the `tidy()` method can extract elements of the computations. For example, you can return how variance each component captures using the argument `type = \"variance\"`. Note that when the PCA recipe step was added, we used the option `id = \"pca\"`. This is not required, but it makes it easier to specify what step the `tidy()` method should consider: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npca_scree <- tidy(barley_pca_rec, id = \"pca\", type = \"variance\")\npca_scree\n#> # A tibble: 2,200 × 4\n#>   terms       value component id   \n#>   <chr>       <dbl>     <int> <chr>\n#> 1 variance 507.5            1 pca  \n#> 2 variance  35.84           2 pca  \n#> 3 variance   3.395          3 pca  \n#> 4 variance   1.511          4 pca  \n#> 5 variance   0.6940         5 pca  \n#> 6 variance   0.4265         6 pca  \n#> # ℹ 2,194 more rows\n\npca_scree %>% count(terms)\n#> # A tibble: 4 × 2\n#>   terms                           n\n#>   <chr>                       <int>\n#> 1 cumulative percent variance   550\n#> 2 cumulative variance           550\n#> 3 percent variance              550\n#> 4 variance                      550\n```\n:::\n\n\n\n\n\nNote that there are 550 entries for each since there are 550 predictor columns. \n\nThe default option for the `tidy()` method with PCA is to return the estimated loadings. This can help untangle which predictors influence the PCA components the most (or least). \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npca_loadings <- tidy(barley_pca_rec, id = \"pca\")\npca_loadings\n#> # A tibble: 302,500 × 4\n#>   terms         value component id   \n#>   <chr>         <dbl> <chr>     <chr>\n#> 1 wvlgth_001 -0.01696 PC1       pca  \n#> 2 wvlgth_002 -0.01704 PC1       pca  \n#> 3 wvlgth_003 -0.01713 PC1       pca  \n#> 4 wvlgth_004 -0.01723 PC1       pca  \n#> 5 wvlgth_005 -0.01734 PC1       pca  \n#> 6 wvlgth_006 -0.01748 PC1       pca  \n#> # ℹ 302,494 more rows\n```\n:::\n\n\n\n\n\nThere are `550^2 = 302500` possible loadings. \n\nTo get the component values for new data, such as the validation set, the `bake()` method can be used. Using `new_data = NULL` returns the training set points:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbarley_pca_rec %>% \n  bake(new_data = NULL, starts_with(\"PC\"))\n#> # A tibble: 4,839 × 2\n#>       PC1   PC2\n#>     <dbl> <dbl>\n#> 1  2.546  6.650\n#> 2  0.1477 6.821\n#> 3 -3.638  5.551\n#> 4 -5.344  5.264\n#> 5 -5.064  4.263\n#> 6  8.857  8.426\n#> # ℹ 4,833 more rows\n```\n:::\n\n\n\n\n\nSince we used `num_comp = 2`, two new features were generated. \n\nWe can also pass new data in, such as the validation set: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npca_score_plot <- \n  barley_pca_rec %>% \n  bake(new_data = barley_val) %>% \n  ggplot(aes(PC1, PC2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\")\n\npca_score_plot\n```\n\n::: {.cell-output-display}\n![](../figures/pca-score-plot-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n\nNote the difference in the axis ranges. If we are considering how much the PCA components explain the original predictors (i.e., not the outcome), it can be very helpful to keep the axis scales common: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npca_score_plot + coord_obs_pred()\n```\n\n::: {.cell-output-display}\n![](../figures/pca-scores-equal-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n\nThis helps avoid over-interpreting proportionally small patterns in the later components. \n\nThe functions `embed::step_pca_sparse()` and `embed::step_pca_sparse_bayes()` have sparse/regularized estimation methods for PCA. Each has an argument called `predictor_prop()` that attempts to control how much sparsity should be used. `predictor_prop = 0` should approximate regular PCA, and values near 1.0 would produce very few non-zero loadings. \n\n<a href=\"https://aml4td.org/chapters/embeddings.html#sec-pca\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n### Independent Component Analysis {#sec-ica}\n\nAn ICA recipe step can also be found in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=recipes\">recipes</a></span> package. The syntax is virtually identical: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(538)\nbarley_ica_rec <-\n  recipe(barley ~ ., data = barley_train) %>% \n  step_ica(all_numeric_predictors(), num_comp = 2, id = \"ica\") %>% \n  prep()\n```\n:::\n\n\n\n\n\nSimilarly, the `tidy()` method returns the ICA loadings: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(barley_ica_rec, id = \"ica\")\n#> # A tibble: 1,100 × 4\n#>   terms      component    value id   \n#>   <chr>      <chr>        <dbl> <chr>\n#> 1 wvlgth_001 IC1       -0.02197 ica  \n#> 2 wvlgth_001 IC2        0.9340  ica  \n#> 3 wvlgth_002 IC1       -0.02196 ica  \n#> 4 wvlgth_002 IC2        0.9307  ica  \n#> 5 wvlgth_003 IC1       -0.02196 ica  \n#> 6 wvlgth_003 IC2        0.9270  ica  \n#> # ℹ 1,094 more rows\n```\n:::\n\n\n\n\n\nMost other dimension reduction techniques (but not PCA and PLS) depend on random numbers. We’ll set them when needed, but it is worth pointing out that you will likely get different results each time you run them. \n\nFor example, when two ICA components are used, the results are not the same but close when using a different random number seed. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(955)\nica_redo <- \n  recipe(barley ~ ., data = barley_train) %>% \n  step_ica(all_numeric_predictors(), num_comp = 2, id = \"ica\") %>% \n  prep()\n\nica_redo %>% tidy(id = \"ica\")\n#> # A tibble: 1,100 × 4\n#>   terms      component    value id   \n#>   <chr>      <chr>        <dbl> <chr>\n#> 1 wvlgth_001 IC1        0.9341  ica  \n#> 2 wvlgth_001 IC2       -0.01989 ica  \n#> 3 wvlgth_002 IC1        0.9307  ica  \n#> 4 wvlgth_002 IC2       -0.01989 ica  \n#> 5 wvlgth_003 IC1        0.9270  ica  \n#> 6 wvlgth_003 IC2       -0.01990 ica  \n#> # ℹ 1,094 more rows\n```\n:::\n\n\n\n\n\nThe individual loading values are different between runs, and components one and two are swapped between invocations with different seeds: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nica_1 <- \n  barley_ica_rec %>% \n  bake(new_data = barley_val) %>% \n  ggplot(aes(IC1, IC2, col = barley)) + \n  geom_point(alpha = 1 / 4, show.legend = FALSE) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred() +\n  labs(title = \"seed = 538\")\n\nica_2 <- \n  ica_redo %>% \n  bake(new_data = barley_val) %>% \n  ggplot(aes(IC1, IC2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred() +\n  labs(title = \"seed = 955\")\n\nica_1 + ica_2\n```\n\n::: {.cell-output-display}\n![](../figures/ica-scores-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\nThis might not cause a difference in performance when the features are used in a predictive model, but if the model uses slopes and intercepts, the parameter estimates will be different each time it is run. \n\n<a href=\"https://aml4td.org/chapters/embeddings.html#sec-ica\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n### Partial Least Squares {#numeric-pls} {#sec-pls}\n\nThe syntax for PLS is also very similar. However, it is a supervised method, so we need to specify the column containing the outcome (the outcome column is not needed after model training). The code below uses `dplyr::vars()` to declare the column name, but a simple character string can also be used. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbarley_pls_rec <-\n  barley_rec %>%\n  step_pls(all_numeric_predictors(), outcome = vars(barley), num_comp = 2,\n           id = \"pls\") %>% \n  prep()\n\n# Loadings: \ntidy(barley_pls_rec, id = \"pls\")\n#> # A tibble: 1,100 × 4\n#>   terms         value component id   \n#>   <chr>         <dbl> <chr>     <chr>\n#> 1 wvlgth_001 -0.05632 PLS1      pls  \n#> 2 wvlgth_001 -0.1572  PLS2      pls  \n#> 3 wvlgth_002 -0.05637 PLS1      pls  \n#> 4 wvlgth_002 -0.1571  PLS2      pls  \n#> 5 wvlgth_003 -0.05642 PLS1      pls  \n#> 6 wvlgth_003 -0.1570  PLS2      pls  \n#> # ℹ 1,094 more rows\n```\n:::\n\n\n\n\n\n<a href=\"https://aml4td.org/chapters/embeddings.html#sec-pls\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n## Multidimensional Scaling {#sec-mds}\n\ntidymodels contains recipe steps for Isomap and UMAP. The latter is accessible via the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=embed\">embed</a></span> package. \n\n### Isomap  {#sec-isomap}\n\nAgain, the syntax is very similar to the previous unsupervised methods. The main two tuning parameters are `num_terms` and `neighbors `. We should also set the seed before execution. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(221)\nbarley_isomap_rec <-\n  barley_rec %>%\n  step_isomap(all_numeric_predictors(), neighbors = 10, num_terms = 2) %>% \n  prep()\n```\n:::\n\n\n\n\n\nWe can project this preprocessing model onto new data: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbarley_isomap_rec %>% \n  bake(new_data = barley_val) %>% \n  ggplot(aes(Isomap1, Isomap2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred()\n```\n\n::: {.cell-output-display}\n![](../figures/isomap-scores-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n\n<a href=\"https://aml4td.org/chapters/embeddings.html#sec-isomap\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n### UMAP {#sec-umap}\n\n`step_umap()`, in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=embed\">embed</a></span> package, has a number of tuning parameters: `neighbors`, `num_comp`, `min_dist`, `learn_rate`, `epochs`, `initial` (initialization method, e.g. \"pca\"), and the optional `target_weight`. \n\nFor an unsupervised embedding: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(724)\nbarley_umap_rec <-\n  barley_rec %>%\n  step_umap(all_numeric_predictors(), neighbors = 10, num_comp = 2) %>% \n  prep()\n```\n:::\n\n\n\n\n\nProjection on new data has the same syntax: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbarley_umap_rec %>% \n  bake(new_data = barley_val) %>% \n  ggplot(aes(UMAP1, UMAP2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred()\n```\n\n::: {.cell-output-display}\n![](../figures/umap-scores-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n\nFor a _supervised_ embedding, the `target_weight` argument is used. A value of zero is unsupervised, and values near 1.0 are completely supervised. As with PLS, the argument for the outcome column is called `outcome` and can be a string of an unquoted name wrapped in `vars()`. \n\n<a href=\"https://aml4td.org/chapters/embeddings.html#sec-umap\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n\n## Centroid-Based Methods  {#sec-centroids}\n\nThere are two steps in recipes for this: \n\n - `step_classdist()`: basic \"distance to centroid\" calculations and,\n - `step_classdist_shrunken()`: nearest shrunken centroids\n \nThese steps are for classification data, so we'll use some example data from the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=modeldata\">modeldata</a></span> package: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntwo_class_dat %>% \n  ggplot(aes(A, B, col = Class)) + \n  geom_point(alpha = 1 / 2) +\n  coord_obs_pred()\n```\n\n::: {.cell-output-display}\n![](../figures/two-class-data-plot-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n\nHere's an example of creating a recipe with the basic class distance computations:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncentroid_rec <-\n  recipe(Class ~ ., data = two_class_dat) %>%\n  step_classdist(all_numeric_predictors(), class = \"Class\") %>% \n  prep()\n```\n:::\n\n\n\n\n\nThe outcome argument is called `\"class\"` and takes a string value for the column name. \n\nThe processed data has a default naming convention of `\"classdist_{class level}\"` and you get one column per class: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbake(centroid_rec, new_data = NULL)\n#> # A tibble: 791 × 5\n#>       A     B Class  classdist_Class1 classdist_Class2\n#>   <dbl> <dbl> <fct>             <dbl>            <dbl>\n#> 1 2.070 1.632 Class1         -0.05795          -0.5526\n#> 2 2.016 1.037 Class1         -1.026             1.647 \n#> 3 1.689 1.367 Class2         -0.8454            0.2437\n#> 4 3.435 1.980 Class2          1.367             1.678 \n#> 5 2.885 1.976 Class1          0.9208            0.3913\n#> 6 3.314 2.406 Class2          1.708             0.4739\n#> # ℹ 785 more rows\n```\n:::\n\n\n\n\n\nThe shrunken version of this step has an additional argument that is the fraction of the complete solutions. The argument name is `threshold`: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncentroid_shrunk_rec <-\n  recipe(Class ~ ., data = two_class_dat) %>%\n  step_classdist_shrunken(all_numeric_predictors(), threshold = 1 / 6, class = \"Class\") %>% \n  prep()\n```\n:::\n\n\n\n\n\n<a href=\"https://aml4td.org/chapters/embeddings.html#sec-centroids\" target=\"_blank\">{{< fa solid rotate-left size=small >}}</a>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}