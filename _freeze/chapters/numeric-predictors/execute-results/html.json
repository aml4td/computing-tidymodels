{
  "hash": "a31d6caa28e223cc68d4a2319b173993",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/transformations/\"\n---\n\n\n# Transforming Numeric Predictors {#sec-numeric-predictors}\n\n\n\n\n\n\nThis chapter is concerned with operations to improve how numeric predictor variables are represented in the data prior to modeling. We separate these operations into two categories: \n\n- _preprocessing_ methods are actions that the _model requires_ such as standardization of variables. \n- _feature engineering_ transformations are those that your particular data set requires to successfully predict the outcome. \n\nIn either case, we estimate these transformations exclusively from the training set and apply them to any data (e.g. the training set, test set, and/or new or unknown data). This is generally true and apply to upcoming chapters on categorical data and other transformations.\n\nIn tidymodels, just about everything that you want to do to your predictors can be accomplished using the R formula method or, better still, the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=recipes\">recipes</a></span> package. We shall describe both. \n\n## Requirements\n\nYou’ll need 4 packages (<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=bestNormalize\">bestNormalize</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=embed\">embed</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=splines2\">splines2</a></span>, and <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidymodels\">tidymodels</a></span>) for this chapter. \nYou can install them via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreq_pkg <- c(\"bestNormalize\", \"embed\", \"tidymodels\", \"splines2\")\n\n# Check to see if they are installed: \npkg_installed <- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list <- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\n```\n:::\n\n\nLet's load the meta package and manage some between-package function conflicts. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())\n```\n:::\n\n\n## Data Sets\n\nThe data sets used here are both in R packages that are already installed. Let's work with the primary data set: the Ames Iowa housing data.\n\nIn the last chapter, our manipulation and splitting code was: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(ames, package = \"modeldata\")\n\names <-\n  ames %>%\n  select(Sale_Price, Bldg_Type, Neighborhood, Year_Built, Gr_Liv_Area, Full_Bath,\n         Half_Bath, Year_Sold, Lot_Area, Central_Air, Longitude, Latitude) %>%\n  mutate(\n    Sale_Price = log10(Sale_Price),\n    Baths = Full_Bath  + Half_Bath/2\n  ) %>%\n  select(-Half_Bath, -Full_Bath)\n\nset.seed(3024)\names_split <- initial_split(ames, strata = Sale_Price, breaks = 5)\names_train <- training(ames_split)\names_test  <- testing(ames_split)\n```\n:::\n\n\nWe'll work with `ames_train` almost excusively here. \n\n## Standard R Formulas\n\nModel formulas in R are identical to those in S, which Chambers and Hastie introduced in _Statistical Models in S_ (1991). A broader discussion can be found in two blog posts ([one](https://rviews.rstudio.com/2017/02/01/the-r-formula-method-the-good-parts/) and [two](https://rviews.rstudio.com/2017/03/01/the-r-formula-method-the-bad-parts/)).\n\nThe formula has a few basic operators: \n\n* The tilde (`~`) separates the outcome columns from the predictor columns. Anything to the left is considered an outcome, and the right-hand side defines predictors (e.g., `outcome ~ predictor`\n* A dot is a wildcard for any columns in the data set that are not outcomes (e.g., `y ~ .`). \n* Plus signs signify the symbolic addition of columns to the formula (typically predictors). For example, `y ~ x1 + x2` indicates one outcome and two predictor columns. To indicate arithmetic addition (or any other computations), you can wrap the items in the identity function `I()` such as `y ~ I(x1 + x2)`. \n* You can use the minus sign to remove columns. This may not be implemented in some modeling functions. \n* The colon indicates interaction terms (described in a future chapter). \n\nThere is further syntax described below. \n\nHere's an example of a basic formula that creates two predictor columns by specifying a symbolic formula comprised of two numeric predictors. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nf_01 <- Sale_Price ~  Baths + Year_Built\n```\n:::\n\n\nHere's a short function to show basic results:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshow_columns <- function(f) {\n  model.matrix(f, data = ames_train) %>% \n  tibble::as_tibble() %>% \n  dplyr::slice(c(1, 3, 9))\n}\nshow_columns(f_01)\n#> # A tibble: 3 × 3\n#>   `(Intercept)` Baths Year_Built\n#>           <dbl> <dbl>      <dbl>\n#> 1             1   1         1961\n#> 2             1   1.5       1971\n#> 3             1   2         1962\n```\n:::\n\n\nIt does not use row-wise arithmetic additions of the two columns. To do that, you can use the identify function: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# One slope term, not two\nf_02 <- Sale_Price ~  I(Full_Bath + Half_Bath)\n```\n:::\n\n\nSymbolic addition creates separate columns of the data set. In chapter TODO, we'll discuss _main effects_ and _interactions_. The main effects are features composed of a single predictor (as in `f_01` above). Interaction effects are one or more model terms that combine the information of all of the predictors in a multiplicative way. There are a few ways to specify them. Here are three methods for specifying two-factor interactions between predictors: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# `:` is used for specific interactions\nf_03 <- Sale_Price ~  Baths + Year_Built + Baths:Year_Built\n\n# `*` is used to make all interactions of two or more terms\nf_04 <- Sale_Price ~  Baths * Year_Built\n\n# `()^D` makes interactions up to order D of all of the columns\n# within the parenthesis\nf_05 <- Sale_Price ~  (Baths + Year_Built)^2\n\nshow_columns(f_05)\n#> # A tibble: 3 × 4\n#>   `(Intercept)` Baths Year_Built `Baths:Year_Built`\n#>           <dbl> <dbl>      <dbl>              <dbl>\n#> 1             1   1         1961              1961 \n#> 2             1   1.5       1971              2956.\n#> 3             1   2         1962              3924\n```\n:::\n\n\nSince `Baths` and `Year_Built` are both numeric, their interactions are created by simply multiplying their values, i.e., `I(Baths * Year_Built)`. \n\nBy default, the model formula creates an intercept column where the value of each row is 1.0. To prevent the intercept from being added, there are two syntaxes: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nf_06 <- Sale_Price ~  Baths - 1 \nf_07 <- Sale_Price ~  Baths + 0\n\nshow_columns(f_07)\n#> # A tibble: 3 × 1\n#>   Baths\n#>   <dbl>\n#> 1   1  \n#> 2   1.5\n#> 3   2\n```\n:::\n\n\nWhat happens with factor predictors? Their specification is the same: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nf_08 <- Sale_Price ~  Bldg_Type \n```\n:::\n\n\nHowever, _most of the time_^[Some model functions require these binary indicators, and others do not. You should assume they convert factor predictors to binary indicators; we will alter you when a specific function does not.], the formula method creates columns of binary 0/1 to replace the original factor column. Since there are 5 possible values of `Bldg_Type`, the formula creates 4 columns of indicator variables, each corresponding to a specific level. The first factor level is excluded by default. This is discussed more in Chapter TODO. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Note that the resulting column names smash the original column\n# name an its factor level together with no delimiter. \nshow_columns(f_08)\n#> # A tibble: 3 × 5\n#>   `(Intercept)` Bldg_TypeTwoFmCon Bldg_TypeDuplex Bldg_TypeTwnhs Bldg_TypeTwnhsE\n#>           <dbl>             <dbl>           <dbl>          <dbl>           <dbl>\n#> 1             1                 0               0              0               0\n#> 2             1                 0               0              1               0\n#> 3             1                 0               1              0               0\n```\n:::\n\n\nFor interaction terms, the syntax is the same as the one shown above. In the case of categorical predictors, all combinations of the predictors are created. In the following case, `Central_Air` has two levels. A two-way interaction of these two predictors creates 4 $\\times$ 1 = 4 interaction columns. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nf_09 <- Sale_Price ~  (Bldg_Type + Central_Air)^2\n\nshow_columns(f_09)\n#> # A tibble: 3 × 10\n#>   `(Intercept)` Bldg_TypeTwoFmCon Bldg_TypeDuplex Bldg_TypeTwnhs Bldg_TypeTwnhsE\n#>           <dbl>             <dbl>           <dbl>          <dbl>           <dbl>\n#> 1             1                 0               0              0               0\n#> 2             1                 0               0              1               0\n#> 3             1                 0               1              0               0\n#> # ℹ 5 more variables: Central_AirY <dbl>, `Bldg_TypeTwoFmCon:Central_AirY` <dbl>,\n#> #   `Bldg_TypeDuplex:Central_AirY` <dbl>, `Bldg_TypeTwnhs:Central_AirY` <dbl>,\n#> #   `Bldg_TypeTwnhsE:Central_AirY` <dbl>\n```\n:::\n\n\nWhat happens when you exclude the intercept? For a single categorical predictor, all factor levels receive a binary indicator column.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nf_10 <- Sale_Price ~  Bldg_Type + 0\n\nshow_columns(f_10)\n#> # A tibble: 3 × 5\n#>   Bldg_TypeOneFam Bldg_TypeTwoFmCon Bldg_TypeDuplex Bldg_TypeTwnhs Bldg_TypeTwnhsE\n#>             <dbl>             <dbl>           <dbl>          <dbl>           <dbl>\n#> 1               1                 0               0              0               0\n#> 2               0                 0               0              1               0\n#> 3               0                 0               1              0               0\n```\n:::\n\n\n_However_, this may produce unexpected results when multiple factor predictors exist. The first factor in the formula creates all possible indicators (e.g., 5 for `Bldg_Type`) while the others have all but one factor level created. For example, these two formulas would have different columns: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nf_11 <- Sale_Price ~ Bldg_Type + Central_Air + 0\nf_12 <- Sale_Price ~ Central_Air + Bldg_Type + 0\n\nshow_columns(f_11) %>% names() %>% sort()\n#> [1] \"Bldg_TypeDuplex\"   \"Bldg_TypeOneFam\"   \"Bldg_TypeTwnhs\"    \"Bldg_TypeTwnhsE\"  \n#> [5] \"Bldg_TypeTwoFmCon\" \"Central_AirY\"\nshow_columns(f_12) %>% names() %>% sort()\n#> [1] \"Bldg_TypeDuplex\"   \"Bldg_TypeTwnhs\"    \"Bldg_TypeTwnhsE\"   \"Bldg_TypeTwoFmCon\"\n#> [5] \"Central_AirN\"      \"Central_AirY\"\n```\n:::\n\n\nThere model predictions and `anova()` results will be the same but the interpretation of their coefficients will be very different. \n\n\nYou can use in-line functions within a recipe. For example: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(splines2)\nf_13 <- Sale_Price ~  log(Gr_Liv_Area) + scale(Lot_Area) + naturalSpline(Latitude, df = 3)\n\nshow_columns(f_13)\n#> # A tibble: 3 × 6\n#>   `(Intercept)` `log(Gr_Liv_Area)` `scale(Lot_Area)`\n#>           <dbl>              <dbl>             <dbl>\n#> 1             1              6.798            0.1994\n#> 2             1              6.895           -1.103 \n#> 3             1              7.455            0.4139\n#> # ℹ 3 more variables: `naturalSpline(Latitude, df = 3)1` <dbl>,\n#> #   `naturalSpline(Latitude, df = 3)2` <dbl>,\n#> #   `naturalSpline(Latitude, df = 3)3` <dbl>\n```\n:::\n\n\nuses three in-line functions:\n\n 1. The first is a simple log transformation of the gross living area.\n 2. The use of `scale()` will compute the mean and standard deviation of `Lot_Area` and use those to center and scale that column. \n 3. The function `splines2::naturalSpline()` will create a set of basis functions (described in chapter TODO) that will replace the original `Latitude` column. \n \nIn the second and third cases, R's machinery will estimate the relevant statistics and embed them as attributes in the corresponding columns. For each in-line function, the exact same operations are conducted on new data (say when `predict()` is called). \n\nFinally, be aware that each formula captures the environment in which it was created. For example: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nenvironment(f_12)\n#> <environment: R_GlobalEnv>\n\n# The number of objects in the session used to create this web page (up to now):\nlength(ls(envir = environment(f_12)))\n#> [1] 22\n```\n:::\n\n\nIf an object that used `f_12` is saved to disk, it will also contain the 22 objects in the global environment. If any of these objects are large, it can unintentionally make the saved data object large. Note that using the base function `object.size()` will not take into account anything stored in the environment (so the binary file size is underestimated). `lobstr::obj_size()` will give a more accurate estimate.\n\n\nThe <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=butcher\">butcher</a></span> package has tools to strip off these unneeded objects from formulas (or objects that contain formulas). Also, `butcher::weigh()` returns a tibble with the size of each element contained in the object (if any).\n\n\n## What is a Recipe? \n\nA recipe is a set of sequential steps that specify what operations should be conducted on a set of predictors. Operations could include: \n\n- Modifying a predictor’s encoding (e.g., date to month/day/year columns)\n- Adding new features, such as basis expansions.\n- Standardizing or transforming individual predictors. \n- Feature extraction or embeddings on multiple predictors.  \n- Removing features.  \n\nRecipes can be used by themselves or as part of a modeling pipeline. For illustration, we’ll show how to use them directly.  The process is to \n\n```\nspecify -> estimate -> apply\n```\n\nthe recipe. In terms of syntax, the analogous functions are: \n\n```\nrecipe() -> prep() -> bake()\n```\n\nWe’ll start off simple by trying to “unskew” a predictor’s distirbution. \n\n## Resolving Skewness\n\nThe main text mentions that the distribution of the `Lot_Area` variable is skewed. Let's see what that looks like. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\names_train %>% \n  ggplot(aes(Lot_Area)) + \n  geom_histogram(bins = 30, col = \"white\", fill = \"#8E195C\", alpha = 1 / 2) +\n  geom_rug(alpha = 1 / 2, length = unit(0.03, \"npc\"), linewidth = 1) +\n  labs(x = \"Lot Area\")\n```\n\n::: {.cell-output-display}\n![](../figures/lot-area-skew-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\nTo get started, we initialize a recipe with the `recipe()` function and a data set:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nunskew_rec <- recipe(Sale_Price ~ ., data = ames_train)\n```\n:::\n\n\nThe formula method really doesn’t do much here: it records the outcome (columns to the left of `~`), which are predictors (to the right of `~` ), and their data types. Note that the `.` in the formula means that all columns, except those to the left, should be considered predictors. When using a formula to start a recipe, keep it simple. It won’t accept any in-line functions (like `sqrt()` or `log()`); it wants you to change the variables inside of _recipe steps_. \n\nRegarding the `data` argument: _any_ data set with the appropriate columns could be used. The initial recipe work is just cataloging the columns. You could even use a “zero row slice” such as `ames_train[0,]` and get the same results. You might want to do something like this if you have a very large training set (to reduce the in-memory footprint). The main advantage of using `ames_train` is convenience (as we’ll see later). \n\nFrom this initial object, we’ll add different recipe step functions to declare what we want to do. Let’s say that we will transform the lot area column using the Yeo-Johnsom transformation. To do this: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nunskew_rec <- \n  recipe(Sale_Price ~ ., data = ames_train) %>% \n  step_YeoJohnson(Lot_Area)\n\n# or use a dplyr selector:\nunskew_rec <- \n  recipe(Sale_Price ~ ., data = ames_train) %>% \n  step_YeoJohnson(any_of(\"Lot_Area\"))\n\nunskew_rec\n#> \n#> ── Recipe ───────────────────────────────────────────────────────────────────────────\n#> \n#> ── Inputs\n#> Number of variables by role\n#> outcome:    1\n#> predictor: 10\n#> \n#> ── Operations\n#> • Yeo-Johnson transformation on: any_of(\"Lot_Area\")\n```\n:::\n\n\nor `starts_with(\"Lot_\")` and so on. \n\nThis only specifies what we want to do. Recall that the Yeo-Johnson transformation _estimates_ a transformation parameter from the data. To estimate the recipe, use `prep()`: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nunskew_rec <- prep(unskew_rec)\n\n# or, to use a different data set: \nunskew_rec <- prep(unskew_rec, training = ames_train)\n#> Warning in prep(unskew_rec, training = ames_train): ! The previous data will be used by `prep()`.\n#> ℹ The data passed using `training` will be ignored.\nunskew_rec\n#> \n#> ── Recipe ───────────────────────────────────────────────────────────────────────────\n#> \n#> ── Inputs\n#> Number of variables by role\n#> outcome:    1\n#> predictor: 10\n#> \n#> ── Training information\n#> Training data contained 2196 data points and no incomplete rows.\n#> \n#> ── Operations\n#> • Yeo-Johnson transformation on: Lot_Area | Trained\n```\n:::\n\n\nNote that the printed recipe shows that `Lot_Area` was resolved from the original request for `any_of(\"Lot_Area\")`. \n\nWhat was the estimate of the transformation parameter? The `tidy()` method can tell us: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Get the list of steps: \ntidy(unskew_rec)\n#> # A tibble: 1 × 6\n#>   number operation type       trained skip  id              \n#>    <int> <chr>     <chr>      <lgl>   <lgl> <chr>           \n#> 1      1 step      YeoJohnson TRUE    FALSE YeoJohnson_VL1H3\n\n# Get information about the first step: \ntidy(unskew_rec, number = 1)\n#> # A tibble: 1 × 3\n#>   terms     value id              \n#>   <chr>     <dbl> <chr>           \n#> 1 Lot_Area 0.1503 YeoJohnson_VL1H3\n```\n:::\n\n\nNow that we have a trained recipe, we can use it via `bake()`: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Get the list of steps: \nbake(unskew_rec, new_data = head(ames_train))\n#> # A tibble: 6 × 11\n#>   Bldg_Type Neighborhood    Year_Built Gr_Liv_Area Year_Sold Lot_Area Central_Air\n#>   <fct>     <fct>                <int>       <int>     <int>    <dbl> <fct>      \n#> 1 OneFam    North_Ames            1961         896      2010    20.52 Y          \n#> 2 OneFam    North_Ames            1971         864      2010    20.11 Y          \n#> 3 Twnhs     Briardale             1971         987      2010    13.67 Y          \n#> 4 Twnhs     Briardale             1971        1092      2010    13.67 Y          \n#> 5 Twnhs     Northpark_Villa       1975         836      2010    14.62 Y          \n#> 6 OneFam    Sawyer_West           1920        1012      2010    19.83 N          \n#> # ℹ 4 more variables: Longitude <dbl>, Latitude <dbl>, Baths <dbl>,\n#> #   Sale_Price <dbl>\n```\n:::\n\n\nDid it work? Let's look at the whole training set: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nunskew_rec %>% \n  bake(new_data = ames_train) %>% \n  ggplot(aes(Lot_Area)) +\n  geom_rug(alpha = 1 / 2, length = unit(0.03, \"npc\"), linewidth = 1) + \n  geom_histogram(bins = 30, col = \"white\", fill = \"#8E195C\", alpha = 1 / 2) +\n  labs(x = \"Lot Area\")\n```\n\n::: {.cell-output-display}\n![](../figures/lot-area-unskew-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\nOne shortcut we can take: the recipe has to apply each step to the training data after it estimates the step. By default, the recipe object saves the processed version of the data set. This can be turned off using the `retain = FALSE` option to `prep()`.  Since the training set is already in the recipe, we can get it with no additional computations using \n\n```r\nbake(unskew_rec, new_data = NULL) \n```\n\nThe main site mentions a few other methods that could be used besides Yeo-Johnson: \n\n - Box-Cox: `step_BoxCox()`\n - Percentile: `step_percentile()` \n - orderNorm: `step_orderNorm()`\n\nNote that the last method has its step function in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=bestNormalize\">bestNormalize</a></span> package; a variety of recipe extension packages can be used. A [**full set of recipe steps**](https://www.tidymodels.org/find/recipes/) for CRAN packages is available on `tidymodels.org`. \n\nThere is also a general step for _simple_ computations that do not need to be estimated. If we were to log transform the data, we would use: \n\n```r\nrecipe(Sale_Price ~ ., data = ames_train) %>% \n  step_mutate(Lot_Area = log10(Lot_Area))\n```\n\nOther single variable transformations can be found in the following R packages: <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=car\">car</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=trafo\">trafo</a></span>, and <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=Transform\">Transform</a></span>. \n\n## More on Recipe Selectors\n\nThe previous section showed a recipe step that operated on a single column. You can select one or more predictors in a variety of different ways within a recipe: \n\n - Bare, unquoted column names such as `Lot_Area`.\n - <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=dplyr\">dplyr</a></span> package selectors, including `starts_with()`, `contained()`, and so on. \n - Special, recipe-only selectors: \n    - Role-based: `all_predictors()`, `all_outcomes()`, and so on. \n    - Type-based: `all_numeric()`, `all_factor()`, ...\n    - Combinations: `all_numeric_predictors()` etc. \n\nTwo important <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=dplyr\">dplyr</a></span> selectors are `all_of()` and `any_of()`. These take character vectors of column names as inputs. `all_of()` will select all of the columns in the vector and will fail if they are not all present when the recipe step is executed. `any_of()` will select any of the columns that are given and won’t fail, even if none are available. \n\nThis is important for a few reasons. Some steps can combine or eliminate columns. A recipe should be fault tolerant; if the previous step removed column `A` and the next step strictly requires it, it will fail. However, if `any_of(c(\"A\"))` is used, it will not ^[More accurately, it will _probably_ be fine. Most steps are permissive; others are not. The previously described `step_mutate()` would fail if `Lot_Area` was previously eliminated.]. \n\nThere is a [documentation page](https://recipes.tidymodels.org/reference/selections.html) for recipe selectors as well as the [reference page](https://recipes.tidymodels.org/reference/has_role.html).\n\n## Standardizing to a common scale \n\nThe two main steps for standardizing columns to have the same units are `step_normalize()` and `step_range()`. A common pattern for the former is: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnorm_rec <- \n  unskew_rec %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors())\n\nnorm_rec\n#> \n#> ── Recipe ───────────────────────────────────────────────────────────────────────────\n#> \n#> ── Inputs\n#> Number of variables by role\n#> outcome:    1\n#> predictor: 10\n#> \n#> ── Training information\n#> Training data contained 2196 data points and no incomplete rows.\n#> \n#> ── Operations\n#> • Yeo-Johnson transformation on: Lot_Area | Trained\n#> • Zero variance filter on: all_predictors()\n#> • Centering and scaling for: all_numeric_predictors()\n```\n:::\n\n\n`step_zv()` is for removing \"zero-variance\" (zv) predictors. These are columns with a single unique value. Since `step_normalize()` will try to divide by a column's standard deviation, this will fail if there is no variation in the column. `step_zv()` will remove such columns that exist in the training set. \n\nWe recycled the previous recipe, which has already been trained. Note that in the output above, only the first step is labeled as “`Trained`”. When we run `prep()` on this recipe, it only estimates the remaining two steps. \n\nAgain, once we prep(are) the recipe, we can use `bake()` to get the normalized data. \n\nAnother important point is that recipes are designed to utilize different data sets appropriately. The training set is used with `prep()` and ensures that all the estimations are based on it. There is, as is appropriate, no re-estimation of quantities when new data are processed. \n\n## Spatial Sign\n\nUnsurprisingly, the step to compute the spatial sign is `step_spatialsign()`. It projects two or more numeric columns onto a multidimensional hypersphere. The resulting data has columns the same name as the input: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bestNormalize)\n\nsp_sign_rec <- \n  recipe(Sale_Price ~ Lot_Area + Gr_Liv_Area, data = ames_train) %>% \n  step_YeoJohnson(any_of(c(\"Lot_Area\", \"Gr_Liv_Area\"))) %>% \n  step_zv(all_predictors()) %>% \n  step_orderNorm(all_numeric_predictors()) %>% \n  step_spatialsign(all_numeric_predictors()) %>% \n  prep()\n\nsp_sign_data <- bake(sp_sign_rec, new_data = NULL)\nsp_sign_data\n#> # A tibble: 2,196 × 3\n#>   Lot_Area Gr_Liv_Area Sale_Price\n#>      <dbl>       <dbl>      <dbl>\n#> 1   0.4496     -0.8932      5.021\n#> 2   0.2349     -0.9720      5.061\n#> 3  -0.9012     -0.4334      4.982\n#> 4  -0.9474     -0.3199      4.944\n#> 5  -0.7424     -0.6700      5.079\n#> 6   0.1537     -0.9881      4.829\n#> # ℹ 2,190 more rows\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsp_sign_data %>% \n  ggplot(aes(Lot_Area, Gr_Liv_Area)) +\n  geom_point(cex =  2, alpha = 1 / 10, pch = 1) +\n  coord_equal() \n```\n\n::: {.cell-output-display}\n![](../figures/sp-sign-1.svg){fig-align='center' width=40%}\n:::\n:::\n\n\n## Other resources for learning about recipes:\n\n- `tidymodels.org`: [_Preprocess your data with recipes_](https://www.tidymodels.org/start/recipes/)\n- _TMwR_ chapter: [_Feature Engineering with recipes_](https://www.tmwr.org/recipes)\n- _TMwR_ chapter: [_Dimensionality Reduction_](https://www.tmwr.org/dimensionality)\n- 2023 Posit conference workshop slides:  [_Intro: Using recipes_](https://workshops.tidymodels.org/archive/2023-09-posit-conf/intro-extra-recipes.html)\n- 2023 Posit conference workshop slides: [_Feature engineering using recipes_](https://workshops.tidymodels.org/archive/2023-09-posit-conf/advanced-02-feature-engineering.html#/title-slide)\n- [_Roles in recipes_](https://recipes.tidymodels.org/articles/Roles.html)\n- [_Ordering of steps_](https://recipes.tidymodels.org/articles/Ordering.html)\n- Stackoverflow Questions tagged [`[r-recipes]`](https://stackoverflow.com/questions/tagged/r-recipes)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}