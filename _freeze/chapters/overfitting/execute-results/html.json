{
  "hash": "2a47bacd047941dee329f4a42f68c648",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/overfitting/\"\n---\n\n# Overfitting {#sec-overfitting}\n\n\n\nSince overfitting is often caused by poor values of tuning parameters, we'll focus on how to work with these values. \n\n## Requirements\n\nYou’ll need 2 packages (<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=bestNormalize\">bestNormalize</a></span> and <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidymodels\">tidymodels</a></span>) for this chapter. \nYou can install them via:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreq_pkg <- c(\"bestNormalize\", \"tidymodels\")\n\n# Check to see if they are installed: \npkg_installed <- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list <- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\n```\n:::\n\nLet's load the meta package and manage some between-package function conflicts. \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(bestNormalize)\ntidymodels_prefer()\n```\n:::\n\n## Tuning Paramters {#sec-tuning-parameters}\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\nThere are currently two main components in a model pipeline: \n\n - a preprocessing method\n - a supervised model fit.\n\nIn tidymodels, the type of object that can hold these two components is called a workflow. Each has arguments, many of which are for tuning parameters. \n\nThere are standardized arguments for most model parameters. For example, regularization in glmnet models and neural networks use the argument name `penalty` even though the latter model would refer to this as weight decay. \n\ntidymodels differentiates between two varieties of tuning parameters: \n\n - main arguments are used by most engines for a model type.\n - engine arguments represent more niche values specific to a few engines. \n\nLet’s look at an example. In the [embeddings chapter](https://aml4td.org/chapters/embeddings.html), the barley data had a high degree of correlation between the predictors. We discussed PCA, PLS, and other methods to deal with this (via  recipe steps). We might try a neural network (say, using the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=brulee\">brulee</a></span> engine) for a model. The code to specify this pipeline would be: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npls_rec <-\n  recipe(barley ~ ., data = barley_train) %>%\n  step_zv(all_predictors()) %>%\n  step_orderNorm(all_numeric_predictors()) %>%\n  step_pls(all_numeric_predictors(),\n           outcome = \"barley\",\n           num_comp = 20) %>%\n  step_normalize(all_predictors())\n\nnnet_spec <-\n  mlp(\n    hidden_units = 10,\n    activation = \"relu\",\n    penalty = 0.01,\n    epochs = 1000,\n    learn_rate = 0.1\n  ) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"brulee\")\n\npls_nnet_wflow <- workflow(pls_rec, nnet_spec)\n```\n:::\n\nWe’ve filled in specific values for each of these arguments, although we don’t know if these are best that we can do. \n\n## Marking Parameters for Optimization {#sec-tag-for-tuning}\n\nTo tune these parameters, we can give them a value of the function `tune()`. This special function just returns an expression with the value \"`tune()`\". For example: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npls_rec <-\n  recipe(barley ~ ., data = barley_train) %>%\n  step_zv(all_predictors()) %>%\n  step_orderNorm(all_numeric_predictors()) %>%\n  step_pls(all_numeric_predictors(),\n           outcome = \"barley\",\n           # For demonstration, we'll use a label\n           num_comp = tune(\"pca comps\")) %>%\n  step_normalize(all_predictors())\n\nnnet_spec <-\n  mlp(\n    hidden_units = tune(),\n    activation = tune(),\n    penalty = tune(),\n    epochs = tune(),\n    learn_rate = tune()\n  ) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"brulee\")\n\npls_nnet_wflow <- workflow(pls_rec, nnet_spec)\n```\n:::\n\nOptionally, we can give a label as an argument to the function: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstr(tune(\"#PCA components\"))\n#>  language tune(\"#PCA components\")\n```\n:::\n\nThis is useful when the pipeline has two arguments with the same name. For example, if you wanted to use splines for two predictors but allow them to have different degrees of freedom, the resulting set of parameters would not be unique since both of them would have the default label of `deg_free`. In this case, one recipe step could use `tune(\"predictor 1 deg free\")` and another could be `tune(\"predictor 2 deg free\")`. \n\nEngine arguments are set by `set_engine()`. For example:  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnnet_spec <-\n  mlp(\n    hidden_units = tune(),\n    activation = tune(),\n    penalty = tune(),\n    epochs = tune(),\n    learn_rate = tune()\n  ) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"brulee\", stop_iter = 5, rate_schedule = tune()) \n\npls_nnet_wflow <- workflow(pls_rec, nnet_spec)\n```\n:::\n\n## Parameter Functions {#sec-param-functions}\n\nEach tuning parameter has a corresponding function from the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=dials\">dials</a></span> package containing information on the parameter type, parameter ranges (or possible values), and other data.  \n\nFor example, the function for the `penalty` argument is: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenalty()\n#> Amount of Regularization (quantitative)\n#> Transformer: log-10 [1e-100, Inf]\n#> Range (transformed scale): [-10, 0]\n```\n:::\n\nThis parameter has a default range from 10<sup>-10</sup> to 1.0.  It also has a corresponding transformation function (log base 10). This means that when values are created, they are uniformly distributed on the log scale. This is common for parameters that have values that span several orders of magnitude and cannot be negative. \n\nWe can change these defaults via arguments: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenalty(range = c(0, 1), trans = scales::transform_identity())\n#> Amount of Regularization (quantitative)\n#> Transformer: identity [-Inf, Inf]\n#> Range (transformed scale): [0, 1]\n```\n:::\n\nIn some cases, we can’t know the range _a priori_. Parameters like the number of possible PCA components or random forest’s $m_{try}$ depend on the data dimensions. In the case of $m_{try}$ , the default has an unknown in its range:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmtry()\n#> # Randomly Selected Predictors (quantitative)\n#> Range: [1, ?]\n```\n:::\n\nWe would need to set this range to use the parameter. \n\nIn a few situations, the argument name to a recipe step or model function will use a dials function that has a different name than the argument. For example, there are a few different types of \"degrees\". There is (real-valued) polynomial exponent degree: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndegree()\n#> Polynomial Degree (quantitative)\n#> Range: [1, 3]\n\n# Data type: \ndegree()$type\n#> [1] \"double\"\n```\n:::\n\nbut for the spline recipe steps we need an integer value: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Data type: \nspline_degree()$type\n#> [1] \"integer\"\n```\n:::\n\nIn some cases, tidymodels has methods for automatically changing the parameter function to be used, the range of values, and so on. We’ll see that in a minute. \n\nThere are also functions to manipulate individual parameters: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Function list:\napropos(\"^value_\")\n#> [1] \"value_inverse\"   \"value_sample\"    \"value_seq\"       \"value_set\"      \n#> [5] \"value_transform\" \"value_validate\"\n\nvalue_seq(hidden_units(), n = 4)\n#> [1]  1  4  7 10\n```\n:::\n\n## Sets of Parameters {#sec-param-sets}\n\nFor our pipeline `pls_nnet_wflow`, we can extract a _parameter set_ that collects all of the parameters and their suggested information. There is a function to do this: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npls_nnet_param <- extract_parameter_set_dials(pls_nnet_wflow)\n\nclass(pls_nnet_param)\n#> [1] \"parameters\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnames(pls_nnet_param)\n#> [1] \"name\"         \"id\"           \"source\"       \"component\"    \"component_id\"\n#> [6] \"object\"\n\npls_nnet_param\n#> Collection of 7 parameters for tuning\n#> \n#>     identifier          type    object\n#>   hidden_units  hidden_units nparam[+]\n#>        penalty       penalty nparam[+]\n#>         epochs        epochs nparam[+]\n#>     activation    activation dparam[+]\n#>     learn_rate    learn_rate nparam[+]\n#>  rate_schedule rate_schedule dparam[+]\n#>      pca comps      num_comp nparam[+]\n#> \n```\n:::\n\nThe difference in the `type` and `identifier` columns only occurs when the `tune()` value has a label (as with the final row). \n\nThe output `\"nparam[+]\"` indicates a numeric parameter, and the plus indicates that it is fully specified. If our pipeline had used $m_{try}$, that value would show `\"nparam[?]\"`. The rate schedule is a qualitative parameter and has a label of `\"dparam[+]\"` (\"d\" for discrete). \n\nLet’s look at the information for the learning rate parameter by viewing the parameter information set by tidymodels. It is different than the default: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npls_nnet_param %>% \n  filter(id == \"learn_rate\") %>% \n  pluck(\"object\")\n#> [[1]]\n#> Learning Rate (quantitative)\n#> Transformer: log-10 [1e-100, Inf]\n#> Range (transformed scale): [-3, -0.2]\n\n# The defaults: \nlearn_rate()\n#> Learning Rate (quantitative)\n#> Transformer: log-10 [1e-100, Inf]\n#> Range (transformed scale): [-10, -1]\n```\n:::\n\nWhy are they different? The main function has a wider range since it can be used by boosted trees, neural networks, UMAP, and other tools. The range is more narrow for this pipeline since we know that neural networks tend to work better with faster learning rates (so we set a different default). \n\nSuppose we want to change the range to be even more narrow. We can use the `update()` function to change defaults or to use a different <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=dials\">dials</a></span> parameter function: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnew_rate <- \n  pls_nnet_param %>% \n  update(learn_rate = learn_rate(c(-2, -1/2)))\n\nnew_rate %>% \n  filter(id == \"learn_rate\") %>% \n  pluck(\"object\")\n#> [[1]]\n#> Learning Rate (quantitative)\n#> Transformer: log-10 [1e-100, Inf]\n#> Range (transformed scale): [-2, -0.5]\n```\n:::\n\nYou don't always have to extract or modify a parameter set; this is an optional tool in case you want to change default values. \n\nThe parameter set is sometimes passed as an argument to grid creation functions or to iterative optimization functions that need to simulate/sample random candidates. For example, to create a random grid with 4 candidate values: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(220)\ngrid_random(pls_nnet_param, size = 4)\n#> # A tibble: 4 × 7\n#>   hidden_units  penalty epochs activation  learn_rate rate_schedule `pca comps`\n#>          <int>    <dbl>  <int> <chr>            <dbl> <chr>               <int>\n#> 1           36 1.883e-3    112 elu            0.04998 cyclic                  3\n#> 2            7 3.417e-2    362 tanhshrink     0.05164 cyclic                  2\n#> 3            7 1.593e-9    444 log_sigmoid    0.03668 cyclic                  1\n#> 4           38 4.803e-8    150 tanhshrink     0.02776 step                    3\n```\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}