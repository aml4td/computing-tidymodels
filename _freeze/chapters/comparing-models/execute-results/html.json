{
  "hash": "84161e8ff1feb47b6459040d62467f03",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/comparing-models/\"\n---\n\n# Comparing Models {#sec-comparing-models}\n\n\n\nThis book's chapter involved taking models that have been fit or resampled and using their results to formally compare them. \n\n## Requirements\n\nYou’ll need 14 packages (<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=bestNormalize\">bestNormalize</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=broom.mixed\">broom.mixed</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=C50\">C50</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=discrim\">discrim</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=embed\">embed</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=glmnet\">glmnet</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=klaR\">klaR</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=lme4\">lme4</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=multcomp\">multcomp</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=rstanarm\">rstanarm</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=rules\">rules</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=splines2\">splines2</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidymodels\">tidymodels</a></span>, and <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidyposterior\">tidyposterior</a></span>) for this chapter. \nYou can install them via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreq_pkg <- c(\"bestNormalize\", \"broom.mixed\", \"C50\", \"discrim\", \"embed\", \n             \"glmnet\", \"klaR\", \"lme4\", \"multcomp\", \"rstanarm\", \"rules\", \"splines2\", \n             \"tidymodels\", \"tidyposterior\")\n\n# Check to see if they are installed: \npkg_installed <- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list <- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\n```\n:::\n\n\nLet's load the meta package and manage some between-package function conflicts. We'll also load some packages that are used for the data analysis. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())\n\n# Other packages that we will load below\n\n# For feature engineering:\nlibrary(embed)\nlibrary(bestNormalize)\n\n# For models:\nlibrary(discrim)\nlibrary(rules)\n\n# For Frequentist analysis:\nlibrary(lme4)\nlibrary(broom.mixed)\nlibrary(multcomp)\n\n# For Bayesian analysis: \nlibrary(tidyposterior)\n```\n:::\n\n\n## Example Data {#sec-forested-compare-setup}\n\nWe'll use the forestation data just as in the book. These data have already been split, and some interactions have been assessed. Those are captured in two remote RData files: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Loads the training and test set data\nload(url(\"https://github.com/aml4td/website/raw/refs/heads/main/RData/forested_data.RData\"))\n\n# Load information needed for interactions:\nload(url(\"https://github.com/aml4td/website/raw/refs/heads/main/RData/forested_interactions.RData\"))\n```\n:::\n\n\nThe first file contains these relevant objects:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nforested_split\n#> <Training/Testing/Total>\n#> <4832/1371/6203>\n\nforested_train\n#> # A tibble: 4,832 × 17\n#>   class  year elevation eastness northness roughness dew_temp precip_annual\n#> * <fct> <dbl>     <dbl>    <dbl>     <dbl>     <dbl>    <dbl>         <dbl>\n#> 1 Yes    2005       113      -25        96        30     6.4           1710\n#> 2 No     2005       164      -84        53        13     6.06          1297\n#> 3 Yes    2005       299       93        34         6     4.43          2545\n#> 4 Yes    2005       806       47       -88        35     1.06           609\n#> 5 Yes    2005       736      -27       -96        53     1.35           539\n#> 6 Yes    2005       636      -48        87         3     1.42           702\n#> # ℹ 4,826 more rows\n#> # ℹ 9 more variables: temp_annual_mean <dbl>, temp_annual_min <dbl>,\n#> #   temp_annual_max <dbl>, temp_january_min <dbl>, vapor_min <dbl>,\n#> #   vapor_max <dbl>, county <fct>, longitude <dbl>, latitude <dbl>\n\nforested_test\n#> # A tibble: 1,371 × 17\n#>   class  year elevation eastness northness roughness dew_temp precip_annual\n#> * <fct> <dbl>     <dbl>    <dbl>     <dbl>     <dbl>    <dbl>         <dbl>\n#> 1 No     2014       308      -70       -70         4     3.07           233\n#> 2 No     2014       119       94        31         2     3.74           200\n#> 3 No     2014       795      -74        66        39     0.17           255\n#> 4 Yes    2014      1825       99        -9        78    -3.37          1241\n#> 5 No     2014       126       83       -55         6     4.08           234\n#> 6 Yes    2014      1056       99         7        57    -1.6            557\n#> # ℹ 1,365 more rows\n#> # ℹ 9 more variables: temp_annual_mean <dbl>, temp_annual_min <dbl>,\n#> #   temp_annual_max <dbl>, temp_january_min <dbl>, vapor_min <dbl>,\n#> #   vapor_max <dbl>, county <fct>, longitude <dbl>, latitude <dbl>\n\nforested_rs\n#> # A tibble: 10 × 2\n#>   splits             id    \n#>   <list>             <chr> \n#> 1 <split [4127/489]> Fold01\n#> 2 <split [4080/491]> Fold02\n#> 3 <split [4074/505]> Fold03\n#> 4 <split [4185/434]> Fold04\n#> 5 <split [4116/475]> Fold05\n#> 6 <split [4049/523]> Fold06\n#> # ℹ 4 more rows\n```\n:::\n\n\nThe second file has this important formula object that contains what we think are important interactions: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nforested_int_form\n#> ~dew_temp:elevation + dew_temp:temp_annual_max + dew_temp:temp_january_min + \n#>     eastness:vapor_max + temp_annual_max:vapor_max\n```\n:::\n\n\n## Model Fitting {#sec-forested-model-fits}\n\nWe'll need to resample a few models. We'll cut to the chase and resample the tuning parameters found to be optimal in the regular text. \n\nFirst, we load some additional packages and create some preliminaries:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# For resampling the models\nctrl_rs <-\n  control_resamples(\n    save_pred = TRUE,\n    parallel_over = \"everything\",\n    save_workflow = TRUE # Keep this for as_workflow_set()\n  )\n\ncls_mtr <- metric_set(accuracy)\n```\n:::\n\n\nWe start by resampling the boosted tree with the tuning parameter values that were found during the grid search: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_spec <- \n  C5_rules(trees = 60, min_n = 20) |> \n  set_engine(\"C5.0\", control = C50::C5.0Control(seed = 864))\n\nboost_wflow <-  workflow(class ~ ., boost_spec) \n\nset.seed(526)\nboost_res <-\n  fit_resamples(\n    boost_wflow,\n    resamples = forested_rs,\n    control = ctrl_rs,\n    metrics = cls_mtr\n  )\n\n# Save the individual resamples and change the name of the accuracy column. \nboost_metrics <- \n  collect_metrics(boost_res, summarize = FALSE) |> \n  select(id, Boosting = .estimate)\n```\n:::\n\n\n::: {.callout-warning}\nAlthough we strive for reproducibility, it can be difficult. The boosted tree results from the main text (where it was tuned) used slightly different random numbers than when we trained for a single model. Nine of the ten resamples had different accuracy results; the mean difference between the two was 0.07%, and the largest difference was 0.84%. This is obviously very small, but it does lead to different results here than in the main text. The conclusions will not change.\n:::\n\nNow let's create the logistic regression. There is a fair amount of preprocessing and feature engineering. The rationale for these will be discussed in a future chapter in Part 4. \n\nYou'll see below that, although the model uses a single penalty value, we [pass a sequence of penalties](https://parsnip.tidymodels.org/reference/glmnet-details.html) to the `lambda` parameter of `glmnet::glmnet()`. See [parsnip issue #431](https://github.com/tidymodels/parsnip/issues/431#issuecomment-782883848) for some background. \n\nThe model is a full ridge regression model since `mixture = 0`. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_rec <-\n  recipe(class ~ ., data = forested_train) |>\n  # standardize numeric predictors\n  step_orderNorm(all_numeric_predictors()) |>\n  # Convert to an effect encoding\n  step_lencode_mixed(county, outcome = \"class\") |>\n  # Create pre-defined interactions\n  step_interact(!!forested_int_form) |>\n  # 10 spline terms for certain predictors\n  step_spline_natural(\n    all_numeric_predictors(),\n    -county,\n    -eastness,\n    -northness,\n    -year,\n    -contains(\"_x_\"),\n    deg_free = 10\n  ) |>\n  # Remove any linear dependencies\n  step_lincomb(all_predictors())\n\n# ------------------------------------------------------------------------------\n\nlogistic_pen <- 10^seq(-6, -1, length.out = 50)\n\nlogistic_spec <- \n  # Values here were determined via grid search:\n  logistic_reg(penalty = 2.12095088792019e-05, mixture = 0.0) |> \n  set_engine(\"glmnet\", path_values = !!logistic_pen)\n\n# ------------------------------------------------------------------------------\n\nlogistic_wflow <- workflow(logistic_rec, logistic_spec)\n\nlogistic_res <- \n  logistic_wflow |> \n  fit_resamples(\n    resamples = forested_rs,\n    control = ctrl_rs,\n    metrics = cls_mtr\n  )\n\nlogistic_metrics <- \n  collect_metrics(logistic_res, summarize = FALSE) |> \n  select(id, Logistic = .estimate)\n```\n:::\n\n\nNow, the naive Bayes model. There is no need for feature engineering and not much in the way of tuning parameters. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnb_rec <-\n  recipe(class ~ ., data = forested_train) |>\n  step_orderNorm(all_numeric_predictors())\n\nnb_wflow <- workflow(nb_rec, naive_Bayes())\n\nnb_res <- \n  nb_wflow |> \n  fit_resamples(\n    resamples = forested_rs,,\n    control = ctrl_rs,\n    metrics = cls_mtr\n  )\n\nnb_metrics <- \n  collect_metrics(nb_res, summarize = FALSE) |> \n  select(id, `Naive Bayes` = .estimate)\n```\n:::\n\n\nDuring resampling, this model will probably provide some warnings when fit that resemble: \n\n> Numerical 0 probability for all classes with observation X\n\nThis occurs because the model multiplies about a dozen probabilities together. The consequence is that some of these products become very close to zero, and R complains a bit about this. \n\n## A Workflow Set {#sec-forested-workflow-set}\n\nAnother method for resampling or tuning a series of preprocessors and/or models is to create a _workflow set_. We'll see these more in later chapters. Let's take our previous set of three model results and coerce them into a workflow set. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nforested_wflow_set <- as_workflow_set(\n    Boosting = boost_res,\n    Logistic = logistic_res,\n    `Naive Bayes` = nb_res\n)\nforested_wflow_set\n#> # A workflow set/tibble: 3 × 4\n#>   wflow_id    info             option    result   \n#>   <chr>       <list>           <list>    <list>   \n#> 1 Boosting    <tibble [1 × 4]> <opts[0]> <rsmp[+]>\n#> 2 Logistic    <tibble [1 × 4]> <opts[0]> <rsmp[+]>\n#> 3 Naive Bayes <tibble [1 × 4]> <opts[0]> <rsmp[+]>\n\nforested_wflow_set |> rank_results()\n#> # A tibble: 3 × 9\n#>   wflow_id    .config         .metric   mean  std_err     n preprocessor model  rank\n#>   <chr>       <chr>           <chr>    <dbl>    <dbl> <int> <chr>        <chr> <int>\n#> 1 Boosting    Preprocessor1_… accura… 0.8952 0.009321    10 formula      C5_r…     1\n#> 2 Logistic    Preprocessor1_… accura… 0.8906 0.01038     10 recipe       logi…     2\n#> 3 Naive Bayes Preprocessor1_… accura… 0.8581 0.01542     10 recipe       naiv…     3\n```\n:::\n\n\nWe'll see the advantage of this in a later section. \n\n## Resampled Data Sets {#sec-compare-resamples}\n\nLet's collect the accuracy values for each resample and model into a data frame. We'll make a \"wide\" version with three columns for each of the three models and a \"long\" version where there is a column for each model and another for the accuracies. We call the column with the model `pipeline`. \n\nWe'll also plot the data to show that it is similar to the one in the main text. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naccuracy_df <- \n  boost_metrics |> \n  full_join(logistic_metrics, by = \"id\") |> \n  full_join(nb_metrics, by = \"id\") \naccuracy_df\n#> # A tibble: 10 × 4\n#>   id     Boosting Logistic `Naive Bayes`\n#>   <chr>     <dbl>    <dbl>         <dbl>\n#> 1 Fold01   0.8671   0.8773        0.8057\n#> 2 Fold02   0.8900   0.8839        0.8310\n#> 3 Fold03   0.9485   0.9505        0.9386\n#> 4 Fold04   0.8687   0.8848        0.8618\n#> 5 Fold05   0.8589   0.8653        0.8547\n#> 6 Fold06   0.8776   0.8413        0.7820\n#> # ℹ 4 more rows\n  \naccuracy_long_df <- \n  accuracy_df |> \n  pivot_longer(cols = c(-id), names_to = \"pipeline\", values_to = \"accuracy\")\naccuracy_long_df\n#> # A tibble: 30 × 3\n#>   id     pipeline    accuracy\n#>   <chr>  <chr>          <dbl>\n#> 1 Fold01 Boosting      0.8671\n#> 2 Fold01 Logistic      0.8773\n#> 3 Fold01 Naive Bayes   0.8057\n#> 4 Fold02 Boosting      0.8900\n#> 5 Fold02 Logistic      0.8839\n#> 6 Fold02 Naive Bayes   0.8310\n#> # ℹ 24 more rows\n\naccuracy_long_df |> \n  ggplot(aes(pipeline, accuracy)) + \n  geom_point(aes(col = id), show.legend = FALSE) + \n  geom_line(aes(group = id, col = id), show.legend = FALSE) +\n  scale_y_continuous(label = label_percent()) +\n  labs(x = NULL)\n```\n\n::: {.cell-output-display}\n![](comparing-models_files/figure-html/collect-resamples-1.svg){fig-align='center' width=70%}\n:::\n:::\n\n\nNow we can start analyzing the data in different ways.  \n\n### Statistical Foundations {#sec-compare-stats}\n\nWe'll start with the Frequentist perspective. \n\n### Frequentist Hypothesis Testing Methods {#sec-nhtm}\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nWe take the wide version of the accuracy data and fit a linear regression to it in a way that accounts for the resample-to-resample effect. Let's take a minute and think about how our model will set up the parameter estimates. \n\nSince the data in the `pipeline` column is currently character, the model function will convert this column to a factor and assign the factor level order alphabetically. This means that the levels are: `Boosting`, `Logistic`, and `Naive Bayes`. Recall that the default parameterization for indicator variables is a \"reference cell\" parameterization, so the intercept corresponds to the first factor level. This means that, in terms of the fixed effects, we interpret parameter estimates as: \n\n$$\n\\hat{y}_{ij} = \\underbrace{\\quad \\hat{\\beta}_0\\quad }_{\\tt{Boosting}} + \\underbrace{\\quad \\hat{\\beta}_1 \\quad }_{\\tt{Boosting - Logistic}} +\\underbrace{\\quad \\hat{\\beta}_2\\quad }_{\\tt{Boosting - Naive\\: Bayes}}\n$$ \n\nUsing `lme4::lmer()` to fit the model, we specify `pipeline` as the fixed effect and designate a random effect due to resampling as `(1|id)`, where `1` symbolizes the intercept. \n\nThe results are: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfreq_mixed_mod <- lmer(accuracy ~ pipeline + (1|id), data = accuracy_long_df)\n\nfreq_mixed_mod |> tidy()\n#> # A tibble: 5 × 6\n#>   effect   group    term                 estimate std.error statistic\n#>   <chr>    <chr>    <chr>                   <dbl>     <dbl>     <dbl>\n#> 1 fixed    <NA>     (Intercept)          0.8952    0.01200    74.57  \n#> 2 fixed    <NA>     pipelineLogistic    -0.004504  0.007520   -0.5989\n#> 3 fixed    <NA>     pipelineNaive Bayes -0.03702   0.007520   -4.922 \n#> 4 ran_pars id       sd__(Intercept)      0.03403  NA          NA     \n#> 5 ran_pars Residual sd__Observation      0.01682  NA          NA\n```\n:::\n\n\nThe model term corresponding to `term = \"sd__(Intercept)\"` is the standard deviation of the random effects, and the one with a `term` value of `sd__Observation` is the residual standard deviation. Note that the former is almost twice the size of the latter; the resample-to-resample effect is _very_ large for these data.  \n\nNotice that p-values that are _not_ automatically computed. This is addressed by reading `help(\"pvalues\")` or the package vignette called \"Fitting Linear Mixed-Effects Models using lme4\". The CRAN PDF for the latter is [here](https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf). We can create approximate p-values using the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=lmerTest\">lmerTest</a></span> or  <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=multicomp\">multicomp</a></span> packages. \n\nWe’ll show the  <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=multicomp\">multicomp</a></span> package but first we will define contrasts of the parameter estimates that provide all three pairwise comparisons.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Coefficients for the estimates to create differences in accuracies.\ncomparisons <- \n  rbind(\"Boosting - Logistic\"    = c(0,  1,  0),     \n        \"Boosting - Naive Bayes\" = c(0,  0,  1),     \n        \"Naive Bayes - Logistic\" = c(0, -1,  1))\n```\n:::\n\n\nWe’ll use the `multcomp::glht()` function^[\"GLHT\" stands for \"General Linear Hypothesis Tests.\"] to generate them. The `glht()` function relies on another function from that package called `mcp()`^[\"MCP\" presumably stands for \"Multiple Comparison Procedure.\"], and that can be used to sort out the details. For the raw (but approximate) p-values, we simply give it a single contrast at a time:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nraw_res <- NULL\nfor (i in 1:nrow(comparisons)) {\n  raw_res <- \n    bind_rows(\n      raw_res, \n      freq_mixed_mod |> \n        glht(linfct = mcp(pipeline = comparisons[i,,drop = FALSE])) |> \n        summary() |> \n        tidy() |> \n        rename(raw_pvalue = adj.p.value)\n    )\n}\n\nraw_res |> \n  select(-term, -null.value)\n#> # A tibble: 3 × 5\n#>   contrast                estimate std.error statistic   raw_pvalue\n#>   <chr>                      <dbl>     <dbl>     <dbl>        <dbl>\n#> 1 Boosting - Logistic    -0.004504  0.007520   -0.5989 0.5493      \n#> 2 Boosting - Naive Bayes -0.03702   0.007520   -4.922  0.0000008554\n#> 3 Naive Bayes - Logistic -0.03251   0.007520   -4.323  0.00001536\n```\n:::\n\n\nAgain, due to differences in random numbers, the accuracy numbers are slightly different. Consequently, the p-values are also different in value but not in interpretation. \n\n<a href=\"https://aml4td.org/chapters/comparing-models.html#sec-nhtm\" >{{< fa solid rotate-left size=small >}}</a>\n\n#### Post Hoc Pairwise Comparisons and Protecting Against False Positive Findings {#sec-post-hoc}\n\nTo compute the Bonferroni or FDR adjustments, we can use the `stats::p.adjust()` package. However, for the FDR, there are _many_ alternatives. In CRAN alone: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntools::CRAN_package_db() |> \n  as_tibble() |> \n  filter(grepl(\"(fdr)|(false discovery rate)\", tolower(Description))) |> \n  mutate(Title = gsub(\"\\n\", \"\", Title)) |> \n  select(Package, Title)\n#> # A tibble: 84 × 2\n#>   Package             Title                                                         \n#>   <chr>               <chr>                                                         \n#> 1 adaptMT             Adaptive P-Value Thresholding for Multiple Hypothesis Testing…\n#> 2 alpha.correction.bh Benjamini-Hochberg Alpha Correction                           \n#> 3 APFr                Multiple Testing Approach using Average Power Function (APF) …\n#> 4 ashr                Methods for Adaptive Shrinkage, using Empirical Bayes         \n#> 5 bayefdr             Bayesian Estimation and Optimisation of Expected False Discov…\n#> 6 BonEV               An Improved Multiple Testing Procedure for Controlling FalseD…\n#> # ℹ 78 more rows\n```\n:::\n\n\nand even more on [Bioconductor](https://bioconductor.org/): \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nBiocManager::available(\"(fdr)|(false discovery rate)\")\n#>  [1] \"bayefdr\"              \"benchmarkfdrData2019\" \"cffdrs\"              \n#>  [4] \"CorrectedFDR\"         \"dfdr\"                 \"DiscreteFDR\"         \n#>  [7] \"EFDR\"                 \"fcfdr\"                \"fdrame\"              \n#> [10] \"fdrci\"                \"fdrDiscreteNull\"      \"FDRestimation\"       \n#> [13] \"FDRsamplesize2\"       \"FDRsampsize\"          \"fdrtool\"             \n#> [16] \"GFDrmst\"              \"GFDrmtl\"              \"LFDR.MLE\"            \n#> [19] \"LFDR.MME\"             \"LFDREmpiricalBayes\"   \"locfdr\"              \n#> [22] \"nzffdr\"               \"onlineFDR\"            \"pafdR\"               \n#> [25] \"pwrFDR\"               \"repfdr\"               \"RFlocalfdr\"          \n#> [28] \"RFlocalfdr.data\"      \"sffdr\"                \"simpleFDR\"           \n#> [31] \"SNVLFDR\"              \"ssize.fdr\"            \"swfdr\"\n```\n:::\n\n\n`stats::p.adjust()` is very simple to use: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nraw_res |> \n  mutate(\n    Bonnferoni = p.adjust(raw_pvalue, method = \"bonferroni\"),\n    FDR = p.adjust(raw_pvalue, method = \"BH\")\n  ) |> \n  select(-term, -null.value, -estimate)\n#> # A tibble: 3 × 6\n#>   contrast               std.error statistic   raw_pvalue  Bonnferoni         FDR\n#>   <chr>                      <dbl>     <dbl>        <dbl>       <dbl>       <dbl>\n#> 1 Boosting - Logistic     0.007520   -0.5989 0.5493       1           0.5493     \n#> 2 Boosting - Naive Bayes  0.007520   -4.922  0.0000008554 0.000002566 0.000002566\n#> 3 Naive Bayes - Logistic  0.007520   -4.323  0.00001536   0.00004609  0.00002304\n```\n:::\n\n\nFor Tukey's HSD, we can use `glht()` again: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfreq_mixed_mod |> \n  glht(linfct = mcp(pipeline = \"Tukey\")) |> \n  summary() |> \n  tidy() |> \n  select(-term, -null.value, -estimate)\n#> # A tibble: 3 × 4\n#>   contrast               std.error statistic adj.p.value\n#>   <chr>                      <dbl>     <dbl>       <dbl>\n#> 1 Logistic - Boosting     0.007520   -0.5989 0.8207     \n#> 2 Naive Bayes - Boosting  0.007520   -4.922  0.000002667\n#> 3 Naive Bayes - Logistic  0.007520   -4.323  0.00004223\n```\n:::\n\n\n<a href=\"https://aml4td.org/chapters/comparing-models.html#sec-post-hoc\" >{{< fa solid rotate-left size=small >}}</a>\n\n#### Comparing Performance using Equivalence Tests {#sec-comparing-equivalence}\n\nWe'll conduct the Two One-Sided Test (TOST) procedure by computing the confidence intervals on the differences, which we can compute via `glht()` with the Tukey correction: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# The defaults that mcp() uses has the opposite order from the books' main\n# test uses (e.g. Logistic - Boosting instead of Boosting - Logistic)\ndifference_int <- \n  freq_mixed_mod |> \n  glht(linfct = mcp(pipeline = \"Tukey\")) |> \n  confint(level = 0.90) |> \n  tidy()\n```\n:::\n\n\nFrom these data, we can create the same plot as the main text: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndifference_int |> \n  ggplot(aes(x = contrast, y = -estimate)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = -conf.low, ymax = -conf.high), width = 1 / 7) +\n  geom_hline(yintercept = c(-0.03, 0.03), linetype = 2) +\n  ylim(-0.07, 0.07) +\n  labs(x = \"Comparison\", y = \"Accuracy Difference\") +\n  scale_y_continuous(label = label_percent()) \n```\n\n::: {.cell-output-display}\n![](comparing-models_files/figure-html/TOST-1.svg){fig-align='center' width=70%}\n:::\n:::\n\n\n<a href=\"https://aml4td.org/chapters/comparing-models.html#sec-comparing-equivalence\" >{{< fa solid rotate-left size=small >}}</a>\n\n### Comparisons Using Bayesian Models {#sec-compare-resample-bayes}\n\nFor Bayesian evaluation, tidymodels replies heavily on the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=rstanarm\">rstanarm</a></span> package, which is based on [Stan](https://mc-stan.org/). The <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidyposterior\">tidyposterior</a></span> package is the main interface for tidymodels objects. The main function, `tidyposterior::perf_mod()`, has methods for several types of objects. First, we’ll show how to use it with a basic data frame of results, then with specific types of tidymodels objects. \n\nWhen the resampled performance estimates are in a data frame, it should be in the \"wide\" format, such as `accuracy_df`. The simplest use of the function is: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(101)\nbasic_bayes_mod <- perf_mod(accuracy_df, refresh = 0)\nbasic_bayes_mod\n#> Bayesian Analysis of Resampling Results\n\n# The underlying Stan model: \nbasic_bayes_mod |> \n  pluck(\"stan\") |> \n  print(digits = 3)\n#> stan_glmer\n#>  family:       gaussian [identity]\n#>  formula:      statistic ~ model + (1 | id)\n#>  observations: 30\n#> ------\n#>                  Median MAD_SD\n#> (Intercept)       0.895  0.011\n#> modelLogistic    -0.004  0.008\n#> modelNaive Bayes -0.037  0.008\n#> \n#> Auxiliary parameter(s):\n#>       Median MAD_SD\n#> sigma 0.018  0.003 \n#> \n#> Error terms:\n#>  Groups   Name        Std.Dev.\n#>  id       (Intercept) 0.03491 \n#>  Residual             0.01857 \n#> Num. levels: id 10 \n#> \n#> ------\n#> * For help interpreting the printed output see ?print.stanreg\n#> * For info on the priors used see ?prior_summary.stanreg\n```\n:::\n\n   \nThe <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=broom.mixed\">broom.mixed</a></span> package's `tidy()` method can also be used:   \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbasic_bayes_mod |> \n  pluck(\"stan\") |> \n  tidy()\n#> # A tibble: 5 × 4\n#>   term                     estimate std.error group   \n#>   <chr>                       <dbl>     <dbl> <chr>   \n#> 1 (Intercept)              0.8950    0.01146  <NA>    \n#> 2 modelLogistic           -0.004410  0.008247 <NA>    \n#> 3 modelNaive Bayes        -0.03717   0.008039 <NA>    \n#> 4 sd_(Intercept).id        0.03491  NA        id      \n#> 5 sd_Observation.Residual  0.01857  NA        Residual\n```\n:::\n\n\n<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidyposterior\">tidyposterior</a></span> also has a `tidy()` method for obtaining a sample from the posterior that has been configured not to be in the format of the parameters but in terms of the model means. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(185)\nbasic_bayes_post <- \n  basic_bayes_mod |> \n  tidy()\n\nbasic_bayes_post\n#> # Posterior samples of performance\n#> # A tibble: 12,000 × 2\n#>   model       posterior\n#>   <chr>           <dbl>\n#> 1 Boosting       0.8913\n#> 2 Logistic       0.9014\n#> 3 Naive Bayes    0.8676\n#> 4 Boosting       0.9014\n#> 5 Logistic       0.9074\n#> 6 Naive Bayes    0.8636\n#> # ℹ 11,994 more rows\n```\n:::\n\n\nThis object also has a summary object to get credible intervals: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(basic_bayes_post,  prob = 0.9)\n#> # A tibble: 3 × 4\n#>   model         mean  lower  upper\n#>   <chr>        <dbl>  <dbl>  <dbl>\n#> 1 Boosting    0.8949 0.8749 0.9156\n#> 2 Logistic    0.8905 0.8700 0.9113\n#> 3 Naive Bayes 0.8579 0.8370 0.8782\n```\n:::\n\n\nand an `autoplot()` method: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(basic_bayes_post)\n```\n\n::: {.cell-output-display}\n![](comparing-models_files/figure-html/tp-autoplot-1.svg){fig-align='center' width=70%}\n:::\n:::\n\n\nTo take a look at pairwise differences, there is `tidyposterior::contrast_models()`. This takes the objects produced by `perf_mod()` and two vector arguments of groups to compare. If these two group arguments are left to their defaults, all comparisons are used: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(185)\nbasic_bayes_diff_post <- \n  basic_bayes_mod |> \n  contrast_models()\n\nbasic_bayes_diff_post\n#> # Posterior samples of performance differences\n#> # A tibble: 12,000 × 4\n#>   difference model_1  model_2  contrast             \n#>        <dbl> <chr>    <chr>    <chr>                \n#> 1 -0.01012   Boosting Logistic Boosting vs. Logistic\n#> 2 -0.005995  Boosting Logistic Boosting vs. Logistic\n#> 3 -0.005884  Boosting Logistic Boosting vs. Logistic\n#> 4  0.0002408 Boosting Logistic Boosting vs. Logistic\n#> 5  0.002266  Boosting Logistic Boosting vs. Logistic\n#> 6  0.01333   Boosting Logistic Boosting vs. Logistic\n#> # ℹ 11,994 more rows\n```\n:::\n\n\nThere is an `autoplot()` method, but the `summary()` method is more interesting since that is where the ROPE analysis can be conducted. Using default arguments gives the 90% credible intervals on the differences: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(basic_bayes_diff_post)\n#> # A tibble: 3 × 9\n#>   contrast        probability     mean     lower   upper  size pract_neg pract_equiv\n#>   <chr>                 <dbl>    <dbl>     <dbl>   <dbl> <dbl>     <dbl>       <dbl>\n#> 1 Boosting vs Lo…      0.6992 0.004403 -0.009418 0.01798     0        NA          NA\n#> 2 Boosting vs Na…      1      0.03701   0.02354  0.05048     0        NA          NA\n#> 3 Logistic vs Na…      0.9998 0.03260   0.01831  0.04643     0        NA          NA\n#> # ℹ 1 more variable: pract_pos <dbl>\n```\n:::\n\n\nUsing the `size` argument enables a ROPE analysis: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(basic_bayes_diff_post, size = 0.03) |> \n  select(contrast, starts_with(\"pract\"))\n#> # A tibble: 3 × 4\n#>   contrast                pract_neg pract_equiv pract_pos\n#>   <chr>                       <dbl>       <dbl>     <dbl>\n#> 1 Boosting vs Logistic       0.0005      0.9962   0.00325\n#> 2 Boosting vs Naive Bayes    0           0.1892   0.8108 \n#> 3 Logistic vs Naive Bayes    0           0.372    0.628\n```\n:::\n\n\nThere are an abundance of options that can be used with `perf_mod()`, including: \n\n- The `transform` argument can convert the metric to a different scale before the analysis and back transforms when creating posteriors. For example, if you believe that $log(RMSE)$ should be used in the analysis, use this argument. \n\n- When we think that the probability distributions of the metrics have different variances, the `hetero_var` is logical and can enable that. Be aware that this option makes the model fit more complex, which may result in convergence issues.\n- Set Stan’s random number seed using the `seed` argument.  \n\nThere are also options that can be passed to the Stan fit, including: \n\n- Sampling-related options such as `chains` (number of MCMC chains) and `cores` (how many things to do in parallel). \n- Prior distributions for parameters. Of interest might be: \n  - `prior_aux`: the prior for the distribution’s scale parameter (e.g. $\\sigma$ for the Gaussian).\n  - `prior_intercept`: the distribution for the random intercepts. \n  - `prior`: distributions for the regression parameters. \n\nFor priors, the [Stan page on priors](https://mc-stan.org/rstanarm/reference/priors.html) is important to read since it details how the default priors are automatically scaled. The [page on sampling](https://mc-stan.org/rstan/reference/stanmodel-method-sampling.html) is also very helpful. \n\nFinally, you can use `perf_mod()` with other types of tidymodels objects. For example, if you want to conduct a _within-model_ analysis where you compare candidates (say from a grid search), you can pass that as the first argument to `perf_mod()`. \n\nIf you have a workflow set, you can perform within- and/or between-model comparisons. This can potentially compare a large number of model/candidate combinations. \n\nWe previously created `forested_wflow_set` to house our resampling results. Here is how we could use that object for the analysis: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(24)\nforested_wflow_set |> \n  perf_mod(\n    seed = 310,\n    iter = 10000,\n    chains = 10,\n    # Don't print a log:\n    refresh = 0,\n    prior_aux = rstanarm::exponential(floor(1/sd(accuracy_long_df$accuracy))), \n    prior_intercept = rstanarm::cauchy(0, 1),\n    prior = rstanarm::normal(0, 5)\n  ) |> \n  contrast_models() |> \n  summary(size = 0.03) |> \n  select(contrast, starts_with(\"pract\"))\n#> # A tibble: 3 × 4\n#>   contrast                pract_neg pract_equiv pract_pos\n#>   <chr>                       <dbl>       <dbl>     <dbl>\n#> 1 Boosting vs Logistic       0.0003      0.9971   0.00264\n#> 2 Boosting vs Naive Bayes    0           0.1941   0.8059 \n#> 3 Logistic vs Naive Bayes    0           0.3749   0.6251\n```\n:::\n\n\n<a href=\"https://aml4td.org/chapters/comparing-models.html#sec-compare-resample-bayes\" >{{< fa solid rotate-left size=small >}}</a>\n\n## Single Holdout Data Sets {#sec-compare-holdout}\n\nFor bootstrap confidence intervals, there is a helper function `rsample::int_pctl()` that has methods that can work for resampled objects (e.g., `tune::fit_resamples()`) and the outputs from the various `tune_*()` functions. \n\nAs an example, _if_ we were using a validation set, we could produce the 90% bootstrap interval for the model via: \n\n\n::: {.cell layout-align=\"center\" mesage='false'}\n\n```{.r .cell-code}\nint_pctl(boost_res, times = 2000, alpha = 0.1)\n```\n:::\n\n\nBy default, this will compute intervals for every tuning parameter candidate and metric in the data (but there are arguments that can be used to restrict the computations to a smaller set).\n\nFor a test set, we encourage users to use `last_fit()`. Let's suppose that we still can't decide if the boosted tree or logistic regression is the final model. We can fit them and evaluate their test set via: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboost_final_res <- \n  last_fit(boost_wflow, split = forested_split, metrics = cls_mtr)\n\nlogistic_final_res <- \n  last_fit(logistic_wflow, split = forested_split, metrics = cls_mtr)\n```\n:::\n\n\nthen use: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(885)\nint_pctl(boost_final_res, times = 2000, alpha = 0.1)\n#> # A tibble: 1 × 6\n#>   .metric  .estimator .lower .estimate .upper .config             \n#>   <chr>    <chr>       <dbl>     <dbl>  <dbl> <chr>               \n#> 1 accuracy bootstrap  0.8651    0.8795 0.8942 Preprocessor1_Model1\n\nset.seed(885)\nint_pctl(logistic_final_res, times = 2000, alpha = 0.1)\n#> # A tibble: 1 × 6\n#>   .metric  .estimator .lower .estimate .upper .config             \n#>   <chr>    <chr>       <dbl>     <dbl>  <dbl> <chr>               \n#> 1 accuracy bootstrap  0.8578    0.8730 0.8877 Preprocessor1_Model1\n```\n:::\n\n\nTo compute intervals on differences, there is no current interface. However, you can use the core functions to get the results. For example, we can join the predictions for different models by their row number: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npaired_class_pred <- \n  logistic_final_res |> \n  collect_predictions() |> \n  select(.row, class, logistic = .pred_class) |> \n  full_join(\n    boost_final_res |> \n      collect_predictions() |> \n      select(.row, boosting = .pred_class),\n    by = \".row\"\n  )\npaired_class_pred\n#> # A tibble: 1,371 × 4\n#>    .row class logistic boosting\n#>   <int> <fct> <fct>    <fct>   \n#> 1  4833 No    No       No      \n#> 2  4834 No    No       No      \n#> 3  4835 No    No       No      \n#> 4  4836 Yes   Yes      Yes     \n#> 5  4837 No    No       No      \n#> 6  4838 Yes   Yes      Yes     \n#> # ℹ 1,365 more rows\n```\n:::\n\n\nthen create bootstrap samples: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(649)\npaired_class_bt <- \n  paired_class_pred |> \n  bootstraps(times = 2000)\n```\n:::\n\n\nWe can write a function to get the difference: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmetric_diff <- function(split) {\n  # Get the bootstrap sample:\n  est <- \n    split |> \n    rsample::analysis() |> \n    # Stack the rows\n    tidyr::pivot_longer(\n      cols = c(-.row, -class),\n      names_to = \"model\",\n      values_to = \"estimate\"\n    ) |> \n    dplyr::group_by(model) |> \n    yardstick::accuracy(class, estimate)\n  tibble::tibble(term = \"Boosting - Logistic\", estimate = -diff(est$.estimate))\n}\n\n# Run on one sample to demonstrate: \npaired_class_bt$splits[[1]] |> metric_diff()\n#> # A tibble: 1 × 2\n#>   term                estimate\n#>   <chr>                  <dbl>\n#> 1 Boosting - Logistic -0.01240\n\n# Run on all:\npaired_class_bt <- \n  paired_class_bt |> \n  mutate(stats = map(splits, metric_diff))\n\nint_pctl(paired_class_bt, stats, alpha = .1)\n#> # A tibble: 1 × 6\n#>   term                   .lower .estimate  .upper .alpha .method   \n#>   <chr>                   <dbl>     <dbl>   <dbl>  <dbl> <chr>     \n#> 1 Boosting - Logistic -0.005106  0.006411 0.01823    0.1 percentile\n```\n:::\n\n\n<a href=\"https://aml4td.org/chapters/comparing-models.html#sec-compare-holdout\" >{{< fa solid rotate-left size=small >}}</a>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}