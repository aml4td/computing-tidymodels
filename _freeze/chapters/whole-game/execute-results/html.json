{
  "hash": "227e809f6cb0265268b0f2035f8ca4d3",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/whole-game/\"\n---\n\n# The Whole Game {#sec-whole-game}\n\nThis [chapter on the main website](https://aml4td.org/chapters/whole-game.html) is a high-level tour of the modeling process. We'll follow the same pattern here by analyzing the same data. We won't reproduce every figure or table but these notes will give you a broad understanding of how the tidymodels framework operates. \n\n## Requirements\n\n\n\nYou’ll need 7 packages (<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=brulee\">brulee</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=Cubist\">Cubist</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=future.mirai\">future.mirai</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=patchwork\">patchwork</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=scales\">scales</a></span>, <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=splines2\">splines2</a></span>, and <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=tidymodels\">tidymodels</a></span>) for this chapter. \nYou can install them via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreq_pkg <- c(\"brulee\", \"Cubist\", \"future.mirai\", \"patchwork\", \"scales\", \"splines2\", \"tidymodels\")\n\n# Check to see if they are installed: \npkg_installed <- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list <- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\n```\n:::\n\n\nOnce you've installed <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=brulee\">brulee</a></span>, you should load it using `library(brulee)` to install the underlying `torch` executables. You only have to do this once. \n\nTwo other packages are described but not directly used: <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=parallel\">parallel</a></span> and <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=future\">future</a></span>. \n\nLet's run some code to get started: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n#> ── Attaching packages ─────────────────────────────────────────── tidymodels 1.3.0 ──\n#> ✔ broom        1.0.8     ✔ recipes      1.2.1\n#> ✔ dials        1.4.0     ✔ rsample      1.3.0\n#> ✔ dplyr        1.1.4     ✔ tibble       3.2.1\n#> ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#> ✔ infer        1.0.8     ✔ tune         1.3.0\n#> ✔ modeldata    1.4.0     ✔ workflows    1.2.0\n#> ✔ parsnip      1.3.1     ✔ workflowsets 1.1.0\n#> ✔ purrr        1.0.4     ✔ yardstick    1.3.2\n#> ── Conflicts ────────────────────────────────────────────── tidymodels_conflicts() ──\n#> ✖ purrr::discard() masks scales::discard()\n#> ✖ dplyr::filter()  masks stats::filter()\n#> ✖ dplyr::lag()     masks stats::lag()\n#> ✖ recipes::step()  masks stats::step()\nlibrary(probably)\n#> \n#> Attaching package: 'probably'\n#> The following objects are masked from 'package:base':\n#> \n#>     as.factor, as.ordered\nlibrary(patchwork)\n\ntidymodels_prefer()\n```\n:::\n\n\nFinally, this note:\n\n::: {.callout-note}\n\nAll of these notes will assume that you have an R session that is running from the root of the directory containing the GitHub repository files. In other words, if you were to execute `list.dirs(recursive = FALSE)`, the output would show entries such as `\"./chapters\"`, `\"./RData\"`, etc.\n\nIf you are not in the right place, use `setwd()` to change the working directory to the correct location. \n\nIf you start by opening the `Rproj` file, you will always start in the right place. \n:::\n\n## The Data {#sec-delivery-times}\n\nThe data set is pre-compiled into a binary format R uses (called \"RData format\" here). It is in the `RData` directory. A `csv` version is also in the `delimited` directory. Let's load it: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nload(\"RData/deliveries.RData\")\n```\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nThere are a lot of ways that you can examine the contents of an object. `View()` is good for data frames; in the RStudio IDE, it opens a spreadsheet-like viewer. `tibble::glimpse()` shows more details about the object, such as classes, but can be a bad choice if you have >50 columns in the data (or if it is a long list or similar). We'll use that: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglimpse(deliveries)\n#> Rows: 10,012\n#> Columns: 31\n#> $ time_to_delivery <dbl> 16.11, 22.95, 30.29, 33.43, 27.23, 19.65, 22.10, 26.63, 3…\n#> $ hour             <dbl> 11.90, 19.23, 18.37, 15.84, 19.62, 12.95, 15.48, 17.05, 1…\n#> $ day              <fct> Thu, Tue, Fri, Thu, Fri, Sat, Sun, Thu, Fri, Sun, Tue, Fr…\n#> $ distance         <dbl> 3.15, 3.69, 2.06, 5.97, 2.52, 3.35, 2.46, 2.21, 2.62, 2.7…\n#> $ item_01          <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n#> $ item_02          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, …\n#> $ item_03          <int> 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n#> $ item_04          <int> 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, …\n#> $ item_05          <int> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_06          <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, …\n#> $ item_07          <int> 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, …\n#> $ item_08          <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, …\n#> $ item_09          <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, …\n#> $ item_10          <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_11          <int> 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_12          <int> 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_13          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_14          <int> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_15          <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_16          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_17          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_18          <int> 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, …\n#> $ item_19          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_20          <int> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_21          <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_22          <int> 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, …\n#> $ item_23          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_24          <int> 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_25          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, …\n#> $ item_26          <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n#> $ item_27          <int> 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, …\n```\n:::\n\n\nWe can see that this is a data frame and, more specifically a specialized version called a [tibble](https://tibble.tidyverse.org). There are 10,012 data points and 31 columns and their types. \n\nNote that the `day` column is a [_factor_](https://r4ds.hadley.nz/factors). This is the preferred way to represent most categorical data (for modeling, at least). A factor catalogs the possible values of the data and stores those _levels_. That is important when we convert categorical predictors to \"dummy variables\" or \"indicators\" and similar operations.  \n\nIn some cases, storing categorical data as integers might seem like a good idea (especially 0/1 for binary data). Do your best *to avoid that*. R (and tidymodels) would instead you use a data type that is designed explicitly for categories (a factor); it knows what to do with factors. If an integer is used, R can't distinguish this from a column of counts (such as the number of times that `item_01` was included in the order). \n\nTo create the histograms of the delivery times, we used this code to create each:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Setup some fancy code for the axes: \nlog_2_breaks <- scales::trans_breaks(\"log2\", function(x) 2^x)\nlog_2_labs   <- scales::trans_format(\"log2\", scales::math_format(2^.x))\n\ndelivery_hist <- \n  deliveries %>% \n  ggplot(aes(x = time_to_delivery)) +\n  geom_histogram(bins = 30, col = \"white\") +\n  geom_rug(alpha = 1 / 4) +\n  labs(x = \"Time Until Delivery (min)\", title = \"(a)\")\n\ndelivery_log_hist <- \n  deliveries %>% \n  ggplot(aes(x = time_to_delivery)) +\n  geom_histogram(bins = 30, col = \"white\") +\n  geom_rug(alpha = 1 / 4) +\n  labs(x = \"Time Until Delivery (min)\", title = \"(b)\") +\n  scale_x_log10(breaks = log_2_breaks, labels = log_2_labs)\n```\n:::\n\n\nYou don't need to assign the plots to objects; you can just print each. We did this so that we can concatenate the two plots with the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=patchwork\">patchwork</a></span> package^[We use a different ggplot theme for the main materials. We'll use the default theme here.]: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndelivery_hist + delivery_log_hist\n```\n\n::: {.cell-output-display}\n![](../figures/whole-game-hists-1.svg){fig-align='center' width=95%}\n:::\n:::\n\n\nIn the code above, we use an option called `\"alpha\"`. This is jargon for transparency; a value of `1/4` means that the points in the rug are 25% opaque. \n\n<a href=\"https://aml4td.org/chapters/whole-game.html#sec-delivery-times\" >{{< fa solid rotate-left size=small >}}</a>\n\n## Data Spending  {#sec-data-spending-whole-game}\n\ntidymodels has a variety of ways to split the data at the outset of your modeling project. We will create a three-way split of the data using a function called `initial_validation_split()`.  \n\nIt uses random numbers so we will set the random number seed before using it. \n\n::: {.callout-note}\n\n# What's a random number seed? \n\nWe are using random numbers (actually [pseudo-random numbers](https://en.wikipedia.org/wiki/Pseudorandomness)). We want to get the same \"random\" values every time we run the same code for reproducibility. To do that, we use the `set.seed()` function and give it an integer value. The value itself doesn't matter. \n\nThe random number stream is like a river. If you want to see the same things in your journey down the river, you must get in at the same exact spot. The seed is like the location where you start a journey (that is always the same). \n\n:::\n\nThe code is below. \n\n - The `prop` argument shows the fraction of the original data that should go into the training set (60%) and the validation set (20%). The remaining 20% are put in the test set. \n \n - The `strata` argument specifies that the splitting should consider the outcome column (`time_to_delivery `). This will be discussed in a future section. In short, the three-way splitting is done in different regions of the outcome data in a way that makes the distribution of the outcome as similar as possible across the three partitions. \n\nWe used a value of 991 to set the seed^[If you are wondering how we get the seed values, I use `sample.int(1000, 1)` to generate random seeds on the fly.]: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(991)\ndelivery_split <-\n  initial_validation_split(deliveries, prop = c(0.6, 0.2), strata = time_to_delivery)\n\n# What is in it? \ndelivery_split\n#> <Training/Validation/Testing/Total>\n#> <6004/2004/2004/10012>\n```\n:::\n\n\nThis object records which rows of the original data go into the training, validation, or test sets. The printed output shows the totals for each as `<train/val/test/total>`. \n\nTo get the data frames with the correct rows, use these three eponymous functions: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndelivery_train <- training(delivery_split)\ndelivery_test  <- testing(delivery_split)\ndelivery_val   <- validation(delivery_split)\n```\n:::\n\n\nWe will mostly work with the training set of 6,004 deliveries. We'll use that to explore the data, fit models, and so on. \n\n<a href=\"https://aml4td.org/chapters/whole-game.html#sec-data-spending-whole-game\" >{{< fa solid rotate-left size=small >}}</a>\n\n## Exploratory Data Analysis {#sec-eda-whole-game}\n\nWe mostly used <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=ggplot2\">ggplot2</a></span> and <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=patchwork\">patchwork</a></span> to create these graphics: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make specific colors for each day\nday_cols <-  c(\"#000000FF\", \"#24FF24FF\", \"#009292FF\",  \"#B66DFFFF\", \n               \"#6DB6FFFF\", \"#920000FF\", \"#FFB6DBFF\")\n\ndelivery_dist <- \n  delivery_train %>% \n  ggplot(aes(x = distance, time_to_delivery)) +\n  geom_point(alpha = 1 / 10, cex = 1) +\n  labs(y = \"Time Until Delivery (min)\", x = \"Distance (miles)\", title = \"(a)\") +\n  # This function creates the smooth trend line. The `se` option shuts off the\n  # confidence band around the line; too much information to put into one plot. \n  geom_smooth(se = FALSE, col = \"red\")\n\ndelivery_day <- \n  delivery_train %>% \n  ggplot(aes(x = day, time_to_delivery, col = day)) +\n  geom_boxplot(show.legend = FALSE)  +\n  labs(y = \"Time Until Delivery (min)\", x = NULL, title = \"(c)\") +\n  scale_color_manual(values = day_cols)\n\ndelivery_time <- \n  delivery_train %>% \n  ggplot(aes(x = hour, time_to_delivery)) +\n  labs(y = \"Time Until Delivery (min)\", x = \"Order Time (decimal hours)\", title = \"(b)\") +\n  geom_point(alpha = 1 / 10, cex = 1) + \n  geom_smooth(se = FALSE, col = \"red\")\n\ndelivery_time_day <- \n  delivery_train %>% \n  ggplot(aes(x = hour, time_to_delivery, col = day)) +\n  labs(y = \"Time Until Delivery (min)\", x = \"Order Time (decimal hours)\", title = \"(d)\") +\n  # With `col = day`, the trends will be estimated separately for each value of 'day'.\n  geom_smooth(se = FALSE) + \n  scale_color_manual(values = day_cols)\n```\n:::\n\n\n<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=patchwork\">patchwork</a></span> puts it together. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Row 1\n( delivery_dist + delivery_time ) / \n  # Row 2\n  ( delivery_day + delivery_time_day ) +\n  # Consolidate the legends\n  plot_layout(guides = 'collect')  & \n  # Place the legend at the bottom\n  theme(legend.title = element_blank(), legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](../figures/delivery-predictors-1.svg){fig-align='center' width=80%}\n:::\n:::\n\n\n<span class=\"pkg\"><a href=\"https://cran.r-project.org/package=ggplot2\">ggplot2</a></span> is a bit noisy. The messages tell you details about how it made the smooth trend line. The code `s(x, bs = \"cs\")` defines a _spline smoother_ that we will see more of shortly (using a different function). \n\nThe methods that we used to compute the effects of the `item_*` columns are more complicated. We must make probabilistic assumptions about the data if we want to get something like a confidence interval. Alternatively, we could specify the empirical distribution function via the bootstrap resampling method. This helps us estimate the standard error of some statistic and use that to compute an interval.\n\nFirst, we make a function that takes some data and [computes our statistics of interest](https://rsample.tidymodels.org/reference/int_pctl.html#arguments). It assumes `x` is the entire data set with the delivery time column and each item column.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntime_ratios <- function(x) {\n  x %>%\n    # The items are in columns; we'll stack these columns on one another.\n    pivot_longer(\n      cols = c(starts_with(\"item\")),\n      names_to = \"predictor\",\n      values_to = \"count\"\n    ) %>%\n    # Collapse the counts into a \"in order\"/\"not in order\" variable. \n    mutate(ordered = ifelse(count > 0, \"yes\", \"no\")) %>%\n    # Compute, for each value of the 'predictor' and 'ordered' columns, \n    # the mean delivery time. \n    summarize(mean = mean(time_to_delivery),\n              .by = c(predictor, ordered)) %>%\n    # Move the means to columns for when they were in the order \n    # and when they were not. The new column names are `yes` and `no`.\n    pivot_wider(id_cols = predictor,\n                names_from = ordered,\n                values_from = mean) %>%\n    # Compute the ratio. This is a fold-difference in delivery times.\n    mutate(ratio = yes / no) %>%\n    select(term = predictor, estimate = ratio)\n}\n```\n:::\n\n\nWhen run in the training set: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntime_ratios(delivery_train)\n#> # A tibble: 27 × 2\n#>   term    estimate\n#>   <chr>      <dbl>\n#> 1 item_01    1.074\n#> 2 item_02    1.010\n#> 3 item_03    1.010\n#> 4 item_04    1.002\n#> 5 item_05    1.005\n#> 6 item_06    1.018\n#> # ℹ 21 more rows\n```\n:::\n\n\nA value of 1.07 means that there is a 7% increase in the delivery time when that item is in the order at least once. \n\nA tidymodels function called `int_pctl()` can take a collection of bootstrap samples of a data set, compute their statistics, and use the results to produce confidence intervals (we'll use 90% intervals). To use it, we'll resample the training set using the `bootstraps()` function and then use a `mutate()` to compute the fold differences. \n \nWe are using random numbers again, so let's reset the seed^[Why are we doing this again? Didn't we already \"jump in the river?\" Yes. If we were executing all of the code here in the exact order (with no typos or commands in between), we would have reproducible pseudo-random numbers. That's usually not how interactive data analysis goes, though. Therefore, we (re)set the seed each time we use randomness.]. \n \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(624)\nresampled_data <- \n  delivery_train %>% \n  select(time_to_delivery, starts_with(\"item\")) %>% \n  # This takes a while to compute. The materials use 5000 bootstraps\n  # but a smaller number is used here for demonstration.\n  bootstraps(times = 1001) \n\nresampled_data\n#> # Bootstrap sampling \n#> # A tibble: 1,001 × 2\n#>   splits              id           \n#>   <list>              <chr>        \n#> 1 <split [6004/2227]> Bootstrap0001\n#> 2 <split [6004/2197]> Bootstrap0002\n#> 3 <split [6004/2156]> Bootstrap0003\n#> 4 <split [6004/2210]> Bootstrap0004\n#> 5 <split [6004/2208]> Bootstrap0005\n#> 6 <split [6004/2227]> Bootstrap0006\n#> # ℹ 995 more rows\n```\n:::\n\n \nThe `splits` column contains the information on each bootstrap sample. To get a specific bootstrap sample, we can use the `analysis(split_object)` function on each element of the `splits` column. `purrr::map()` takes each split, extracts the bootstrap sample, then computes all of the ratios^[This `map()` call can be made much faster by using the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=furrr\">furrr</a></span> package. It has versions of the `purrr::map()` functions that can run in parallel.]. \n \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampled_ratios <- \n  resampled_data %>% \n  mutate(stats = map(splits, ~ time_ratios(analysis(.x))))\n\nresampled_ratios\n#> # Bootstrap sampling \n#> # A tibble: 1,001 × 3\n#>   splits              id            stats            \n#>   <list>              <chr>         <list>           \n#> 1 <split [6004/2227]> Bootstrap0001 <tibble [27 × 2]>\n#> 2 <split [6004/2197]> Bootstrap0002 <tibble [27 × 2]>\n#> 3 <split [6004/2156]> Bootstrap0003 <tibble [27 × 2]>\n#> 4 <split [6004/2210]> Bootstrap0004 <tibble [27 × 2]>\n#> 5 <split [6004/2208]> Bootstrap0005 <tibble [27 × 2]>\n#> 6 <split [6004/2227]> Bootstrap0006 <tibble [27 × 2]>\n#> # ℹ 995 more rows\n\n# An example: \nresampled_ratios$stats[[1]]\n#> # A tibble: 27 × 2\n#>   term    estimate\n#>   <chr>      <dbl>\n#> 1 item_01    1.070\n#> 2 item_02    1.018\n#> 3 item_03    1.008\n#> 4 item_04    1.008\n#> 5 item_05    1.017\n#> 6 item_06    1.007\n#> # ℹ 21 more rows\n```\n:::\n\n\n`rsample::int_pctl()` can consume these results and produce an interval for each item column^[You can see another example of bootstrap intervals at [tidymodels.org](https://www.tidymodels.org/learn/statistics/bootstrap/).]. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampled_intervals <- \n  resampled_ratios %>% \n  int_pctl(stats, alpha = 0.1) \n\nresampled_intervals\n#> # A tibble: 27 × 6\n#>   term    .lower .estimate .upper .alpha .method   \n#>   <chr>    <dbl>     <dbl>  <dbl>  <dbl> <chr>     \n#> 1 item_01 1.052      1.075  1.097    0.1 percentile\n#> 2 item_02 0.9950     1.009  1.024    0.1 percentile\n#> 3 item_03 0.9940     1.009  1.024    0.1 percentile\n#> 4 item_04 0.9879     1.002  1.016    0.1 percentile\n#> 5 item_05 0.9884     1.005  1.021    0.1 percentile\n#> 6 item_06 1.002      1.018  1.033    0.1 percentile\n#> # ℹ 21 more rows\n```\n:::\n\n\nHere's our plot: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampled_intervals %>% \n  # Convert the folds to percentages and make the item values\n  # a little cleaner:\n  mutate(\n    term = gsub(\"_0\", \" \", term),\n    term = factor(gsub(\"_\", \" \", term)),\n    term = reorder(term, .estimate),\n    increase = .estimate - 1,\n  ) %>% \n  ggplot(aes(increase, term)) + \n  geom_vline(xintercept = 0, col = \"red\", alpha = 1 / 3) +\n  geom_point() + \n  geom_errorbar(aes(xmin = .lower - 1, xmax = .upper - 1), width = 1 / 2) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(y = NULL, x = \"Increase in Delivery Time When Ordered\") +\n  theme(axis.text.y = element_text(hjust = 0))\n```\n\n::: {.cell-output-display}\n![](../figures/time-ratio-plot-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n<a href=\"https://aml4td.org/chapters/whole-game.html#sec-eda-whole-game\" >{{< fa solid rotate-left size=small >}}</a>\n\n## Model Development {#sec-model-development-whole-game}\n\nThe analyses in this section define a model pipeline, fit it to the training set, and then measure performance using the validation set. We'll review the three evaluated models and describe how those computations were done.\n\nBefore we get started, we need to specify how to measure model effectiveness. The materials use the mean absolute error (MAE). To specify this _performance metric_, you can use the `yardstick::metric_set()` function and give it the function names for specific metrics (like the `yardstick::mae()` function): \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreg_metrics <- metric_set(mae)\n```\n:::\n\n\nWe'll show you how to use `reg_metrics` in a bit.\n\n### Linear Regression {.unnumbered}\n\nThe linear regression model is fairly simple to define and fit. Before we get to that, we must introduce a major tidymodels component: **the recipe**. \n\nA recipe is a set of instructions defining a potential series of computations on the predictor variables to put them into a format the model (or data set) requires. For example, the day-of-the-week factor must be converted into a numeric format. We'll use the standard \"Dummy variable\" approach to do that. Additionally, our exploratory data analysis discovered that: \n\n* There is a nonlinear relationship between the outcome and the time of the order. \n* This nonlinear relationship is different for different days. This is an interaction effect between a qualitative predictor (`day`) and a nonlinear function of another (`hour`).\n* There also appeared to be an additional nonlinear effect for the order distance. \n\nWe can initialize a recipe using a simple formula method: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspline_rec <- recipe(time_to_delivery ~ ., data = delivery_train)\n```\n:::\n\n\nThere are a few things that this function call does: \n\n* The formula declares that the column `time_to_delivery` is the outcome (since it is on the left-hand side of the tilde). The dot on the right-hand side indicates that all of the columns in `delivery_train`, besides the outcome, should be treated as predictors. \n* The recipe collects information on each column's _type_. For example, it understands that `day` is a factor and that the `item_*` columns are numeric. \n\nLet's add to the recipe by converting `day` to indicator columns. We do this by adding a step to the recipe via: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspline_rec <- \n  recipe(time_to_delivery ~ ., data = delivery_train) %>% \n  step_dummy(all_factor_predictors()) \n```\n:::\n\n\nThe first argument to step functions is the variables that should be affected by the function. We can use any <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=dplyr\">dplyr</a></span> [selector](https://dplyr.tidyverse.org/reference/select.html) such as `everything()` and/or the bare column names. Here, we want to change every factor column that was the role of \"predictor\". For this purpose, recipes have an [extended set of selector functions](https://recipes.tidymodels.org/reference/selections.html). \n\nOnce the recipe is processed, this step will record which columns were captured by `all_factor_predictors()`, retain their factor levels, then convert them to a set of 0/1 indicators for each predictor/level. \n\nUnlike base R's formula method, the resulting columns are named rationally. By default, it uses the pattern `{column name}_{level}` for the new features. So, the column `day` will not exist after this step. It is replaced with columns such as `day_Thursday` and so on. \n\nThe next recipe step is probably unnecessary for this data set but automatically using it is not problematic. What happens if there is a factor level that occurs very infrequently? It is possible that this will only be observed in the validation or test set. `step_dummy()` will make a column for that factor level since it knows it exists but the training set will have all zeros for this column; it has zero variance. We can screen these out using `step_zv()` ('zv' = zero-variance): \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspline_rec <- \n  recipe(time_to_delivery ~ ., data = delivery_train) %>% \n  step_dummy(all_factor_predictors()) %>% \n  step_zv(all_predictors()) \n```\n:::\n\n\nNow, we can address the nonlinear effects. We'll use a spline basis expansion (described later on the main page) that creates additional columns from some numeric predictor. We'll use a _natural spline_ function and create ten new columns for both `hour` and `distance`: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspline_rec <- \n  recipe(time_to_delivery ~ ., data = delivery_train) %>% \n  step_dummy(all_factor_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_spline_natural(hour, distance, deg_free = 10)\n```\n:::\n\n\nThe naming convention for these new features are `hour_01` ...  `hour_10` and so on. The original `hour` column is removed (same for the `distance` column). \n\nThis step allows the linear regression to have nonlinear relationships between predictors and the outcome. \n\nFinally, we can create interactions. In base R, an interaction between variables `a` and `b` is specified in the formula using `a:b`. We'll use the same method here with `step_interact()`. The main difference is that the columns `day` and `hour` no longer exist at this point. To capture all of the interactions, we can use the `:` convention with selector functions. Using `starts_wth(\"day_\")` will capture the existing indicator columns and, similarly, `starts_wth(\"hour_\")` finds the appropriate spline terms. Our final recipe is then:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspline_rec <- \n  recipe(time_to_delivery ~ ., data = delivery_train) %>% \n  step_dummy(all_factor_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_spline_natural(hour, distance, deg_free = 10) %>% \n  step_interact(~ starts_with(\"hour_\"):starts_with(\"day_\"))\n```\n:::\n\n\n::: {.callout-note}\n# Learn More About Recipes\n\nYou can learn more about recipes later and there is material in the tidymodels book as well as `tidymodels.org`. \n\n - [_Feature Engineering with recipes_](https://www.tmwr.org/recipes)\n - [_Dimensionality Reduction_](https://www.tmwr.org/dimensionality)\n - [_Encoding Categorical Data_](https://www.tmwr.org/categorical)\n - [_Get Started: Preprocess your data with recipes_](https://www.tidymodels.org/start/recipes/)\n - [Articles with recipes](https://www.tidymodels.org/learn/#category=recipes)\n - [A list of recipe steps on CRAN](https://www.tidymodels.org/find/recipes/)\n \n:::\n\nTo specify the linear regression model, we use one of the functions from the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=parsnip\">parsnip</a></span> package called... `linear_reg()`. Since we are using ordinary least squares, this function defaults to `stats::lm().` \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This creates a model specification: \nlm_spec <- linear_reg()\nlm_spec\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: lm\n```\n:::\n\n\nThe _engine_ mentioned here is the computational method to fit the model. R has many ways to do this and `\"lm\"` is the _default engine_. \n\nHow do we combine the recipe and the model specifications? The best approach is to make a pipeline-like object called a _workflow_: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlin_reg_wflow <- \n  workflow() %>% \n  add_model(lm_spec) %>% \n  add_recipe(spline_rec)\n```\n:::\n\n\nWe can use the `fit()` function to fit the workflow to the training set. This executes the recipe on the data set then passes the appropriate data to `stats::lm()`: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlin_reg_fit <- fit(lin_reg_wflow, data = delivery_train)\n```\n:::\n\n\nWe can print the results out but the results are kind of long:\n\n<details>\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlin_reg_fit\n#> ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: linear_reg()\n#> \n#> ── Preprocessor ─────────────────────────────────────────────────────────────────────\n#> 4 Recipe Steps\n#> \n#> • step_dummy()\n#> • step_zv()\n#> • step_spline_natural()\n#> • step_interact()\n#> \n#> ── Model ────────────────────────────────────────────────────────────────────────────\n#> \n#> Call:\n#> stats::lm(formula = ..y ~ ., data = data)\n#> \n#> Coefficients:\n#>       (Intercept)            item_01            item_02            item_03  \n#>           13.3434             1.2444             0.6461             0.7313  \n#>           item_04            item_05            item_06            item_07  \n#>            0.2820             0.5839             0.5250             0.5063  \n#>           item_08            item_09            item_10            item_11  \n#>            0.6376             0.7368             1.5341             0.5172  \n#>           item_12            item_13            item_14            item_15  \n#>            0.5945             0.5799             0.5080             0.6432  \n#>           item_16            item_17            item_18            item_19  \n#>            0.4309             0.6089             0.4028            -0.3568  \n#>           item_20            item_21            item_22            item_23  \n#>            0.5280             0.7743             0.5105             0.6876  \n#>           item_24            item_25            item_26            item_27  \n#>            0.8126             0.4768             0.5274             0.5773  \n#>           day_Tue            day_Wed            day_Thu            day_Fri  \n#>           -1.1200            -2.8421            -3.7059            -4.3130  \n#>           day_Sat            day_Sun            hour_01            hour_02  \n#>           -4.6659            -3.0919             1.8776             2.5588  \n#>           hour_03            hour_04            hour_05            hour_06  \n#>            2.5826             3.8464             2.5826             4.0220  \n#>           hour_07            hour_08            hour_09            hour_10  \n#>            4.0112             5.9056            -0.8782            11.1905  \n#>       distance_01        distance_02        distance_03        distance_04  \n#>           -0.0523             0.5777             0.6157             0.7524  \n#>       distance_05        distance_06        distance_07        distance_08  \n#>            1.6695             1.7837             2.9039             3.3200  \n#>       distance_09        distance_10  hour_01_x_day_Tue  hour_01_x_day_Wed  \n#>          -19.3835            75.2868             1.3304             1.2875  \n#> hour_01_x_day_Thu  hour_01_x_day_Fri  hour_01_x_day_Sat  hour_01_x_day_Sun  \n#>            2.4448             1.3560             2.4253             1.5058  \n#> hour_02_x_day_Tue  hour_02_x_day_Wed  hour_02_x_day_Thu  hour_02_x_day_Fri  \n#>            0.2268             3.1441             3.9958             4.8608  \n#> hour_02_x_day_Sat  hour_02_x_day_Sun  hour_03_x_day_Tue  hour_03_x_day_Wed  \n#>            4.8380             3.7002             2.9359             6.4783  \n#> hour_03_x_day_Thu  hour_03_x_day_Fri  hour_03_x_day_Sat  hour_03_x_day_Sun  \n#>            7.4590             9.9423            10.9673             5.7549  \n#> hour_04_x_day_Tue  hour_04_x_day_Wed  hour_04_x_day_Thu  hour_04_x_day_Fri  \n#>            1.6701             6.2609             9.7183            12.1586  \n#> hour_04_x_day_Sat  hour_04_x_day_Sun  hour_05_x_day_Tue  hour_05_x_day_Wed  \n#>           13.9715             7.9502             3.9981             9.4129  \n#> hour_05_x_day_Thu  hour_05_x_day_Fri  hour_05_x_day_Sat  hour_05_x_day_Sun  \n#>           12.1748            16.5402            17.5038             9.3518  \n#> hour_06_x_day_Tue  hour_06_x_day_Wed  hour_06_x_day_Thu  hour_06_x_day_Fri  \n#>            2.5202             8.2079            11.9678            15.7150  \n#> hour_06_x_day_Sat  hour_06_x_day_Sun  hour_07_x_day_Tue  hour_07_x_day_Wed  \n#> \n#> ...\n#> and 14 more lines.\n```\n:::\n\n\n</details>\n\nOne helpful function is `tidy()`. It is designed to return the object results rationally, helpfully. In our case, the `tidy()` method for an `lm` object gives us a nice data frame back with information on the fitted coefficients: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(lin_reg_fit)\n#> # A tibble: 114 × 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)  13.34     1.585       8.420 4.664e-17\n#> 2 item_01       1.244    0.1031     12.08  3.498e-33\n#> 3 item_02       0.6461   0.06865     9.412 6.852e-21\n#> 4 item_03       0.7313   0.06914    10.58  6.463e-26\n#> 5 item_04       0.2820   0.06260     4.505 6.776e- 6\n#> 6 item_05       0.5839   0.07871     7.418 1.360e-13\n#> # ℹ 108 more rows\n```\n:::\n\n\nUnlike the `summary()` method for `lm` objects, this object can immediately be used in plots or tables. \n\nAnother valuable supporting function is `augment()`. It can take a model object and data set and attach the prediction columns to the data frame. Essentially, this is an upgraded version of `predict()`. Let's predict the validation set: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_reg_val_pred <- augment(lin_reg_fit, new_data = delivery_val)\nnames(lm_reg_val_pred)\n#>  [1] \".pred\"            \".resid\"           \"time_to_delivery\" \"hour\"            \n#>  [5] \"day\"              \"distance\"         \"item_01\"          \"item_02\"         \n#>  [9] \"item_03\"          \"item_04\"          \"item_05\"          \"item_06\"         \n#> [13] \"item_07\"          \"item_08\"          \"item_09\"          \"item_10\"         \n#> [17] \"item_11\"          \"item_12\"          \"item_13\"          \"item_14\"         \n#> [21] \"item_15\"          \"item_16\"          \"item_17\"          \"item_18\"         \n#> [25] \"item_19\"          \"item_20\"          \"item_21\"          \"item_22\"         \n#> [29] \"item_23\"          \"item_24\"          \"item_25\"          \"item_26\"         \n#> [33] \"item_27\"\n```\n:::\n\n\nWhat is our MAE? This is where we use our metric set `reg_metrics`. Note that there is a column in the results called `.pred`. For regression models, this is the predicted delivery time for each order in the validation set. We can use that and the original observed outcome column to estimate the MAE^[We could have used the `yardstick::mae()` directly instead of stuffing that function in a metric set. Since we often want to collect more than one type of performance statistic, we're showing how to use a metric set.]: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_reg_val_pred %>% \n  reg_metrics(truth = time_to_delivery, estimate = .pred)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 mae     standard       1.609\n```\n:::\n\n\nThe units are fractional minutes. \n\nAt this point, we can make diagnostic plots of our data and so on. \n\nLet's take a minor distraction that will pay off a bit later. The main page mentions that we can treat the validation set as a single resample of the data. If we were to do that, our code wouldn't have to change much when we get into more complex scenarios such as cross-validation or model tuning. To do this, we can convert the initial split object into a resampling set (a.k.a. an `rset`): \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndelivery_rs <- validation_set(delivery_split)\n\nclass(delivery_rs)\n#> [1] \"validation_set\" \"rset\"           \"tbl_df\"         \"tbl\"           \n#> [5] \"data.frame\"\n\ndelivery_rs\n#> # A tibble: 1 × 2\n#>   splits              id        \n#>   <list>              <chr>     \n#> 1 <split [6004/2004]> validation\n```\n:::\n\n\nThis packages the training and validation sets together in a way that it knows when to use each data set appropriately. \n\nSince we are treating this as if it were resampling, we can use the `fit_resamples()` function to do much of the manual work we just showed. We'll add a _control object_ to the mix to specify that we want to retain the validation set predictions (and our original workflow).  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nctrl_rs <- control_resamples(save_pred = TRUE, save_workflow = TRUE)\nlin_reg_res <-\n  fit_resamples(lin_reg_wflow,\n                resamples = delivery_rs,\n                control = ctrl_rs,\n                metrics = reg_metrics)\n```\n:::\n\n\nThe benefit here is that there are a lot of _helper_ functions to simplify your code. For example, to get the validation set MAE and predictions: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(lin_reg_res)\n#> # A tibble: 1 × 6\n#>   .metric .estimator  mean     n std_err .config             \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 mae     standard   1.609     1      NA Preprocessor1_Model1\n\ncollect_predictions(lin_reg_res)\n#> # A tibble: 2,004 × 5\n#>   .pred id          .row time_to_delivery .config             \n#>   <dbl> <chr>      <int>            <dbl> <chr>               \n#> 1 30.12 validation  6005            27.23 Preprocessor1_Model1\n#> 2 23.02 validation  6006            22.10 Preprocessor1_Model1\n#> 3 28.38 validation  6007            26.63 Preprocessor1_Model1\n#> 4 31.04 validation  6008            30.84 Preprocessor1_Model1\n#> 5 38.57 validation  6009            41.17 Preprocessor1_Model1\n#> 6 27.04 validation  6010            27.04 Preprocessor1_Model1\n#> # ℹ 1,998 more rows\n```\n:::\n\n\nThe <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=probably\">probably</a></span> package also has a nice helper to check the calibration of the model via a plot: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncal_plot_regression(lin_reg_res)\n```\n\n::: {.cell-output-display}\n![](../figures/lm-cal-plot-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n<a href=\"https://aml4td.org/chapters/whole-game.html#linear-regression\" >{{< fa solid rotate-left size=small >}}</a>\n\n### Rule-Based Ensemble {.unnumbered}\n\nTo fit the Cubist model, we need to load one of the tidymodels _extension packages_ called <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=rules\">rules</a></span>. It has the tools to fit this model and will automatically (and silently) use the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=Cubist\">Cubist</a></span> package when the model fit occurs. \n\nWe'll create a model specification that has an enselmble size of 100:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rules)\n\n# A model specification: \ncb_spec <- cubist_rules(committees = 100)\n```\n:::\n\n\nOne advantage of rule-based models is that very little preprocessing is required (i.e., no dummy variables or spline terms). For that reason, we'll use a simple R formula instead of a recipe: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncb_wflow <- \n  workflow() %>% \n  add_model(cb_spec) %>% \n  add_formula(time_to_delivery ~ .)\n```\n:::\n\n\nLet's go straight to `fit_resamples()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncb_res <-\n  fit_resamples(\n    cb_wflow, \n    resamples = delivery_rs, \n    control = ctrl_rs, \n    metrics = reg_metrics\n  )\n\ncollect_metrics(cb_res)\n#> # A tibble: 1 × 6\n#>   .metric .estimator  mean     n std_err .config             \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 mae     standard   1.416     1      NA Preprocessor1_Model1\n```\n:::\n\n\nThe calibration plot: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncal_plot_regression(cb_res)\n```\n\n::: {.cell-output-display}\n![](../figures/cb-cal-plot-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\nThis is pretty simple and demonstrates that, after an initial investment in learning tidymodels syntax, the process of fitting different models does not require huge changes to your scripts. \n\nTo get the model fit, we previously used `fit()`. With resampling objects (and the tuning objects that we are about to see), there is another helper function called `fit_best()` that will create the model from the entire training set using the resampling results^[This is possible since we previously used the `save_workflow = TRUE` option in the control function.]: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncb_fit <- fit_best(cb_res)\n\ncb_fit\n#> ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n#> Preprocessor: Formula\n#> Model: cubist_rules()\n#> \n#> ── Preprocessor ─────────────────────────────────────────────────────────────────────\n#> time_to_delivery ~ .\n#> \n#> ── Model ────────────────────────────────────────────────────────────────────────────\n#> \n#> Call:\n#> cubist.default(x = x, y = y, committees = 100)\n#> \n#> Number of samples: 6004 \n#> Number of predictors: 30 \n#> \n#> Number of committees: 100 \n#> Number of rules per committee: 31, 22, 23, 26, 21, 24, 23, 23, 22, 23, 21, 22, 18, 24, 20, 22, 16, 26, 25, 26 ...\n```\n:::\n\n\nThe `tidy()` method is also helpful here. It contains all of the rules and corresponding regression models. Let's get these values for the second rule in the fourth ensemble:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrule_details <- tidy(cb_fit)\n\nrule_details %>% \n  filter(committee == 4 & rule_num == 2) %>% \n  pluck(\"rule\")\n#> [1] \"( hour <= 14.946 ) & ( day  %in% c( 'Mon','Tue','Wed','Thu','Sun' ) ) & ( distance <= 4.52 )\"\n\nrule_details %>% \n  filter(committee == 4 & rule_num == 2) %>% \n  select(estimate) %>% \n  pluck(1) %>% \n  pluck(1)\n#> # A tibble: 15 × 2\n#>   term        estimate\n#>   <chr>          <dbl>\n#> 1 (Intercept)   -11.24\n#> 2 hour            2.05\n#> 3 distance        0.78\n#> 4 item_01        -0.5 \n#> 5 item_02         0.3 \n#> 6 item_05         0.8 \n#> # ℹ 9 more rows\n```\n:::\n\n\n<a href=\"https://aml4td.org/chapters/whole-game.html#rule-based-ensemble\" >{{< fa solid rotate-left size=small >}}</a>\n\n### Neural Network {.unnumbered}\n\nThe model function for this type of model is `parsnip::mlp()` (MLP is short for \"multi-layer perceptron\"). There are quite a few packages for neural networks in R. tidymodels has interfaces to several engines: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshow_engines(\"mlp\")\n#> # A tibble: 8 × 2\n#>   engine mode          \n#>   <chr>  <chr>         \n#> 1 keras  classification\n#> 2 keras  regression    \n#> 3 nnet   classification\n#> 4 nnet   regression    \n#> 5 brulee classification\n#> 6 brulee regression    \n#> # ℹ 2 more rows\n```\n:::\n\n\nWe'll use the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=brulee\">brulee</a></span> package. This uses the `torch` software to fit the model. We'll only tune the number of hidden units (for now, see later chapters). To mark _any_ parameter for tuning, we pass the `tune()` function to an argument: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnnet_spec <- \n  mlp(\n    hidden_units = tune(),\n    # Some specific argument values that we chose:\n    penalty = 0.01,\n    learn_rate = 0.1,\n    epochs = 5000\n  ) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"brulee\", stop_iter = 10, rate_schedule = \"cyclic\")\n```\n:::\n\n\nA few notes on this: \n\n* The arguments to `mlp()` are called the _main arguments_ since they are used by several engines. \n* The default engine uses the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=nnet\">nnet</a></span> package; `set_engine()` specifies that we want to use the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=brulee\">brulee</a></span> package instead. \n* Two arguments (`stop_iter` and `rate_schedule`) are specific to our engine. We set them here (and can also pass a `tune()` value to them). \n*  Neural networks can fit both classification and regression models. We must state what model type (called a \"mode\") to create. \n\nThis model requires the conversion to dummy variables but does not require features to handle nonlinear trends and interactions. One additional preprocessing step that is required is to put the predictors in the same units (e.g., not miles or hours). There are a few ways to do this. We will center and scale the predictors using `recipes::step_normalize()`: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnorm_rec <- \n  recipe(time_to_delivery ~ ., data = delivery_train) %>% \n  step_dummy(all_factor_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors())\n\nnnet_wflow <- \n  workflow() %>% \n  add_model(nnet_spec) %>% \n  add_recipe(norm_rec)\n```\n:::\n\n\nUnlike the previous models, we are tuning one of the hyper-parameters. Instead of `fit_resamples()`, we'll use `tune_grid()` to search over a predefined set of values for the number of hidden units. We can set _how many_ values we should try or directly declare the candidate values. We'll do the latter and, for simplicity, use a smaller range of values. \n\nFinally, we'll use another control function: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# The main materials used 2:100\nnnet_grid <- tibble(hidden_units = 2:10)\n\nctrl_grid <- control_grid(save_pred = TRUE, save_workflow = TRUE)\n\n# The model initializes the parameters with random numbers so set the seed:\nset.seed(388)\nnnet_res <-\n  tune_grid(nnet_wflow,\n            resamples = delivery_rs,\n            control = ctrl_grid,\n            grid = nnet_grid,\n            metrics = reg_metrics)\n```\n:::\n\n\nThere are some additional helper functions for model tuning. For example, we can rank the models based on a metric: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshow_best(nnet_res, metric = \"mae\")\n#> # A tibble: 5 × 7\n#>   hidden_units .metric .estimator  mean     n std_err .config             \n#>          <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1            9 mae     standard   1.499     1      NA Preprocessor1_Model8\n#> 2            6 mae     standard   1.522     1      NA Preprocessor1_Model5\n#> 3           10 mae     standard   1.522     1      NA Preprocessor1_Model9\n#> 4            7 mae     standard   1.525     1      NA Preprocessor1_Model6\n#> 5            5 mae     standard   1.550     1      NA Preprocessor1_Model4\n```\n:::\n\n\nWe can also return the parameter with the numerically best results^[As impressive as the `torch` ecosystem is, it is not as optimized for reproducibility. These results may vary from run to run due to the inability to fix some of the random numbers used and their use of different numerical tolerances across operating systems.]: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbest_hidden_units <- select_best(nnet_res, metric = \"mae\")\nbest_hidden_units\n#> # A tibble: 1 × 2\n#>   hidden_units .config             \n#>          <int> <chr>               \n#> 1            9 Preprocessor1_Model8\n```\n:::\n\n\nThe `autoplot()` method can visualize the relationship between the tuning parameter(s) and the performance metric(s). \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(nnet_res, metric = \"mae\")\n```\n\n::: {.cell-output-display}\n![](../figures/nnet-autoplot-1.svg){fig-align='center' width=70%}\n:::\n:::\n\n\n`tune::collect_predictions()` will automatically return the out-of-sample predictions for every candidate model (e.g., every tuning parameter value.). We might not want them all; it has an argument called `parameters` that can be used to filter the results. \n\n`probably::cal_plot_regression()` automatically shows the results for each tuning parameter combination. For example: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncal_plot_regression(nnet_res)\n```\n\n::: {.cell-output-display}\n![](../figures/nnet-cal-plot-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\nThere are two options if we need a model fit on the training set. If the numerically best parameter is best (i.e., smallest MAE), then `tune::fit_best()` is the easiest approach. Alternatively, you can choose the exact tuning parameter values you desire and _splice_ them into the model to replace the current values of `tune()`. To do this, there is a `finalize_workflow()` function. It takes a data frame with one row and columns for each tuning parameter. Here's an example where we decide that 10 hidden units are best: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(814)\nnnet_fit <- \n  nnet_wflow %>% \n  finalize_workflow(tibble(hidden_units = 10)) %>% \n  fit(data = delivery_train)\n```\n:::\n\n\n<a href=\"https://aml4td.org/chapters/whole-game.html#neural-network\" >{{< fa solid rotate-left size=small >}}</a>\n\n## Aside: Parallel Processing {#sec-parallel-processing}\n\nFor model tuning, we are fitting many models. With grid search, these models are not dependent on one another. For this reason, it is possible to compute these model fits simultaneously (i.e., in parallel).  \n\nTo do so, tidymodels requires you to specify a _parallel backend_. There are several types, and we will use the mirai system since it works on all operating systems. For this technology, we can run the following commands before running `fit_resamples()` or any of the `tune_*()` functions: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncores <- parallel::detectCores(logical = FALSE)\nlibrary(future.mirai)\nplan(mirai_multisession, workers = cores)\n```\n:::\n\n\nThere can be significant speed-ups when running in parallel. See the [section in the tidymodels book](https://www.tmwr.org/grid-search#parallel-processing) for more details. \n\n## Calibration\n\nThe functions to calibrate predictions are in the <span class=\"pkg\"><a href=\"https://cran.r-project.org/package=probably\">probably</a></span> package and have names that start with `cal_*`. There are methods that work on the results from `fit_resamples()` or the `tune_*()` functions, but you can also just use a data frame of predicted values. \n\nWe must estimate the trend with the validation set. If we use our object `lin_reg_res`, it knows what data to use: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlin_reg_cal <- cal_estimate_linear(lin_reg_res)\n#> Registered S3 method overwritten by 'butcher':\n#>   method                 from    \n#>   as.character.dev_topic generics\nlin_reg_cal\n#> \n#> ── Regression Calibration\n#> Method: Generalized additive model\n#> Source class: Tune Results\n#> Data points: 2,004\n#> Truth variable: `time_to_delivery`\n#> Estimate variable: `.pred`\n```\n:::\n\n\nAs you'll see in a minute, the function `probably::cal_apply()` calibrates new predictions.  \n \n<a href=\"https://aml4td.org/chapters/whole-game.html#sec-calibration-whole-game\" >{{< fa solid rotate-left size=small >}}</a> \n \n## Test Set Results {#sec-test-results-whole-game}\n\nAs with `best_fit()`, there are two ways to predict the test set.\n\nThe more manual approach is to fit the model on the training set, use `predict()` or `augment()` to compute the test set predictions, calibrate them with our object, then use our metric to compute performance. If we had not already fit the model, the pre-calibration code is: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlin_reg_fit <- fit(lin_reg_wflow, delivery_train)\nlin_reg_test_pred <- augment(lin_reg_fit, delivery_test)\n\nlin_reg_test_pred %>% \n  reg_metrics(time_to_delivery, .pred)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 mae     standard       1.607\n\n# plot the uncalibrated results: \nlin_reg_test_pred %>% \n  cal_plot_regression(truth = time_to_delivery, estimate = .pred)\n```\n\n::: {.cell-output-display}\n![](../figures/lm-test-final-uncal-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\nThere is a shortcut for the first three commands. `tune::last_fit()` takes our initial split object and automatically does the rest (but not calibration yet): \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlin_reg_test_res <- \n  lin_reg_wflow %>% \n  last_fit(delivery_split, metrics = reg_metrics)\n```\n:::\n\n\nWe can pull out the elements we need from this object using some `extract_*()` and `collect_*()` functions. Here are a few: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Test set metrics:\ncollect_metrics(lin_reg_test_res)\n#> # A tibble: 1 × 4\n#>   .metric .estimator .estimate .config             \n#>   <chr>   <chr>          <dbl> <chr>               \n#> 1 mae     standard       1.607 Preprocessor1_Model1\n\n# Test set predictions: \ncollect_predictions(lin_reg_test_res)\n#> # A tibble: 2,004 × 5\n#>   .pred id                .row time_to_delivery .config             \n#>   <dbl> <chr>            <int>            <dbl> <chr>               \n#> 1 15.98 train/test split     7            18.02 Preprocessor1_Model1\n#> 2 16.03 train/test split    14            17.57 Preprocessor1_Model1\n#> 3 27.60 train/test split    16            26.71 Preprocessor1_Model1\n#> 4 17.15 train/test split    29            17.64 Preprocessor1_Model1\n#> 5 32.24 train/test split    33            32.19 Preprocessor1_Model1\n#> 6 20.18 train/test split    34            20.31 Preprocessor1_Model1\n#> # ℹ 1,998 more rows\n\n# Final model fit: \nlin_reg_fit <- extract_fit_parsnip(lin_reg_test_res)\n\n# cal_plot_regression(lin_reg_test_res)\n```\n:::\n\n\nNow let's calibrate and compute performance: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# apply calibration\nlin_reg_test_pred_cal <- \n  lin_reg_test_pred %>% \n  cal_apply(lin_reg_cal)\n\nlin_reg_test_pred_cal %>% \n  reg_metrics(time_to_delivery, .pred)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 mae     standard       1.545\n\n# plot the calibrated results: \nlin_reg_test_pred_cal %>% \n  cal_plot_regression(truth = time_to_delivery, estimate = .pred)\n```\n\n::: {.cell-output-display}\n![](../figures/lm-test-final-cal-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n<a href=\"https://aml4td.org/chapters/whole-game.html#sec-test-results-whole-game\" >{{< fa solid rotate-left size=small >}}</a> \n\n## Conclusion\n\nThis has been an abbreviated, high-level introduction to using tidymodels. Future chapters will go into much more detail on these subjects and illustrate additional features and functions as needed. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}