[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidymodels Computing Supplement",
    "section": "",
    "text": "Preface\nThis is a computing supplement to the main website that uses the tidymodels framework for modeling. The structure is similar to the website, but the content here shows how to use this software (and sometimes others) for each topic.\nWe also want these materials to be reusable and open. The sources are in the source GitHub repository with a Creative Commons license attached (see below).\nTo cite this work, we suggest:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Tidymodels Computing Supplement",
    "section": "License",
    "text": "License\n\nThis work is licensed under CC BY-SA 4.0",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Tidymodels Computing Supplement",
    "section": "Intended Audience",
    "text": "Intended Audience\nReaders should have used R before but do not have to be experts. If you are new to R, we suggest taking a look at R for Data Science.\nYou do not have to be a modeling expert either. We hope that you have used a linear or logistic regression before and understand basic statistical concepts such as correlation, variability, probabilities, etc.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-can-i-ask-questions",
    "href": "index.html#how-can-i-ask-questions",
    "title": "Tidymodels Computing Supplement",
    "section": "How can I ask questions?",
    "text": "How can I ask questions?\nIf you have questions about the content, it is probably best to ask on a public forum, like cross-validated or Posit Community. You’ll most likely get a faster answer there if you take the time to ask the questions in the best way possible.\nIf you want a direct answer from us, you should follow what I call Yihui’s Rule: add an issue to GitHub (labeled as “Discussion”) first. It may take some time for us to get back to you.\nIf you think there is a bug, please file an issue.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#can-i-contribute",
    "href": "index.html#can-i-contribute",
    "title": "Tidymodels Computing Supplement",
    "section": "Can I contribute?",
    "text": "Can I contribute?\nThere is a contributing page with details on how to get up and running to compile the materials (there are a lot of software dependencies) and suggestions on how to help.\nIf you just want to fix a typo, you can make a pull request to alter the appropriate .qmd file.\nPlease feel free to improve the quality of this content by submitting pull requests. A merged PR will make you appear in the contributor list.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#computing-notes",
    "href": "index.html#computing-notes",
    "title": "Tidymodels Computing Supplement",
    "section": "Computing Notes",
    "text": "Computing Notes\nQuarto was used to compile and render the materials\n\n\nQuarto 1.7.31\n[✓] Checking environment information...\n[✓] Checking versions of quarto binary dependencies...\n      Pandoc version 3.6.3: OK\n      Dart Sass version 1.85.1: OK\n      Deno version 1.46.3: OK\n      Typst version 0.13.0: OK\n[✓] Checking versions of quarto dependencies......OK\n[✓] Checking Quarto installation......OK\n      Version: 1.7.31\n[✓] Checking tools....................OK\n      TinyTeX: (external install)\n      Chromium: (not installed)\n[✓] Checking LaTeX....................OK\n      Using: TinyTex\n      Version: 2025\n[✓] Checking Chrome Headless....................OK\n      Using: Chrome found on system\n      Source: MacOS known location\n[✓] Checking basic markdown render....OK\n[✓] Checking Python 3 installation....OK\n      Version: 3.9.6\n      Jupyter: (None)\n      Jupyter is not available in this Python installation.\n[✓] Checking R installation...........OK\n      Version: 4.5.0\n      LibPaths:\n      knitr: 1.50\n      rmarkdown: 2.29\n[✓] Checking Knitr engine render......OK\n\n\nR version 4.5.0 (2025-04-11) was used for the majority of the computations. torch 2.5.1 was also used. The versions of the primary R modeling and visualization packages used here are:\n\n\n\n\n\n\n\n\n\naorsf (0.1.5)\n\nbestNormalize (1.9.1)\n\nbroom (1.0.8)\n\n\n\nbroom.mixed (0.2.9.6)\n\nbrulee (0.5.0)\n\nC50 (0.2.0)\n\n\n\ncaret (7.0-1)\n\ncolino (0.0.1)\n\nCubist (0.5.0)\n\n\n\ndials (1.4.0)\n\ndimRed (0.2.7)\n\ndiscrim (1.0.1)\n\n\n\ndoParallel (1.0.17)\n\ndownlit (0.4.4.9000)\n\ndplyr (1.1.4)\n\n\n\ne1071 (1.7-16)\n\nembed (1.1.5)\n\nemmeans (1.11.1)\n\n\n\nfastICA (1.2-7)\n\nfinetune (1.2.1)\n\nforested (0.1.0)\n\n\n\nfuture.mirai (0.10.0)\n\nGA (3.2.4)\n\nggplot2 (3.5.2)\n\n\n\nglmnet (4.1-9)\n\ngt (1.0.0)\n\nhardhat (1.4.1)\n\n\n\nhstats (1.2.1)\n\nigraph (2.1.4)\n\njanitor (2.2.1)\n\n\n\nklaR (1.7-3)\n\nlme4 (1.1-37)\n\nMatrix (1.7-3)\n\n\n\nmixOmics (6.32.0)\n\nmodeldata (1.4.0)\n\nmodeldatatoo (0.3.0)\n\n\n\nnaniar (1.1.0)\n\nparsnip (1.3.2)\n\npartykit (1.2-24)\n\n\n\npatchwork (1.3.1)\n\nprobably (1.1.0)\n\npurrr (1.0.4)\n\n\n\nQSARdata (1.3)\n\nragg (1.4.0)\n\nranger (0.17.0)\n\n\n\nRANN (2.6.2)\n\nrecipes (1.2.1)\n\nrmarkdown (2.29)\n\n\n\nrsample (1.3.0)\n\nRSpectra (0.16-2)\n\nrstudioapi (0.17.1)\n\n\n\nrules (1.0.2)\n\nsf (1.0-21)\n\nsfd (0.1.0)\n\n\n\nspatialsample (0.6.0)\n\nsplines2 (0.5.4)\n\nstopwords (2.3)\n\n\n\ntextrecipes (1.1.0)\n\ntidymodels (1.3.0)\n\ntidyposterior (1.0.1)\n\n\n\ntidyr (1.3.1)\n\ntidysdm (1.0.0)\n\ntorch (0.15.0)\n\n\n\ntune (1.3.0)\n\nusethis (3.1.0)\n\nviridis (0.6.5)\n\n\n\nworkflows (1.2.0)\n\nworkflowsets (1.1.1)\n\nxgboost (1.7.11.1)\n\n\n\nxml2 (1.3.8)\n\nyardstick (1.3.2)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/news.html",
    "href": "chapters/news.html",
    "title": "News",
    "section": "",
    "text": "Changelog",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section",
    "href": "chapters/news.html#section",
    "title": "News",
    "section": "2025-06-24",
    "text": "2025-06-24\nA new chapter (14  Comparing Models) on model comparisons was added, concluding Part 3 of the book.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-1",
    "href": "chapters/news.html#section-1",
    "title": "News",
    "section": "2025-05-21",
    "text": "2025-05-21\nNew chapters on iterative optimization and feature selection.\nWhenever a section has a corresponding computing supplement, an icon on the main site is shown that directly links to that section. In this computing supplement, you can get back to the main site’s section via this icon: .\nWe also move to mirai for parallel processing.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-2",
    "href": "chapters/news.html#section-2",
    "title": "News",
    "section": "2025-01-17",
    "text": "2025-01-17\nSmall update that standardized the url anchors.\nSome sections were reorganized.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-3",
    "href": "chapters/news.html#section-3",
    "title": "News",
    "section": "2024-09-23",
    "text": "2024-09-23\nComputing chapters caught up to the website (chapter 11, grid search)",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-4",
    "href": "chapters/news.html#section-4",
    "title": "News",
    "section": "2023-12-11",
    "text": "2023-12-11\nAdded a chapter on initial data splitting.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/contributing.html",
    "href": "chapters/contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "Software\nPlease note that the aml4td project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.\nIf you plan to do anything beyond fixing a typo, the best thing you can do is to open an issue and discuss changes before you spend a lot of time doing them.\nIf you don’t have a lot of experience with git or GitHub, take a look at the wonderful Happy Git and GitHub for the useR.\nIf you want to contribute, some general advice is:\nA merged PR will make you appear in the contributor list (see below). It will, however, be considered a donation of your work to this project. You are still bound by the conditions of the license, meaning that you are not considered an author, copyright holder, or owner of the content once it has been merged in.\nYou will mostly work with the *.qmd files in the chapters directory.\nHere is a list of the elements in the repo:\nRegarding R packages, the repository has a DESCRIPTION file as if it were an R package. This lets us specify precisely what packages and their versions should be installed. The packages listed in the imports field contain packages for modeling/analysis and packages used to make the website/book. Some basic system requirements are likely needed to install packages: Fortran, gdal, and others.\nThe main requirements are as follows.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing.html#software",
    "href": "chapters/contributing.html#software",
    "title": "Contributing",
    "section": "",
    "text": "Quarto\nQuarto is an open-source scientific and technical publishing system. Quarto version 1.7.31 is used to compile the website.\nR and renv\nR version 4.5.0 (2025-04-11) is what we are currently using. We suggest using rig to manage R versions. There are several IDEs that you can use. We’ve used RStudio (&gt;= 2023.6.1.524).\nThe current strategy is to use the renv (&gt;= version 1.1.4) package to make this project more isolated, portable and reproducible.\nTo get package dependencies installed…\nWhen you open the computing-tidymodels.Rproj file, the renv package should be automatically installed/updated (if needed). For example:\n# Bootstrapping renv 1.0.3 ---------------------------------------------------\n- Downloading renv ... OK\n- Installing renv  ... OK\n\nThe following package(s) will be installed:\n- BiocManager [1.30.22]\nThese packages will be installed into \"~/content/aml4td/renv/library/R-4.3/x86_64-apple-darwin20\".\nif you try to compile the book, you probably get and error:\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nYou can get more information using renv::status() but you can get them installed by first running renv::activate(). As an example:\n&gt; renv::activate()\n\nRestarting R session...\n\n- Project '~/content/aml4td' loaded. [renv 1.0.3]\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nSince we have package versions recorded in the lockfile, we can installed them using renv::restore(). Here is an example of that output:\n&gt; renv::restore() \nThe following package(s) will be updated:\n\n# Bioconductor ---------------------------------------------------------------\n- mixOmics         [* -&gt; mixOmicsTeam/mixOmics]\n\n# CRAN -----------------------------------------------------------------------\n- BiocManager      [1.30.22 -&gt; 1.30.21.1]\n- lattice          [0.21-9 -&gt; 0.21-8]\n- Matrix           [1.6-1.1 -&gt; 1.6-0]\n- nlme             [3.1-163 -&gt; 3.1-162]\n- rpart            [4.1.21 -&gt; 4.1.19]\n- survival         [3.5-7 -&gt; 3.5-5]\n- abind            [* -&gt; 1.4-5]\n\n&lt;snip&gt;\n\n- zip              [* -&gt; 2.2.0]\n- zoo              [* -&gt; 1.8-12]\n\n# GitHub ---------------------------------------------------------------------\n- BiocParallel     [* -&gt; Bioconductor/BiocParallel@devel]\n- BiocVersion      [* -&gt; Bioconductor/BiocVersion@devel]\n- modeldatatoo     [* -&gt; tidymodels/modeldatatoo@HEAD]\n- parsnip          [* -&gt; tidymodels/parsnip@HEAD]\n- usethis          [* -&gt; r-lib/usethis@HEAD]\n\n# RSPM -----------------------------------------------------------------------\n- bslib            [* -&gt; 0.5.1]\n- fansi            [* -&gt; 1.0.5]\n- fontawesome      [* -&gt; 0.5.2]\n- ggplot2          [* -&gt; 3.4.4]\n- htmltools        [* -&gt; 0.5.6.1]\n- withr            [* -&gt; 2.5.1]\n\nDo you want to proceed? [Y/n]: y\n\n# Downloading packages -------------------------------------------------------\n- Downloading BiocManager from CRAN ...         OK [569 Kb in 0.19s]\n- Downloading nlme from CRAN ...                OK [828.7 Kb in 0.19s]\n- Downloading BH from CRAN ...                  OK [12.7 Mb in 0.4s]\n- Downloading BiocVersion from GitHub ...       OK [826 bytes in 0.37s]\n\n&lt;snip&gt;\n\nDepending on whether you have to install packages from source, you may need to install some system dependencies and try again (I had to install libgit2 the last time I did this).\nOnce you have everything installed, we recommend installing the underlying torch computational libraries. You can do this by loading the torch package A download will automatically begin if you need one.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing.html#contributor-list",
    "href": "chapters/contributing.html#contributor-list",
    "title": "Contributing",
    "section": "Contributor List",
    "text": "Contributor List\nThe would like to thank users who have made a contribution to the project: @jrosell.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Installation\ntidymodels is a framework for creating statistical and machine learning models in R. The framework consists of a set of tightly coupled R packages that are designed in the same way. The project began in late 2016.\nThe main tidymodels resources are:\nWe’ll reference these and other resources as needed.\ntidymodels is built in R so you’ll need to install that. We used R version 4.5.0 (2025-04-11) for these notes. To install R, you can go to CRAN1 to download it for your operating system. If you are comfortable at the command line, the rig application is an excellent way to install and manage R versions.\nYou probably want to use an integrated development environment (IDE); it will make your life much better. We use the RStudio IDE, which can be downloaded here. Other applications are Visual Studio and emacs.\nTo use tidymodels, you need to install multiple packages. The core packages are bundled into a “verse” package called tidymodels. When you install that, you get the primary packages as well as some tidyverse packages such as dplyr and ggplot2.\nTo install it, you can use\nWe suggest using the pak package for installation. To do this, first install that and then use it for further installations:\ninstall.packages(\"pak\")\n\n# check that it is installed then use it to install tidymodels\nif (require(pak)) {\n  pak::pak(\"tidymodels\")\n}",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#installation",
    "href": "chapters/introduction.html#installation",
    "title": "1  Introduction",
    "section": "",
    "text": "install.packages(\"tidymodels\")",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-tidymodels-pkg",
    "href": "chapters/introduction.html#sec-tidymodels-pkg",
    "title": "1  Introduction",
    "section": "\n1.2 Loading Tidymodels",
    "text": "1.2 Loading Tidymodels\nOnce you do that, load tidymodels:\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────────────── tidymodels 1.3.0 ──\n#&gt; ✔ broom        1.0.8     ✔ recipes      1.2.1\n#&gt; ✔ dials        1.4.0     ✔ rsample      1.3.0\n#&gt; ✔ dplyr        1.1.4     ✔ tibble       3.3.0\n#&gt; ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#&gt; ✔ infer        1.0.8     ✔ tune         1.3.0\n#&gt; ✔ modeldata    1.4.0     ✔ workflows    1.2.0\n#&gt; ✔ parsnip      1.3.2     ✔ workflowsets 1.1.1\n#&gt; ✔ purrr        1.0.4     ✔ yardstick    1.3.2\n#&gt; ── Conflicts ────────────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\n\nThe default output shows the packages that are automatically attached. There are a lot of functions in tidy models, but by loading this meta-package, you don’t have to remember which functions come from which packages.\nNote the lines at the bottom that messages like :\n\ndplyr::filter() masks stats::filter()\n\nThis means that two packages, dplyr and stats, have functions with the same name (filter())2. If you were to type filter at an R prompt, the function that you get corresponds to the one in the most recently loaded package. That’s not ideal.\nTo handle this, we have a function called tidymodels_prefer(). When you use this, it prioritizes functions from the tidy models and tidyverse groups so that you get those 3 If you want to see the specific conflicts and how we resolve them, see this output:\n\ntidymodels_prefer(quiet = FALSE)\n#&gt; [conflicted] Will prefer agua::refit over any other package.\n#&gt; [conflicted] Will prefer DALEX::explain over any other package.\n#&gt; [conflicted] Will prefer dials::Laplace over any other package.\n#&gt; [conflicted] Will prefer dials::max_rules over any other package.\n#&gt; [conflicted] Will prefer dials::neighbors over any other package.\n#&gt; [conflicted] Will prefer dials::prune over any other package.\n#&gt; [conflicted] Will prefer dials::smoothness over any other package.\n#&gt; [conflicted] Will prefer dplyr::collapse over any other package.\n#&gt; [conflicted] Will prefer dplyr::combine over any other package.\n#&gt; [conflicted] Will prefer dplyr::filter over any other package.\n#&gt; [conflicted] Will prefer dplyr::rename over any other package.\n#&gt; [conflicted] Will prefer dplyr::select over any other package.\n#&gt; [conflicted] Will prefer dplyr::slice over any other package.\n#&gt; [conflicted] Will prefer ggplot2::`%+%` over any other package.\n#&gt; [conflicted] Will prefer ggplot2::margin over any other package.\n#&gt; [conflicted] Will prefer parsnip::bart over any other package.\n#&gt; [conflicted] Will prefer parsnip::fit over any other package.\n#&gt; [conflicted] Will prefer parsnip::mars over any other package.\n#&gt; [conflicted] Will prefer parsnip::pls over any other package.\n#&gt; [conflicted] Will prefer purrr::cross over any other package.\n#&gt; [conflicted] Will prefer purrr::invoke over any other package.\n#&gt; [conflicted] Will prefer purrr::map over any other package.\n#&gt; [conflicted] Will prefer recipes::discretize over any other package.\n#&gt; [conflicted] Will prefer recipes::step over any other package.\n#&gt; [conflicted] Will prefer recipes::update over any other package.\n#&gt; [conflicted] Will prefer rsample::populate over any other package.\n#&gt; [conflicted] Will prefer scales::rescale over any other package.\n#&gt; [conflicted] Will prefer themis::step_downsample over any other package.\n#&gt; [conflicted] Will prefer themis::step_upsample over any other package.\n#&gt; [conflicted] Will prefer tidyr::expand over any other package.\n#&gt; [conflicted] Will prefer tidyr::extract over any other package.\n#&gt; [conflicted] Will prefer tidyr::pack over any other package.\n#&gt; [conflicted] Will prefer tidyr::unpack over any other package.\n#&gt; [conflicted] Will prefer tune::parameters over any other package.\n#&gt; [conflicted] Will prefer tune::tune over any other package.\n#&gt; [conflicted] Will prefer yardstick::get_weights over any other package.\n#&gt; [conflicted] Will prefer yardstick::precision over any other package.\n#&gt; [conflicted] Will prefer yardstick::recall over any other package.\n#&gt; [conflicted] Will prefer yardstick::spec over any other package.\n#&gt; [conflicted] Removing existing preference.\n#&gt; [conflicted] Will prefer recipes::update over Matrix::update.\n#&gt; ── Conflicts ───────────────────────────────────────────────── tidymodels_prefer() ──\n\nIf you want to know more about why tidymodels exists, we’ve written a bit about this in the tidymodels book. The second chapter describes how tidyverse principles can be used for modeling.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-repro",
    "href": "chapters/introduction.html#sec-repro",
    "title": "1  Introduction",
    "section": "\n1.3 Package Versions and Reproducability",
    "text": "1.3 Package Versions and Reproducability\nWe will do our best to use versions of our packages corresponding to the CRAN versions. We can’t always do that, and, for many packages, a version number ending with a value in the 9000 range (e.g., version “1.1.4.9001”) means that it was a development version of the package and was most likely installed from a GitHub repository.\nAt the end of each session, we’ll show which packages were loaded and used:\n\nsessioninfo::session_info()\n#&gt; ─ Session info ────────────────────────────────────────────────────────────────────\n#&gt;  setting  value\n#&gt;  version  R version 4.5.0 (2025-04-11)\n#&gt;  os       macOS Sequoia 15.5\n#&gt;  system   aarch64, darwin20\n#&gt;  ui       X11\n#&gt;  language (EN)\n#&gt;  collate  en_US.UTF-8\n#&gt;  ctype    en_US.UTF-8\n#&gt;  tz       America/New_York\n#&gt;  date     2025-06-27\n#&gt;  pandoc   3.6.3 @ /Applications/quarto/bin/tools/ (via rmarkdown)\n#&gt;  quarto   1.7.31 @ /usr/local/bin/quarto\n#&gt; \n#&gt; ─ Packages ────────────────────────────────────────────────────────────────────────\n#&gt;  ! package      * version    date (UTC) lib source\n#&gt;  P backports      1.5.0      2024-05-23 [?] RSPM\n#&gt;  P BiocManager    1.30.26    2025-06-05 [?] CRAN (R 4.5.0)\n#&gt;  P broom        * 1.0.8      2025-03-28 [?] CRAN (R 4.5.0)\n#&gt;  P cachem         1.1.0      2024-05-16 [?] RSPM\n#&gt;  P class          7.3-23     2025-01-01 [?] CRAN (R 4.5.0)\n#&gt;  P cli            3.6.5      2025-04-23 [?] RSPM (R 4.5.0)\n#&gt;  P codetools      0.2-20     2024-03-31 [?] CRAN (R 4.5.0)\n#&gt;  P conflicted     1.2.0      2023-02-01 [?] RSPM\n#&gt;  P data.table     1.17.6     2025-06-17 [?] CRAN (R 4.5.0)\n#&gt;  P dials        * 1.4.0      2025-02-13 [?] RSPM\n#&gt;  P DiceDesign     1.10       2023-12-07 [?] RSPM\n#&gt;  P digest         0.6.37     2024-08-19 [?] RSPM\n#&gt;  P downlit        0.4.4.9000 2025-05-21 [?] Github (r-lib/downlit@cdaa2f1)\n#&gt;  P dplyr        * 1.1.4      2023-11-17 [?] RSPM\n#&gt;  P evaluate       1.0.4      2025-06-18 [?] CRAN (R 4.5.0)\n#&gt;  P farver         2.1.2      2024-05-13 [?] RSPM\n#&gt;  P fastmap        1.2.0      2024-05-15 [?] RSPM\n#&gt;  P foreach        1.5.2      2022-02-02 [?] RSPM\n#&gt;  P furrr          0.3.1      2022-08-15 [?] RSPM\n#&gt;  P future         1.58.0     2025-06-05 [?] CRAN (R 4.5.0)\n#&gt;  P future.apply   1.20.0     2025-06-06 [?] CRAN (R 4.5.0)\n#&gt;  P generics       0.1.4      2025-05-09 [?] CRAN (R 4.5.0)\n#&gt;  P ggplot2      * 3.5.2      2025-04-09 [?] RSPM\n#&gt;  P globals        0.18.0     2025-05-08 [?] RSPM\n#&gt;  P glue           1.8.0      2024-09-30 [?] RSPM (R 4.5.0)\n#&gt;  P gower          1.0.2      2024-12-17 [?] RSPM\n#&gt;  P GPfit          1.0-9      2025-04-12 [?] RSPM\n#&gt;  P gtable         0.3.6      2024-10-25 [?] RSPM\n#&gt;  P hardhat        1.4.1      2025-01-31 [?] RSPM\n#&gt;  P htmltools      0.5.8.1    2024-04-04 [?] RSPM\n#&gt;  P htmlwidgets    1.6.4      2023-12-06 [?] CRAN (R 4.5.0)\n#&gt;  P infer        * 1.0.8      2025-04-14 [?] RSPM\n#&gt;  P ipred          0.9-15     2024-07-18 [?] RSPM\n#&gt;  P iterators      1.0.14     2022-02-05 [?] CRAN (R 4.5.0)\n#&gt;  P jsonlite       2.0.0      2025-03-27 [?] RSPM\n#&gt;  P knitr          1.50       2025-03-16 [?] CRAN (R 4.5.0)\n#&gt;  P lattice        0.22-7     2025-04-02 [?] RSPM\n#&gt;  P lava           1.8.1      2025-01-12 [?] RSPM\n#&gt;  P lhs            1.2.0      2024-06-30 [?] RSPM\n#&gt;  P lifecycle      1.0.4      2023-11-07 [?] RSPM (R 4.5.0)\n#&gt;  P listenv        0.9.1      2024-01-29 [?] CRAN (R 4.5.0)\n#&gt;  P lubridate      1.9.4      2024-12-08 [?] CRAN (R 4.5.0)\n#&gt;  P magrittr       2.0.3      2022-03-30 [?] RSPM (R 4.5.0)\n#&gt;  P MASS           7.3-65     2025-02-28 [?] CRAN (R 4.5.0)\n#&gt;  P Matrix         1.7-3      2025-03-11 [?] CRAN (R 4.5.0)\n#&gt;  P memoise        2.0.1      2021-11-26 [?] RSPM\n#&gt;  P modeldata    * 1.4.0      2024-06-19 [?] RSPM\n#&gt;  P nnet           7.3-20     2025-01-01 [?] CRAN (R 4.5.0)\n#&gt;  P parallelly     1.45.0     2025-06-02 [?] CRAN (R 4.5.0)\n#&gt;  P parsnip      * 1.3.2      2025-05-28 [?] local\n#&gt;  P pillar         1.10.2     2025-04-05 [?] RSPM\n#&gt;  P pkgconfig      2.0.3      2019-09-22 [?] RSPM\n#&gt;  P prodlim        2025.04.28 2025-04-28 [?] RSPM\n#&gt;  P purrr        * 1.0.4      2025-02-05 [?] RSPM (R 4.5.0)\n#&gt;  P R6             2.6.1      2025-02-15 [?] RSPM\n#&gt;  P RColorBrewer   1.1-3      2022-04-03 [?] RSPM\n#&gt;  P Rcpp           1.0.14     2025-01-12 [?] RSPM\n#&gt;  P recipes      * 1.2.1      2025-05-21 [?] Github (tidymodels/recipes@96bd6fd)\n#&gt;    renv           1.1.4      2025-03-20 [1] RSPM\n#&gt;  P rlang          1.1.6      2025-04-11 [?] RSPM (R 4.5.0)\n#&gt;  P rmarkdown      2.29       2024-11-04 [?] RSPM\n#&gt;  P rpart          4.1.24     2025-01-07 [?] CRAN (R 4.5.0)\n#&gt;  P rsample      * 1.3.0      2025-04-02 [?] RSPM\n#&gt;  P rstudioapi     0.17.1     2024-10-22 [?] CRAN (R 4.5.0)\n#&gt;  P scales       * 1.4.0      2025-04-24 [?] CRAN (R 4.5.0)\n#&gt;  P sessioninfo    1.2.3      2025-02-05 [?] RSPM\n#&gt;  P survival       3.8-3      2024-12-17 [?] CRAN (R 4.5.0)\n#&gt;  P tibble       * 3.3.0      2025-06-08 [?] CRAN (R 4.5.0)\n#&gt;  P tidymodels   * 1.3.0      2025-02-21 [?] RSPM\n#&gt;  P tidyr        * 1.3.1      2024-01-24 [?] RSPM\n#&gt;  P tidyselect     1.2.1      2024-03-11 [?] RSPM\n#&gt;  P timechange     0.3.0      2024-01-18 [?] RSPM\n#&gt;  P timeDate       4041.110   2024-09-22 [?] RSPM\n#&gt;  P tune         * 1.3.0      2025-02-21 [?] RSPM\n#&gt;  P vctrs          0.6.5      2023-12-01 [?] RSPM (R 4.5.0)\n#&gt;  P withr          3.0.2      2024-10-28 [?] RSPM\n#&gt;  P workflows    * 1.2.0      2025-02-19 [?] CRAN (R 4.5.0)\n#&gt;  P workflowsets * 1.1.1      2025-05-27 [?] CRAN (R 4.5.0)\n#&gt;  P xfun           0.52       2025-04-02 [?] RSPM\n#&gt;  P yaml           2.3.10     2024-07-26 [?] CRAN (R 4.5.0)\n#&gt;  P yardstick    * 1.3.2      2025-01-22 [?] CRAN (R 4.5.0)\n#&gt; \n#&gt;  [1] /Users/max/github/computing-tidymodels/renv/library/macos/R-4.5/aarch64-apple-darwin20\n#&gt;  [2] /Users/max/Library/Caches/org.R-project.R/R/renv/sandbox/macos/R-4.5/aarch64-apple-darwin20/4cd76b74\n#&gt; \n#&gt;  * ── Packages attached to the search path.\n#&gt;  P ── Loaded and on-disk path mismatch.\n#&gt; \n#&gt; ───────────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#footnotes",
    "href": "chapters/introduction.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "The Comprehensive R Archive Network↩︎\nThe syntax foo::bar() means that the function bar() is inside of the package foo When used together, this is often referred to as “calling the function by its namespace.”. You can do this in your code, and developers often do. However, it’s fairly ugly. ↩︎\nUnfortunately, this is not a guarantee but it does work most of the time.↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html",
    "href": "chapters/whole-game.html",
    "title": "2  The Whole Game",
    "section": "",
    "text": "2.1 Requirements\nThis chapter on the main website is a high-level tour of the modeling process. We’ll follow the same pattern here by analyzing the same data. We won’t reproduce every figure or table but these notes will give you a broad understanding of how the tidymodels framework operates.\nYou’ll need 7 packages (brulee, Cubist, future.mirai, patchwork, scales, splines2, and tidymodels) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"brulee\", \"Cubist\", \"future.mirai\", \"patchwork\", \"scales\", \"splines2\", \"tidymodels\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nOnce you’ve installed brulee, you should load it using library(brulee) to install the underlying torch executables. You only have to do this once.\nTwo other packages are described but not directly used: parallel and future.\nLet’s run some code to get started:\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────────────── tidymodels 1.3.0 ──\n#&gt; ✔ broom        1.0.8     ✔ recipes      1.2.1\n#&gt; ✔ dials        1.4.0     ✔ rsample      1.3.0\n#&gt; ✔ dplyr        1.1.4     ✔ tibble       3.3.0\n#&gt; ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#&gt; ✔ infer        1.0.8     ✔ tune         1.3.0\n#&gt; ✔ modeldata    1.4.0     ✔ workflows    1.2.0\n#&gt; ✔ parsnip      1.3.2     ✔ workflowsets 1.1.1\n#&gt; ✔ purrr        1.0.4     ✔ yardstick    1.3.2\n#&gt; ── Conflicts ────────────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\nlibrary(probably)\n#&gt; \n#&gt; Attaching package: 'probably'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.factor, as.ordered\nlibrary(patchwork)\n\ntidymodels_prefer()\nFinally, this note:",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#requirements",
    "href": "chapters/whole-game.html#requirements",
    "title": "2  The Whole Game",
    "section": "",
    "text": "Note\n\n\n\nAll of these notes will assume that you have an R session that is running from the root of the directory containing the GitHub repository files. In other words, if you were to execute list.dirs(recursive = FALSE), the output would show entries such as \"./chapters\", \"./RData\", etc.\nIf you are not in the right place, use setwd() to change the working directory to the correct location.\nIf you start by opening the Rproj file, you will always start in the right place.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-delivery-times",
    "href": "chapters/whole-game.html#sec-delivery-times",
    "title": "2  The Whole Game",
    "section": "\n2.2 The Data",
    "text": "2.2 The Data\nThe data set is pre-compiled into a binary format R uses (called “RData format” here). It is in the RData directory. A csv version is also in the delimited directory. Let’s load it:\n\nload(\"RData/deliveries.RData\")\n\nThere are a lot of ways that you can examine the contents of an object. View() is good for data frames; in the RStudio IDE, it opens a spreadsheet-like viewer. tibble::glimpse() shows more details about the object, such as classes, but can be a bad choice if you have &gt;50 columns in the data (or if it is a long list or similar). We’ll use that:\n\nglimpse(deliveries)\n#&gt; Rows: 10,012\n#&gt; Columns: 31\n#&gt; $ time_to_delivery &lt;dbl&gt; 16.11, 22.95, 30.29, 33.43, 27.23, 19.65, 22.10, 26.63, 3…\n#&gt; $ hour             &lt;dbl&gt; 11.90, 19.23, 18.37, 15.84, 19.62, 12.95, 15.48, 17.05, 1…\n#&gt; $ day              &lt;fct&gt; Thu, Tue, Fri, Thu, Fri, Sat, Sun, Thu, Fri, Sun, Tue, Fr…\n#&gt; $ distance         &lt;dbl&gt; 3.15, 3.69, 2.06, 5.97, 2.52, 3.35, 2.46, 2.21, 2.62, 2.7…\n#&gt; $ item_01          &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n#&gt; $ item_02          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, …\n#&gt; $ item_03          &lt;int&gt; 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n#&gt; $ item_04          &lt;int&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_05          &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_06          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, …\n#&gt; $ item_07          &lt;int&gt; 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, …\n#&gt; $ item_08          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, …\n#&gt; $ item_09          &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, …\n#&gt; $ item_10          &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_11          &lt;int&gt; 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_12          &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_13          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_14          &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_15          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_16          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_17          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_18          &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, …\n#&gt; $ item_19          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_20          &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_21          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_22          &lt;int&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, …\n#&gt; $ item_23          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_24          &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_25          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, …\n#&gt; $ item_26          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_27          &lt;int&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, …\n\nWe can see that this is a data frame and, more specifically a specialized version called a tibble. There are 10,012 data points and 31 columns and their types.\nNote that the day column is a factor. This is the preferred way to represent most categorical data (for modeling, at least). A factor catalogs the possible values of the data and stores those levels. That is important when we convert categorical predictors to “dummy variables” or “indicators” and similar operations.\nIn some cases, storing categorical data as integers might seem like a good idea (especially 0/1 for binary data). Do your best to avoid that. R (and tidymodels) would instead you use a data type that is designed explicitly for categories (a factor); it knows what to do with factors. If an integer is used, R can’t distinguish this from a column of counts (such as the number of times that item_01 was included in the order).\nTo create the histograms of the delivery times, we used this code to create each:\n\n# Setup some fancy code for the axes: \nlog_2_breaks &lt;- scales::trans_breaks(\"log2\", function(x) 2^x)\nlog_2_labs   &lt;- scales::trans_format(\"log2\", scales::math_format(2^.x))\n\ndelivery_hist &lt;- \n  deliveries %&gt;% \n  ggplot(aes(x = time_to_delivery)) +\n  geom_histogram(bins = 30, col = \"white\") +\n  geom_rug(alpha = 1 / 4) +\n  labs(x = \"Time Until Delivery (min)\", title = \"(a)\")\n\ndelivery_log_hist &lt;- \n  deliveries %&gt;% \n  ggplot(aes(x = time_to_delivery)) +\n  geom_histogram(bins = 30, col = \"white\") +\n  geom_rug(alpha = 1 / 4) +\n  labs(x = \"Time Until Delivery (min)\", title = \"(b)\") +\n  scale_x_log10(breaks = log_2_breaks, labels = log_2_labs)\n\nYou don’t need to assign the plots to objects; you can just print each. We did this so that we can concatenate the two plots with the patchwork package1:\n\ndelivery_hist + delivery_log_hist\n\n\n\n\n\n\n\nIn the code above, we use an option called \"alpha\". This is jargon for transparency; a value of 1/4 means that the points in the rug are 25% opaque.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-data-spending-whole-game",
    "href": "chapters/whole-game.html#sec-data-spending-whole-game",
    "title": "2  The Whole Game",
    "section": "\n2.3 Data Spending",
    "text": "2.3 Data Spending\ntidymodels has a variety of ways to split the data at the outset of your modeling project. We will create a three-way split of the data using a function called initial_validation_split().\nIt uses random numbers so we will set the random number seed before using it.\n\n\n\n\n\n\nWhat’s a random number seed?\n\n\n\nWe are using random numbers (actually pseudo-random numbers). We want to get the same “random” values every time we run the same code for reproducibility. To do that, we use the set.seed() function and give it an integer value. The value itself doesn’t matter.\nThe random number stream is like a river. If you want to see the same things in your journey down the river, you must get in at the same exact spot. The seed is like the location where you start a journey (that is always the same).\n\n\nThe code is below.\n\nThe prop argument shows the fraction of the original data that should go into the training set (60%) and the validation set (20%). The remaining 20% are put in the test set.\nThe strata argument specifies that the splitting should consider the outcome column (time_to_delivery). This will be discussed in a future section. In short, the three-way splitting is done in different regions of the outcome data in a way that makes the distribution of the outcome as similar as possible across the three partitions.\n\nWe used a value of 991 to set the seed2:\n\nset.seed(991)\ndelivery_split &lt;-\n  initial_validation_split(deliveries, prop = c(0.6, 0.2), strata = time_to_delivery)\n\n# What is in it? \ndelivery_split\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;6004/2004/2004/10012&gt;\n\nThis object records which rows of the original data go into the training, validation, or test sets. The printed output shows the totals for each as &lt;train/val/test/total&gt;.\nTo get the data frames with the correct rows, use these three eponymous functions:\n\ndelivery_train &lt;- training(delivery_split)\ndelivery_test  &lt;- testing(delivery_split)\ndelivery_val   &lt;- validation(delivery_split)\n\nWe will mostly work with the training set of 6,004 deliveries. We’ll use that to explore the data, fit models, and so on.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-eda-whole-game",
    "href": "chapters/whole-game.html#sec-eda-whole-game",
    "title": "2  The Whole Game",
    "section": "\n2.4 Exploratory Data Analysis",
    "text": "2.4 Exploratory Data Analysis\nWe mostly used ggplot2 and patchwork to create these graphics:\n\n# Make specific colors for each day\nday_cols &lt;-  c(\"#000000FF\", \"#24FF24FF\", \"#009292FF\",  \"#B66DFFFF\", \n               \"#6DB6FFFF\", \"#920000FF\", \"#FFB6DBFF\")\n\ndelivery_dist &lt;- \n  delivery_train %&gt;% \n  ggplot(aes(x = distance, time_to_delivery)) +\n  geom_point(alpha = 1 / 10, cex = 1) +\n  labs(y = \"Time Until Delivery (min)\", x = \"Distance (miles)\", title = \"(a)\") +\n  # This function creates the smooth trend line. The `se` option shuts off the\n  # confidence band around the line; too much information to put into one plot. \n  geom_smooth(se = FALSE, col = \"red\")\n\ndelivery_day &lt;- \n  delivery_train %&gt;% \n  ggplot(aes(x = day, time_to_delivery, col = day)) +\n  geom_boxplot(show.legend = FALSE)  +\n  labs(y = \"Time Until Delivery (min)\", x = NULL, title = \"(c)\") +\n  scale_color_manual(values = day_cols)\n\ndelivery_time &lt;- \n  delivery_train %&gt;% \n  ggplot(aes(x = hour, time_to_delivery)) +\n  labs(y = \"Time Until Delivery (min)\", x = \"Order Time (decimal hours)\", title = \"(b)\") +\n  geom_point(alpha = 1 / 10, cex = 1) + \n  geom_smooth(se = FALSE, col = \"red\")\n\ndelivery_time_day &lt;- \n  delivery_train %&gt;% \n  ggplot(aes(x = hour, time_to_delivery, col = day)) +\n  labs(y = \"Time Until Delivery (min)\", x = \"Order Time (decimal hours)\", title = \"(d)\") +\n  # With `col = day`, the trends will be estimated separately for each value of 'day'.\n  geom_smooth(se = FALSE) + \n  scale_color_manual(values = day_cols)\n\npatchwork puts it together.\n\n# Row 1\n( delivery_dist + delivery_time ) / \n  # Row 2\n  ( delivery_day + delivery_time_day ) +\n  # Consolidate the legends\n  plot_layout(guides = 'collect')  & \n  # Place the legend at the bottom\n  theme(legend.title = element_blank(), legend.position = \"bottom\")\n\n\n\n\n\n\n\nggplot2 is a bit noisy. The messages tell you details about how it made the smooth trend line. The code s(x, bs = \"cs\") defines a spline smoother that we will see more of shortly (using a different function).\nThe methods that we used to compute the effects of the item_* columns are more complicated. We must make probabilistic assumptions about the data if we want to get something like a confidence interval. Alternatively, we could specify the empirical distribution function via the bootstrap resampling method. This helps us estimate the standard error of some statistic and use that to compute an interval.\nFirst, we make a function that takes some data and computes our statistics of interest. It assumes x is the entire data set with the delivery time column and each item column.\n\ntime_ratios &lt;- function(x) {\n  x %&gt;%\n    # The items are in columns; we'll stack these columns on one another.\n    pivot_longer(\n      cols = c(starts_with(\"item\")),\n      names_to = \"predictor\",\n      values_to = \"count\"\n    ) %&gt;%\n    # Collapse the counts into a \"in order\"/\"not in order\" variable. \n    mutate(ordered = ifelse(count &gt; 0, \"yes\", \"no\")) %&gt;%\n    # Compute, for each value of the 'predictor' and 'ordered' columns, \n    # the mean delivery time. \n    summarize(mean = mean(time_to_delivery),\n              .by = c(predictor, ordered)) %&gt;%\n    # Move the means to columns for when they were in the order \n    # and when they were not. The new column names are `yes` and `no`.\n    pivot_wider(id_cols = predictor,\n                names_from = ordered,\n                values_from = mean) %&gt;%\n    # Compute the ratio. This is a fold-difference in delivery times.\n    mutate(ratio = yes / no) %&gt;%\n    select(term = predictor, estimate = ratio)\n}\n\nWhen run in the training set:\n\ntime_ratios(delivery_train)\n#&gt; # A tibble: 27 × 2\n#&gt;   term    estimate\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 item_01    1.074\n#&gt; 2 item_02    1.010\n#&gt; 3 item_03    1.010\n#&gt; 4 item_04    1.002\n#&gt; 5 item_05    1.005\n#&gt; 6 item_06    1.018\n#&gt; # ℹ 21 more rows\n\nA value of 1.07 means that there is a 7% increase in the delivery time when that item is in the order at least once.\nA tidymodels function called int_pctl() can take a collection of bootstrap samples of a data set, compute their statistics, and use the results to produce confidence intervals (we’ll use 90% intervals). To use it, we’ll resample the training set using the bootstraps() function and then use a mutate() to compute the fold differences.\nWe are using random numbers again, so let’s reset the seed3.\n\nset.seed(624)\nresampled_data &lt;- \n  delivery_train %&gt;% \n  select(time_to_delivery, starts_with(\"item\")) %&gt;% \n  # This takes a while to compute. The materials use 5000 bootstraps\n  # but a smaller number is used here for demonstration.\n  bootstraps(times = 1001) \n\nresampled_data\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 1,001 × 2\n#&gt;   splits              id           \n#&gt;   &lt;list&gt;              &lt;chr&gt;        \n#&gt; 1 &lt;split [6004/2227]&gt; Bootstrap0001\n#&gt; 2 &lt;split [6004/2197]&gt; Bootstrap0002\n#&gt; 3 &lt;split [6004/2156]&gt; Bootstrap0003\n#&gt; 4 &lt;split [6004/2210]&gt; Bootstrap0004\n#&gt; 5 &lt;split [6004/2208]&gt; Bootstrap0005\n#&gt; 6 &lt;split [6004/2227]&gt; Bootstrap0006\n#&gt; # ℹ 995 more rows\n\nThe splits column contains the information on each bootstrap sample. To get a specific bootstrap sample, we can use the analysis(split_object) function on each element of the splits column. purrr::map() takes each split, extracts the bootstrap sample, then computes all of the ratios4.\n\nresampled_ratios &lt;- \n  resampled_data %&gt;% \n  mutate(stats = map(splits, ~ time_ratios(analysis(.x))))\n\nresampled_ratios\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 1,001 × 3\n#&gt;   splits              id            stats            \n#&gt;   &lt;list&gt;              &lt;chr&gt;         &lt;list&gt;           \n#&gt; 1 &lt;split [6004/2227]&gt; Bootstrap0001 &lt;tibble [27 × 2]&gt;\n#&gt; 2 &lt;split [6004/2197]&gt; Bootstrap0002 &lt;tibble [27 × 2]&gt;\n#&gt; 3 &lt;split [6004/2156]&gt; Bootstrap0003 &lt;tibble [27 × 2]&gt;\n#&gt; 4 &lt;split [6004/2210]&gt; Bootstrap0004 &lt;tibble [27 × 2]&gt;\n#&gt; 5 &lt;split [6004/2208]&gt; Bootstrap0005 &lt;tibble [27 × 2]&gt;\n#&gt; 6 &lt;split [6004/2227]&gt; Bootstrap0006 &lt;tibble [27 × 2]&gt;\n#&gt; # ℹ 995 more rows\n\n# An example: \nresampled_ratios$stats[[1]]\n#&gt; # A tibble: 27 × 2\n#&gt;   term    estimate\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 item_01    1.070\n#&gt; 2 item_02    1.018\n#&gt; 3 item_03    1.008\n#&gt; 4 item_04    1.008\n#&gt; 5 item_05    1.017\n#&gt; 6 item_06    1.007\n#&gt; # ℹ 21 more rows\n\nrsample::int_pctl() can consume these results and produce an interval for each item column5.\n\nresampled_intervals &lt;- \n  resampled_ratios %&gt;% \n  int_pctl(stats, alpha = 0.1) \n\nresampled_intervals\n#&gt; # A tibble: 27 × 6\n#&gt;   term    .lower .estimate .upper .alpha .method   \n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n#&gt; 1 item_01 1.052      1.075  1.097    0.1 percentile\n#&gt; 2 item_02 0.9950     1.009  1.024    0.1 percentile\n#&gt; 3 item_03 0.9940     1.009  1.024    0.1 percentile\n#&gt; 4 item_04 0.9879     1.002  1.016    0.1 percentile\n#&gt; 5 item_05 0.9884     1.005  1.021    0.1 percentile\n#&gt; 6 item_06 1.002      1.018  1.033    0.1 percentile\n#&gt; # ℹ 21 more rows\n\nHere’s our plot:\n\nresampled_intervals %&gt;% \n  # Convert the folds to percentages and make the item values\n  # a little cleaner:\n  mutate(\n    term = gsub(\"_0\", \" \", term),\n    term = factor(gsub(\"_\", \" \", term)),\n    term = reorder(term, .estimate),\n    increase = .estimate - 1,\n  ) %&gt;% \n  ggplot(aes(increase, term)) + \n  geom_vline(xintercept = 0, col = \"red\", alpha = 1 / 3) +\n  geom_point() + \n  geom_errorbar(aes(xmin = .lower - 1, xmax = .upper - 1), width = 1 / 2) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(y = NULL, x = \"Increase in Delivery Time When Ordered\") +\n  theme(axis.text.y = element_text(hjust = 0))",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-model-development-whole-game",
    "href": "chapters/whole-game.html#sec-model-development-whole-game",
    "title": "2  The Whole Game",
    "section": "\n2.5 Model Development",
    "text": "2.5 Model Development\nThe analyses in this section define a model pipeline, fit it to the training set, and then measure performance using the validation set. We’ll review the three evaluated models and describe how those computations were done.\nBefore we get started, we need to specify how to measure model effectiveness. The materials use the mean absolute error (MAE). To specify this performance metric, you can use the yardstick::metric_set() function and give it the function names for specific metrics (like the yardstick::mae() function):\n\nreg_metrics &lt;- metric_set(mae)\n\nWe’ll show you how to use reg_metrics in a bit.\nLinear Regression\nThe linear regression model is fairly simple to define and fit. Before we get to that, we must introduce a major tidymodels component: the recipe.\nA recipe is a set of instructions defining a potential series of computations on the predictor variables to put them into a format the model (or data set) requires. For example, the day-of-the-week factor must be converted into a numeric format. We’ll use the standard “Dummy variable” approach to do that. Additionally, our exploratory data analysis discovered that:\n\nThere is a nonlinear relationship between the outcome and the time of the order.\nThis nonlinear relationship is different for different days. This is an interaction effect between a qualitative predictor (day) and a nonlinear function of another (hour).\nThere also appeared to be an additional nonlinear effect for the order distance.\n\nWe can initialize a recipe using a simple formula method:\n\nspline_rec &lt;- recipe(time_to_delivery ~ ., data = delivery_train)\n\nThere are a few things that this function call does:\n\nThe formula declares that the column time_to_delivery is the outcome (since it is on the left-hand side of the tilde). The dot on the right-hand side indicates that all of the columns in delivery_train, besides the outcome, should be treated as predictors.\nThe recipe collects information on each column’s type. For example, it understands that day is a factor and that the item_* columns are numeric.\n\nLet’s add to the recipe by converting day to indicator columns. We do this by adding a step to the recipe via:\n\nspline_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) \n\nThe first argument to step functions is the variables that should be affected by the function. We can use any dplyr selector such as everything() and/or the bare column names. Here, we want to change every factor column that was the role of “predictor”. For this purpose, recipes have an extended set of selector functions.\nOnce the recipe is processed, this step will record which columns were captured by all_factor_predictors(), retain their factor levels, then convert them to a set of 0/1 indicators for each predictor/level.\nUnlike base R’s formula method, the resulting columns are named rationally. By default, it uses the pattern {column name}_{level} for the new features. So, the column day will not exist after this step. It is replaced with columns such as day_Thursday and so on.\nThe next recipe step is probably unnecessary for this data set but automatically using it is not problematic. What happens if there is a factor level that occurs very infrequently? It is possible that this will only be observed in the validation or test set. step_dummy() will make a column for that factor level since it knows it exists but the training set will have all zeros for this column; it has zero variance. We can screen these out using step_zv() (‘zv’ = zero-variance):\n\nspline_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  step_zv(all_predictors()) \n\nNow, we can address the nonlinear effects. We’ll use a spline basis expansion (described later on the main page) that creates additional columns from some numeric predictor. We’ll use a natural spline function and create ten new columns for both hour and distance:\n\nspline_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(hour, distance, deg_free = 10)\n\nThe naming convention for these new features are hour_01 … hour_10 and so on. The original hour column is removed (same for the distance column).\nThis step allows the linear regression to have nonlinear relationships between predictors and the outcome.\nFinally, we can create interactions. In base R, an interaction between variables a and b is specified in the formula using a:b. We’ll use the same method here with step_interact(). The main difference is that the columns day and hour no longer exist at this point. To capture all of the interactions, we can use the : convention with selector functions. Using starts_wth(\"day_\") will capture the existing indicator columns and, similarly, starts_wth(\"hour_\") finds the appropriate spline terms. Our final recipe is then:\n\nspline_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(hour, distance, deg_free = 10) %&gt;% \n  step_interact(~ starts_with(\"hour_\"):starts_with(\"day_\"))\n\n\n\n\n\n\n\nLearn More About Recipes\n\n\n\nYou can learn more about recipes later and there is material in the tidymodels book as well as tidymodels.org.\n\nFeature Engineering with recipes\nDimensionality Reduction\nEncoding Categorical Data\nGet Started: Preprocess your data with recipes\nArticles with recipes\nA list of recipe steps on CRAN\n\n\n\nTo specify the linear regression model, we use one of the functions from the parsnip package called… linear_reg(). Since we are using ordinary least squares, this function defaults to stats::lm().\n\n# This creates a model specification: \nlm_spec &lt;- linear_reg()\nlm_spec\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\nThe engine mentioned here is the computational method to fit the model. R has many ways to do this and \"lm\" is the default engine.\nHow do we combine the recipe and the model specifications? The best approach is to make a pipeline-like object called a workflow:\n\nlin_reg_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_spec) %&gt;% \n  add_recipe(spline_rec)\n\nWe can use the fit() function to fit the workflow to the training set. This executes the recipe on the data set then passes the appropriate data to stats::lm():\n\nlin_reg_fit &lt;- fit(lin_reg_wflow, data = delivery_train)\n\nWe can print the results out but the results are kind of long:\n\nlin_reg_fit\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────────────\n#&gt; 4 Recipe Steps\n#&gt; \n#&gt; • step_dummy()\n#&gt; • step_zv()\n#&gt; • step_spline_natural()\n#&gt; • step_interact()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)            item_01            item_02            item_03  \n#&gt;           13.3434             1.2444             0.6461             0.7313  \n#&gt;           item_04            item_05            item_06            item_07  \n#&gt;            0.2820             0.5839             0.5250             0.5063  \n#&gt;           item_08            item_09            item_10            item_11  \n#&gt;            0.6376             0.7368             1.5341             0.5172  \n#&gt;           item_12            item_13            item_14            item_15  \n#&gt;            0.5945             0.5799             0.5080             0.6432  \n#&gt;           item_16            item_17            item_18            item_19  \n#&gt;            0.4309             0.6089             0.4028            -0.3568  \n#&gt;           item_20            item_21            item_22            item_23  \n#&gt;            0.5280             0.7743             0.5105             0.6876  \n#&gt;           item_24            item_25            item_26            item_27  \n#&gt;            0.8126             0.4768             0.5274             0.5773  \n#&gt;           day_Tue            day_Wed            day_Thu            day_Fri  \n#&gt;           -1.1200            -2.8421            -3.7059            -4.3130  \n#&gt;           day_Sat            day_Sun            hour_01            hour_02  \n#&gt;           -4.6659            -3.0919             1.8776             2.5588  \n#&gt;           hour_03            hour_04            hour_05            hour_06  \n#&gt;            2.5826             3.8464             2.5826             4.0220  \n#&gt;           hour_07            hour_08            hour_09            hour_10  \n#&gt;            4.0112             5.9056            -0.8782            11.1905  \n#&gt;       distance_01        distance_02        distance_03        distance_04  \n#&gt;           -0.0523             0.5777             0.6157             0.7524  \n#&gt;       distance_05        distance_06        distance_07        distance_08  \n#&gt;            1.6695             1.7837             2.9039             3.3200  \n#&gt;       distance_09        distance_10  hour_01_x_day_Tue  hour_01_x_day_Wed  \n#&gt;          -19.3835            75.2868             1.3304             1.2875  \n#&gt; hour_01_x_day_Thu  hour_01_x_day_Fri  hour_01_x_day_Sat  hour_01_x_day_Sun  \n#&gt;            2.4448             1.3560             2.4253             1.5058  \n#&gt; hour_02_x_day_Tue  hour_02_x_day_Wed  hour_02_x_day_Thu  hour_02_x_day_Fri  \n#&gt;            0.2268             3.1441             3.9958             4.8608  \n#&gt; hour_02_x_day_Sat  hour_02_x_day_Sun  hour_03_x_day_Tue  hour_03_x_day_Wed  \n#&gt;            4.8380             3.7002             2.9359             6.4783  \n#&gt; hour_03_x_day_Thu  hour_03_x_day_Fri  hour_03_x_day_Sat  hour_03_x_day_Sun  \n#&gt;            7.4590             9.9423            10.9673             5.7549  \n#&gt; hour_04_x_day_Tue  hour_04_x_day_Wed  hour_04_x_day_Thu  hour_04_x_day_Fri  \n#&gt;            1.6701             6.2609             9.7183            12.1586  \n#&gt; hour_04_x_day_Sat  hour_04_x_day_Sun  hour_05_x_day_Tue  hour_05_x_day_Wed  \n#&gt;           13.9715             7.9502             3.9981             9.4129  \n#&gt; hour_05_x_day_Thu  hour_05_x_day_Fri  hour_05_x_day_Sat  hour_05_x_day_Sun  \n#&gt;           12.1748            16.5402            17.5038             9.3518  \n#&gt; hour_06_x_day_Tue  hour_06_x_day_Wed  hour_06_x_day_Thu  hour_06_x_day_Fri  \n#&gt;            2.5202             8.2079            11.9678            15.7150  \n#&gt; hour_06_x_day_Sat  hour_06_x_day_Sun  hour_07_x_day_Tue  hour_07_x_day_Wed  \n#&gt; \n#&gt; ...\n#&gt; and 14 more lines.\n\nOne helpful function is tidy(). It is designed to return the object results rationally, helpfully. In our case, the tidy() method for an lm object gives us a nice data frame back with information on the fitted coefficients:\n\ntidy(lin_reg_fit)\n#&gt; # A tibble: 114 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)  13.34     1.585       8.420 4.664e-17\n#&gt; 2 item_01       1.244    0.1031     12.08  3.498e-33\n#&gt; 3 item_02       0.6461   0.06865     9.412 6.852e-21\n#&gt; 4 item_03       0.7313   0.06914    10.58  6.463e-26\n#&gt; 5 item_04       0.2820   0.06260     4.505 6.776e- 6\n#&gt; 6 item_05       0.5839   0.07871     7.418 1.360e-13\n#&gt; # ℹ 108 more rows\n\nUnlike the summary() method for lm objects, this object can immediately be used in plots or tables.\nAnother valuable supporting function is augment(). It can take a model object and data set and attach the prediction columns to the data frame. Essentially, this is an upgraded version of predict(). Let’s predict the validation set:\n\nlm_reg_val_pred &lt;- augment(lin_reg_fit, new_data = delivery_val)\nnames(lm_reg_val_pred)\n#&gt;  [1] \".pred\"            \".resid\"           \"time_to_delivery\" \"hour\"            \n#&gt;  [5] \"day\"              \"distance\"         \"item_01\"          \"item_02\"         \n#&gt;  [9] \"item_03\"          \"item_04\"          \"item_05\"          \"item_06\"         \n#&gt; [13] \"item_07\"          \"item_08\"          \"item_09\"          \"item_10\"         \n#&gt; [17] \"item_11\"          \"item_12\"          \"item_13\"          \"item_14\"         \n#&gt; [21] \"item_15\"          \"item_16\"          \"item_17\"          \"item_18\"         \n#&gt; [25] \"item_19\"          \"item_20\"          \"item_21\"          \"item_22\"         \n#&gt; [29] \"item_23\"          \"item_24\"          \"item_25\"          \"item_26\"         \n#&gt; [33] \"item_27\"\n\nWhat is our MAE? This is where we use our metric set reg_metrics. Note that there is a column in the results called .pred. For regression models, this is the predicted delivery time for each order in the validation set. We can use that and the original observed outcome column to estimate the MAE6:\n\nlm_reg_val_pred %&gt;% \n  reg_metrics(truth = time_to_delivery, estimate = .pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       1.609\n\nThe units are fractional minutes.\nAt this point, we can make diagnostic plots of our data and so on.\nLet’s take a minor distraction that will pay off a bit later. The main page mentions that we can treat the validation set as a single resample of the data. If we were to do that, our code wouldn’t have to change much when we get into more complex scenarios such as cross-validation or model tuning. To do this, we can convert the initial split object into a resampling set (a.k.a. an rset):\n\ndelivery_rs &lt;- validation_set(delivery_split)\n\nclass(delivery_rs)\n#&gt; [1] \"validation_set\" \"rset\"           \"tbl_df\"         \"tbl\"           \n#&gt; [5] \"data.frame\"\n\ndelivery_rs\n#&gt; # A tibble: 1 × 2\n#&gt;   splits              id        \n#&gt;   &lt;list&gt;              &lt;chr&gt;     \n#&gt; 1 &lt;split [6004/2004]&gt; validation\n\nThis packages the training and validation sets together in a way that it knows when to use each data set appropriately.\nSince we are treating this as if it were resampling, we can use the fit_resamples() function to do much of the manual work we just showed. We’ll add a control object to the mix to specify that we want to retain the validation set predictions (and our original workflow).\n\nctrl_rs &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\nlin_reg_res &lt;-\n  fit_resamples(lin_reg_wflow,\n                resamples = delivery_rs,\n                control = ctrl_rs,\n                metrics = reg_metrics)\n\nThe benefit here is that there are a lot of helper functions to simplify your code. For example, to get the validation set MAE and predictions:\n\ncollect_metrics(lin_reg_res)\n#&gt; # A tibble: 1 × 6\n#&gt;   .metric .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   1.609     1      NA Preprocessor1_Model1\n\ncollect_predictions(lin_reg_res)\n#&gt; # A tibble: 2,004 × 5\n#&gt;   .pred id          .row time_to_delivery .config             \n#&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 30.12 validation  6005            27.23 Preprocessor1_Model1\n#&gt; 2 23.02 validation  6006            22.10 Preprocessor1_Model1\n#&gt; 3 28.38 validation  6007            26.63 Preprocessor1_Model1\n#&gt; 4 31.04 validation  6008            30.84 Preprocessor1_Model1\n#&gt; 5 38.57 validation  6009            41.17 Preprocessor1_Model1\n#&gt; 6 27.04 validation  6010            27.04 Preprocessor1_Model1\n#&gt; # ℹ 1,998 more rows\n\nThe probably package also has a nice helper to check the calibration of the model via a plot:\n\ncal_plot_regression(lin_reg_res)\n\n\n\n\n\n\n\n\nRule-Based Ensemble\nTo fit the Cubist model, we need to load one of the tidymodels extension packages called rules. It has the tools to fit this model and will automatically (and silently) use the Cubist package when the model fit occurs.\nWe’ll create a model specification that has an enselmble size of 100:\n\nlibrary(rules)\n\n# A model specification: \ncb_spec &lt;- cubist_rules(committees = 100)\n\nOne advantage of rule-based models is that very little preprocessing is required (i.e., no dummy variables or spline terms). For that reason, we’ll use a simple R formula instead of a recipe:\n\ncb_wflow &lt;- \n  workflow() %&gt;% \n  add_model(cb_spec) %&gt;% \n  add_formula(time_to_delivery ~ .)\n\nLet’s go straight to fit_resamples():\n\ncb_res &lt;-\n  fit_resamples(\n    cb_wflow, \n    resamples = delivery_rs, \n    control = ctrl_rs, \n    metrics = reg_metrics\n  )\n\ncollect_metrics(cb_res)\n#&gt; # A tibble: 1 × 6\n#&gt;   .metric .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   1.416     1      NA Preprocessor1_Model1\n\nThe calibration plot:\n\ncal_plot_regression(cb_res)\n\n\n\n\n\n\n\nThis is pretty simple and demonstrates that, after an initial investment in learning tidymodels syntax, the process of fitting different models does not require huge changes to your scripts.\nTo get the model fit, we previously used fit(). With resampling objects (and the tuning objects that we are about to see), there is another helper function called fit_best() that will create the model from the entire training set using the resampling results7:\n\ncb_fit &lt;- fit_best(cb_res)\n\ncb_fit\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: cubist_rules()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────────────\n#&gt; time_to_delivery ~ .\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; cubist.default(x = x, y = y, committees = 100)\n#&gt; \n#&gt; Number of samples: 6004 \n#&gt; Number of predictors: 30 \n#&gt; \n#&gt; Number of committees: 100 \n#&gt; Number of rules per committee: 31, 22, 23, 26, 21, 24, 23, 23, 22, 23, 21, 22, 18, 24, 20, 22, 16, 26, 25, 26 ...\n\nThe tidy() method is also helpful here. It contains all of the rules and corresponding regression models. Let’s get these values for the second rule in the fourth ensemble:\n\nrule_details &lt;- tidy(cb_fit)\n\nrule_details %&gt;% \n  filter(committee == 4 & rule_num == 2) %&gt;% \n  pluck(\"rule\")\n#&gt; [1] \"( hour &lt;= 14.946 ) & ( day  %in% c( 'Mon','Tue','Wed','Thu','Sun' ) ) & ( distance &lt;= 4.52 )\"\n\nrule_details %&gt;% \n  filter(committee == 4 & rule_num == 2) %&gt;% \n  select(estimate) %&gt;% \n  pluck(1) %&gt;% \n  pluck(1)\n#&gt; # A tibble: 15 × 2\n#&gt;   term        estimate\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 (Intercept)   -11.24\n#&gt; 2 hour            2.05\n#&gt; 3 distance        0.78\n#&gt; 4 item_01        -0.5 \n#&gt; 5 item_02         0.3 \n#&gt; 6 item_05         0.8 \n#&gt; # ℹ 9 more rows\n\n\nNeural Network\nThe model function for this type of model is parsnip::mlp() (MLP is short for “multi-layer perceptron”). There are quite a few packages for neural networks in R. tidymodels has interfaces to several engines:\n\nshow_engines(\"mlp\")\n#&gt; # A tibble: 8 × 2\n#&gt;   engine mode          \n#&gt;   &lt;chr&gt;  &lt;chr&gt;         \n#&gt; 1 keras  classification\n#&gt; 2 keras  regression    \n#&gt; 3 nnet   classification\n#&gt; 4 nnet   regression    \n#&gt; 5 brulee classification\n#&gt; 6 brulee regression    \n#&gt; # ℹ 2 more rows\n\nWe’ll use the brulee package. This uses the torch software to fit the model. We’ll only tune the number of hidden units (for now, see later chapters). To mark any parameter for tuning, we pass the tune() function to an argument:\n\nnnet_spec &lt;- \n  mlp(\n    hidden_units = tune(),\n    # Some specific argument values that we chose:\n    penalty = 0.01,\n    learn_rate = 0.1,\n    epochs = 5000\n  ) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"brulee\", stop_iter = 10, rate_schedule = \"cyclic\")\n\nA few notes on this:\n\nThe arguments to mlp() are called the main arguments since they are used by several engines.\nThe default engine uses the nnet package; set_engine() specifies that we want to use the brulee package instead.\nTwo arguments (stop_iter and rate_schedule) are specific to our engine. We set them here (and can also pass a tune() value to them).\nNeural networks can fit both classification and regression models. We must state what model type (called a “mode”) to create.\n\nThis model requires the conversion to dummy variables but does not require features to handle nonlinear trends and interactions. One additional preprocessing step that is required is to put the predictors in the same units (e.g., not miles or hours). There are a few ways to do this. We will center and scale the predictors using recipes::step_normalize():\n\nnorm_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\nnnet_wflow &lt;- \n  workflow() %&gt;% \n  add_model(nnet_spec) %&gt;% \n  add_recipe(norm_rec)\n\nUnlike the previous models, we are tuning one of the hyper-parameters. Instead of fit_resamples(), we’ll use tune_grid() to search over a predefined set of values for the number of hidden units. We can set how many values we should try or directly declare the candidate values. We’ll do the latter and, for simplicity, use a smaller range of values.\nFinally, we’ll use another control function:\n\n# The main materials used 2:100\nnnet_grid &lt;- tibble(hidden_units = 2:10)\n\nctrl_grid &lt;- control_grid(save_pred = TRUE, save_workflow = TRUE)\n\n# The model initializes the parameters with random numbers so set the seed:\nset.seed(388)\nnnet_res &lt;-\n  tune_grid(nnet_wflow,\n            resamples = delivery_rs,\n            control = ctrl_grid,\n            grid = nnet_grid,\n            metrics = reg_metrics)\n\nThere are some additional helper functions for model tuning. For example, we can rank the models based on a metric:\n\nshow_best(nnet_res, metric = \"mae\")\n#&gt; # A tibble: 5 × 7\n#&gt;   hidden_units .metric .estimator  mean     n std_err .config             \n#&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1            9 mae     standard   1.499     1      NA Preprocessor1_Model8\n#&gt; 2            6 mae     standard   1.522     1      NA Preprocessor1_Model5\n#&gt; 3           10 mae     standard   1.522     1      NA Preprocessor1_Model9\n#&gt; 4            7 mae     standard   1.525     1      NA Preprocessor1_Model6\n#&gt; 5            5 mae     standard   1.550     1      NA Preprocessor1_Model4\n\nWe can also return the parameter with the numerically best results8:\n\nbest_hidden_units &lt;- select_best(nnet_res, metric = \"mae\")\nbest_hidden_units\n#&gt; # A tibble: 1 × 2\n#&gt;   hidden_units .config             \n#&gt;          &lt;int&gt; &lt;chr&gt;               \n#&gt; 1            9 Preprocessor1_Model8\n\nThe autoplot() method can visualize the relationship between the tuning parameter(s) and the performance metric(s).\n\nautoplot(nnet_res, metric = \"mae\")\n\n\n\n\n\n\n\ntune::collect_predictions() will automatically return the out-of-sample predictions for every candidate model (e.g., every tuning parameter value.). We might not want them all; it has an argument called parameters that can be used to filter the results.\nprobably::cal_plot_regression() automatically shows the results for each tuning parameter combination. For example:\n\ncal_plot_regression(nnet_res)\n\n\n\n\n\n\n\nThere are two options if we need a model fit on the training set. If the numerically best parameter is best (i.e., smallest MAE), then tune::fit_best() is the easiest approach. Alternatively, you can choose the exact tuning parameter values you desire and splice them into the model to replace the current values of tune(). To do this, there is a finalize_workflow() function. It takes a data frame with one row and columns for each tuning parameter. Here’s an example where we decide that 10 hidden units are best:\n\nset.seed(814)\nnnet_fit &lt;- \n  nnet_wflow %&gt;% \n  finalize_workflow(tibble(hidden_units = 10)) %&gt;% \n  fit(data = delivery_train)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-parallel-processing",
    "href": "chapters/whole-game.html#sec-parallel-processing",
    "title": "2  The Whole Game",
    "section": "\n2.6 Aside: Parallel Processing",
    "text": "2.6 Aside: Parallel Processing\nFor model tuning, we are fitting many models. With grid search, these models are not dependent on one another. For this reason, it is possible to compute these model fits simultaneously (i.e., in parallel).\nTo do so, tidymodels requires you to specify a parallel backend. There are several types, and we will use the mirai system since it works on all operating systems. For this technology, we can run the following commands before running fit_resamples() or any of the tune_*() functions:\n\ncores &lt;- parallel::detectCores(logical = FALSE)\nlibrary(future.mirai)\nplan(mirai_multisession, workers = cores)\n\nThere can be significant speed-ups when running in parallel. See the section in the tidymodels book for more details.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#calibration",
    "href": "chapters/whole-game.html#calibration",
    "title": "2  The Whole Game",
    "section": "\n2.7 Calibration",
    "text": "2.7 Calibration\nThe functions to calibrate predictions are in the probably package and have names that start with cal_*. There are methods that work on the results from fit_resamples() or the tune_*() functions, but you can also just use a data frame of predicted values.\nWe must estimate the trend with the validation set. If we use our object lin_reg_res, it knows what data to use:\n\nlin_reg_cal &lt;- cal_estimate_linear(lin_reg_res)\n#&gt; Registered S3 method overwritten by 'butcher':\n#&gt;   method                 from    \n#&gt;   as.character.dev_topic generics\nlin_reg_cal\n#&gt; \n#&gt; ── Regression Calibration\n#&gt; Method: Generalized additive model calibration\n#&gt; Source class: Tune Results\n#&gt; Data points: 2,004\n#&gt; Truth variable: `time_to_delivery`\n#&gt; Estimate variable: `.pred`\n\nAs you’ll see in a minute, the function probably::cal_apply() calibrates new predictions.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-test-results-whole-game",
    "href": "chapters/whole-game.html#sec-test-results-whole-game",
    "title": "2  The Whole Game",
    "section": "\n2.8 Test Set Results",
    "text": "2.8 Test Set Results\nAs with best_fit(), there are two ways to predict the test set.\nThe more manual approach is to fit the model on the training set, use predict() or augment() to compute the test set predictions, calibrate them with our object, then use our metric to compute performance. If we had not already fit the model, the pre-calibration code is:\n\nlin_reg_fit &lt;- fit(lin_reg_wflow, delivery_train)\nlin_reg_test_pred &lt;- augment(lin_reg_fit, delivery_test)\n\nlin_reg_test_pred %&gt;% \n  reg_metrics(time_to_delivery, .pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       1.607\n\n# plot the uncalibrated results: \nlin_reg_test_pred %&gt;% \n  cal_plot_regression(truth = time_to_delivery, estimate = .pred)\n\n\n\n\n\n\n\nThere is a shortcut for the first three commands. tune::last_fit() takes our initial split object and automatically does the rest (but not calibration yet):\n\nlin_reg_test_res &lt;- \n  lin_reg_wflow %&gt;% \n  last_fit(delivery_split, metrics = reg_metrics)\n\nWe can pull out the elements we need from this object using some extract_*() and collect_*() functions. Here are a few:\n\n# Test set metrics:\ncollect_metrics(lin_reg_test_res)\n#&gt; # A tibble: 1 × 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard       1.607 Preprocessor1_Model1\n\n# Test set predictions: \ncollect_predictions(lin_reg_test_res)\n#&gt; # A tibble: 2,004 × 5\n#&gt;   .pred id                .row time_to_delivery .config             \n#&gt;   &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 15.98 train/test split     7            18.02 Preprocessor1_Model1\n#&gt; 2 16.03 train/test split    14            17.57 Preprocessor1_Model1\n#&gt; 3 27.60 train/test split    16            26.71 Preprocessor1_Model1\n#&gt; 4 17.15 train/test split    29            17.64 Preprocessor1_Model1\n#&gt; 5 32.24 train/test split    33            32.19 Preprocessor1_Model1\n#&gt; 6 20.18 train/test split    34            20.31 Preprocessor1_Model1\n#&gt; # ℹ 1,998 more rows\n\n# Final model fit: \nlin_reg_fit &lt;- extract_fit_parsnip(lin_reg_test_res)\n\n# cal_plot_regression(lin_reg_test_res)\n\nNow let’s calibrate and compute performance:\n\n# apply calibration\nlin_reg_test_pred_cal &lt;- \n  lin_reg_test_pred %&gt;% \n  cal_apply(lin_reg_cal)\n\nlin_reg_test_pred_cal %&gt;% \n  reg_metrics(time_to_delivery, .pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       1.545\n\n# plot the calibrated results: \nlin_reg_test_pred_cal %&gt;% \n  cal_plot_regression(truth = time_to_delivery, estimate = .pred)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#conclusion",
    "href": "chapters/whole-game.html#conclusion",
    "title": "2  The Whole Game",
    "section": "\n2.9 Conclusion",
    "text": "2.9 Conclusion\nThis has been an abbreviated, high-level introduction to using tidymodels. Future chapters will go into much more detail on these subjects and illustrate additional features and functions as needed.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#footnotes",
    "href": "chapters/whole-game.html#footnotes",
    "title": "2  The Whole Game",
    "section": "",
    "text": "We use a different ggplot theme for the main materials. We’ll use the default theme here.↩︎\nIf you are wondering how we get the seed values, I use sample.int(1000, 1) to generate random seeds on the fly.↩︎\nWhy are we doing this again? Didn’t we already “jump in the river?” Yes. If we were executing all of the code here in the exact order (with no typos or commands in between), we would have reproducible pseudo-random numbers. That’s usually not how interactive data analysis goes, though. Therefore, we (re)set the seed each time we use randomness.↩︎\nThis map() call can be made much faster by using the furrr package. It has versions of the purrr::map() functions that can run in parallel.↩︎\nYou can see another example of bootstrap intervals at tidymodels.org.↩︎\nWe could have used the yardstick::mae() directly instead of stuffing that function in a metric set. Since we often want to collect more than one type of performance statistic, we’re showing how to use a metric set.↩︎\nThis is possible since we previously used the save_workflow = TRUE option in the control function.↩︎\nAs impressive as the torch ecosystem is, it is not as optimized for reproducibility. These results may vary from run to run due to the inability to fix some of the random numbers used and their use of different numerical tolerances across operating systems.↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html",
    "href": "chapters/initial-data-splitting.html",
    "title": "3  Initial Data Splitting",
    "section": "",
    "text": "3.1 Requirements\nWe’ll illustrate how to conduct an initial split of your data into different partitions (used for different purposes).\nYou’ll need 4 packages (caret, spatialsample, tidymodels, and tidysdm) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"caret\", \"spatialsample\", \"tidymodels\", \"tidysdm\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\ntidymodels_prefer()\nThe data used here are both in R packages that are already installed. Let’s work with the primary data set: the Ames Iowa housing data.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-ames-intro",
    "href": "chapters/initial-data-splitting.html#sec-ames-intro",
    "title": "3  Initial Data Splitting",
    "section": "\n3.2 The Ames Housing Data",
    "text": "3.2 The Ames Housing Data\nThese data are in the modeldata package, which is part of tidymodels. Let’s load the data, subset a few columns, and modify the sale price units. We’ll also combine the two bathroom-related columns into a single column.\n\ndata(ames, package = \"modeldata\")\n\names &lt;-\n  ames %&gt;%\n  select(Sale_Price, Bldg_Type, Neighborhood, Year_Built, Gr_Liv_Area, Full_Bath,\n         Half_Bath, Year_Sold, Lot_Area, Central_Air, Longitude, Latitude) %&gt;%\n  mutate(\n    Sale_Price = log10(Sale_Price),\n    Baths = Full_Bath  + Half_Bath/2\n  ) %&gt;%\n  select(-Half_Bath, -Full_Bath)\n\nglimpse(ames)\n#&gt; Rows: 2,930\n#&gt; Columns: 11\n#&gt; $ Sale_Price   &lt;dbl&gt; 5.332, 5.021, 5.236, 5.387, 5.279, 5.291, 5.329, 5.282, 5.374…\n#&gt; $ Bldg_Type    &lt;fct&gt; OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, TwnhsE, Twnhs…\n#&gt; $ Neighborhood &lt;fct&gt; North_Ames, North_Ames, North_Ames, North_Ames, Gilbert, Gilb…\n#&gt; $ Year_Built   &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 1995, 1999, 1…\n#&gt; $ Gr_Liv_Area  &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616, 1804, 16…\n#&gt; $ Year_Sold    &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2…\n#&gt; $ Lot_Area     &lt;int&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005, 5389, 75…\n#&gt; $ Central_Air  &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y…\n#&gt; $ Longitude    &lt;dbl&gt; -93.62, -93.62, -93.62, -93.62, -93.64, -93.64, -93.63, -93.6…\n#&gt; $ Latitude     &lt;dbl&gt; 42.05, 42.05, 42.05, 42.05, 42.06, 42.06, 42.06, 42.06, 42.06…\n#&gt; $ Baths        &lt;dbl&gt; 1.0, 1.0, 1.5, 2.5, 2.5, 2.5, 2.0, 2.0, 2.0, 2.5, 2.5, 2.0, 2…\n\ntidymodels requires that, for outcome data, any basic transformations should occur before data splitting.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-basic-splitting",
    "href": "chapters/initial-data-splitting.html#sec-basic-splitting",
    "title": "3  Initial Data Splitting",
    "section": "\n3.3 Simple Data Splitting",
    "text": "3.3 Simple Data Splitting\nThere are a few main functions for an initial split:\n\n\nrsample::initial_split(): completely random splits and stratified splits.\n\nrsample::initial_time_split(): non-random splits for times series; the most recent data are used for testing.\n\nrsample::initial_validation_split() and rsample::initial_validation_time_split(): an initial split into three partitions.\n\nrsample::group_initial_split(): for situations with repeated measures or other important grouping factors.\n\nMost of our applications will use the first function, where the default is to use 75% for training and 25% for testing. This is determined at random; there is no need to randomly sort the rows before splitting. By default, a simple random split is used.\nLet’s do that with the Ames data:\n\nset.seed(3024)\names_split &lt;- initial_split(ames)\n\names_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;2197/733/2930&gt;\n\nThe output shows the size of the resulting data sets. To get the two data sets, there are simple accessor functions:\n\names_train &lt;- training(ames_split)\names_test  &lt;- testing(ames_split)\n\nConsistent with the printed output, there are 2,197 data points in the training set and 733 reserved for testing.\nWe won’t touch on initial_time_split() here but only mention that it takes the fraction of the data specified for testing from the bottom/tail of the data frame. Unlike the previous function, the order of the rows matters since the rows at the end of the data frame as used as the test set.\ngroup_initial_split() and initial_validation_split() are discussed in more detail below.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-split-with-outcome",
    "href": "chapters/initial-data-splitting.html#sec-split-with-outcome",
    "title": "3  Initial Data Splitting",
    "section": "\n3.4 Using the Outcome",
    "text": "3.4 Using the Outcome\nFor the Ames data, we know that the distribution of sale prices has some outlying points. To deal with this, we can use a stratified split (on the outcome) using 5 quantiles of the data in ames:\n\nset.seed(3024)\names_block_split &lt;- initial_split(ames, strata = Sale_Price, breaks = 5)\n\names_block_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;2196/734/2930&gt;\n\nNothing else changes; the same functions can be used to access the training and testing data.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-split-with-predictors",
    "href": "chapters/initial-data-splitting.html#sec-split-with-predictors",
    "title": "3  Initial Data Splitting",
    "section": "\n3.5 Using the Predictors",
    "text": "3.5 Using the Predictors\nInstead of using the outcome to partition the data, other columns can be used when applicable. The text mentions using the twinning package (CRAN page). The same authors have a second approach that can be found in the SPlit package (CRAN). Both are straightforward to use.\nMaximum dissimilarity sampling can be conducted using caret::maxDissim(). It starts with an initial set of one or more or fewer data points to use as a starter. Unless there is a specific set of points of interest, picking one close to the center of the multivariate predictor distribution might make sense. Here is some code that uses the geographic coordinates as the splitting variables:\n\n# Since we will be using distances in the calculations, create centered \n# and scaled versions of the coordinates then add a row index column. \names_scaled &lt;-\n  ames %&gt;%\n  select(Longitude, Latitude) %&gt;%\n  mutate(\n    scaled_lon = scale(Longitude)[,1], \n    scaled_lat = scale(Latitude)[,1]\n  ) %&gt;%\n  select(starts_with(\"scaled\")) %&gt;% \n  add_rowindex()\n\n# Select an initial data point closest to the middle\nseed_row &lt;-\n  ames_scaled %&gt;%\n  mutate(\n    dist = (scaled_lon)^2 + (scaled_lat)^2\n  ) %&gt;%\n  slice_min(dist, n = 1) %&gt;%\n  pluck(\".row\")\n\n# Partition the data\names_scaled_seed &lt;- ames_scaled %&gt;% slice( seed_row)\names_scaled_pool &lt;- ames_scaled %&gt;% slice(-seed_row)\n\n# Conduct the selection process\nselection_path &lt;- \n  caret::maxDissim(\n    # Only give the function the predictor columns for each data set\n    ames_scaled_seed %&gt;% select(-.row), \n    ames_scaled_pool %&gt;% select(-.row), \n    n = 24\n  )\n\n# Get the selected row numbers that correspond to the 'ames' data frame.\nselected_rows &lt;- c(seed_row, ames_scaled_pool$.row[selection_path])\n\nselected_data &lt;- ames %&gt;% slice(selected_rows)\n\n# A non-map plot of the values: \nselected_data %&gt;%\n  mutate(xend = lead(Longitude), yend = lead(Latitude)) %&gt;%\n  ggplot(aes(Longitude, Latitude)) +\n  geom_point() +\n  geom_segment(aes(xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.1, \"inches\"), type = \"closed\"),\n               col = \"blue\", alpha = 1 / 5) +\n  theme_bw()",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-three-way-split",
    "href": "chapters/initial-data-splitting.html#sec-three-way-split",
    "title": "3  Initial Data Splitting",
    "section": "\n3.6 Validation Sets",
    "text": "3.6 Validation Sets\nTo add a validation set at the outset, initial_validation_split() works the same as initial_split(). The prop argument requires two values now: the first is the training set proportion, and the second is for the validation set. In this example below, we add 80% to training, 10% to validation, and the remaining 10% to the testing set:\n\nset.seed(4)\names_val_split &lt;- initial_validation_split(ames, strata = Sale_Price, prop = c(0.8, 0.1))\n\names_val_split\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;2342/293/295/2930&gt;\n\nAgain, the acquisition of data is the same but has the additional use of the validation() function:\n\names_train &lt;- training(ames_val_split)\names_val   &lt;- validation(ames_val_split)\names_test  &lt;- testing(ames_val_split)\n\nrsample::initial_validation_time_split() does the same thing but based on the ordering of the data (as opposed to random selection).\nSuppose a data frame had 100 rows. Using prop = c(0.8, 0.1) would place the first 80 rows into training, the next 10 into validation, and the last 10 into testing. Keeping the data appropriately ordered is important when using validation sets in tidymodels.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-multilevel-splitting",
    "href": "chapters/initial-data-splitting.html#sec-multilevel-splitting",
    "title": "3  Initial Data Splitting",
    "section": "\n3.7 Multi-Level Data",
    "text": "3.7 Multi-Level Data\nThis section will focus on data with a rational grouping of data. For example, medical data might follow patient over time so that there are multiple rows per patient. The patient is the independent experimental unit (IEU), meaning that the data between patients are thought to be independent, and those within a patient are (statistically) related. We want to partition the data so that all of the data for each IEU end up in either the training or test sets but not both. We want to sample the data by the group – where the group in this example is the patient.\nThere are other applications of grouped data but the example data that we’ll use fits into the description above: 27 patients were followed and had data collected at four time points. The data are in the nlme package:\n\ndata(Orthodont, package = \"nlme\")\nglimpse(Orthodont)\n#&gt; Rows: 108\n#&gt; Columns: 4\n#&gt; $ distance &lt;dbl&gt; 26.0, 25.0, 29.0, 31.0, 21.5, 22.5, 23.0, 26.5, 23.0, 22.5, 24.0,…\n#&gt; $ age      &lt;dbl&gt; 8, 10, 12, 14, 8, 10, 12, 14, 8, 10, 12, 14, 8, 10, 12, 14, 8, 10…\n#&gt; $ Subject  &lt;ord&gt; M01, M01, M01, M01, M02, M02, M02, M02, M03, M03, M03, M03, M04, …\n#&gt; $ Sex      &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Male, Male,…\n\nTo use rsample::group_initial_split(), we must supply a group argument that corresponds to one of the columns in the data. There is also a prop argument that specifies the proportion of the groups that should go into the training set.\n\nset.seed(93)\north_split &lt;- group_initial_split(Orthodont, group = Subject, prop = 2 / 3)\n\n# The numbers in this output are individual rows (not numbers of groups)\north_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;72/36/108&gt;\n\nFrom here, the code to get the resulting data sets is the same as previously shown. We’ll also verify that no subjects are in both data sets:\n\north_tr &lt;- training(orth_split)\north_te  &lt;- testing(orth_split)\n\n# Is there any overlap in the subjects? \nsubjects_train &lt;- unique(orth_tr$Subject)\nsubjects_test  &lt;- unique(orth_te$Subject)\n\nintersect(subjects_train, subjects_test)\n#&gt; ordered()\n#&gt; 27 Levels: M16 &lt; M05 &lt; M02 &lt; M11 &lt; M07 &lt; M08 &lt; M03 &lt; M12 &lt; M13 &lt; M14 &lt; ... &lt; F11",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-spatial-splitting",
    "href": "chapters/initial-data-splitting.html#sec-spatial-splitting",
    "title": "3  Initial Data Splitting",
    "section": "\n3.8 Splitting Spatial Data",
    "text": "3.8 Splitting Spatial Data\nFor spatial data, we need to convert longitude and latitude to a special type of vector called a geometry vector. The sf package can do this so let’s load that and two others:\n\nlibrary(sf)\n#&gt; Linking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\nlibrary(spatialsample)\nlibrary(tidysdm)\n\nThe function to make the conversion is sf::st_as_sf():\n\names_sf &lt;-\n  ames %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n\names_sf %&gt;% select(geometry)\n#&gt; Simple feature collection with 2930 features and 0 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -93.69 ymin: 41.99 xmax: -93.58 ymax: 42.06\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 2,930 × 1\n#&gt;         geometry\n#&gt;      &lt;POINT [°]&gt;\n#&gt; 1 (-93.62 42.05)\n#&gt; 2 (-93.62 42.05)\n#&gt; 3 (-93.62 42.05)\n#&gt; 4 (-93.62 42.05)\n#&gt; 5 (-93.64 42.06)\n#&gt; 6 (-93.64 42.06)\n#&gt; # ℹ 2,924 more rows\n\nThe crs argument specifies a coordinate reference system. We used 4326 since it contains the US.\nNote that this is a sticky column and can’t be removed via normal means. There is a function to do this though: sf::st_drop_geometry().\nThe tidysdm package can create an initial data split using the methods given in the spatialsample package. The function is tidysdm::spatial_initial_split(). The main argument is prop for the amount of data in the testing set. Let’s put 20% into testing using prop = 0.2.\nAfter specifying this, we can set a blocking strategy using spatialsample, such as:\n\n\nspatial_block_cv(): create a regular grid.\n\nspatial_clustering_cv(): use a clustering method to group the data.\n\nFor the blocking method, we need to specify the number of blocks (n), whether the block is square or hexagonal (square), and (optionally) the radius for a buffer (buffer). These options will define how to create blocks. There is another option that designates how the blocks are assigned to the training set: method. You should experiment with these for your data. We’ll use method = \"continuous\" in this example.\n\nset.seed(318)\names_block_split &lt;-\n  spatial_initial_split(\n    ames_sf, \n    prop = 0.2, \n    strategy = spatial_block_cv,\n    method = \"continuous\",\n    n = 25, \n    square = FALSE)\names_block_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;2326/604/2930&gt;\n\nThere is a handy autoplot() method to visually assess the results:\n\nautoplot(ames_block_split, cex = 1 / 2) + \n  ggtitle(\"Blocking, no buffer\") + \n  theme_bw()\n\n\n\n\n\n\n\nLet’s add a buffer around these blocks to stop adjacent data from being the training and testing sets. We can experiment with the buffer argument until we have results that seem acceptable.\n\nset.seed(318)\names_block_buff_split &lt;-\n  spatial_initial_split(\n    ames_sf, \n    prop = 0.2, \n    strategy = spatial_block_cv,\n    method = \"continuous\",\n    n = 25, \n    square = FALSE,\n    buffer = 250)\names_block_buff_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;1587/604/2930&gt;\n\nThe buffer points are on gray:\n\nautoplot(ames_block_buff_split, cex = 1 / 2) + \n  ggtitle(\"Blocking with a buffer\") + \n  theme_bw()\n\n\n\n\n\n\n\nAs with the previous methods, the training() and testing() functions are used to obtain those data sets.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html",
    "href": "chapters/missing-data.html",
    "title": "4  Missing Data",
    "section": "",
    "text": "4.1 Requirements\nThis chapter outlines how to work with missing data when building prediction models.\nA general discussion on missing data in R can be found in R for Data Science (2e). The Missing Book is an excellent reference that supplements this chapter.\nThe data will be taken from A morphometric modeling approach to distinguishing among bobcat, coyote, and gray fox scats. The data set is designed to see how well experts can determine which of three species (bobcats, coyotes, and gray foxes) can be identified by their poop feces (a.k.a. scat). There are physical measurements as well as some laboratory tests that can be used as predictors. The species is the outcome. The data are in the modeldata package.\nYou’ll need 3 packages (naniar, ranger, and tidymodels) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"naniar\", \"ranger\", \"tidymodels\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())\nThe data are automatically attached when the tidymodels package is loaded. The data frame is named scat. Let’s split the data into training and testing, create some resamples (to be discussed in chapter TODO), and a data frame of predictor values. For both data splitting steps, we’ll stratify by the species since the frequencies of each are not balanced.\nset.seed(383)\nscat_split &lt;- initial_split(scat, strata = Species)\nscat_tr &lt;- training(scat_split)\nscat_te &lt;- testing(scat_split)\n\nscat_rs &lt;- vfold_cv(scat_tr, repeats = 5, strata = Species)\n\nscat_tr_preds &lt;- scat_tr %&gt;% select(-Species)\nHere is the breakdown of the species per data partition:\nscat_tr %&gt;% count(Species)\n#&gt; # A tibble: 3 × 2\n#&gt;   Species      n\n#&gt;   &lt;fct&gt;    &lt;int&gt;\n#&gt; 1 bobcat      42\n#&gt; 2 coyote      21\n#&gt; 3 gray_fox    18\nscat_te %&gt;% count(Species)\n#&gt; # A tibble: 3 × 2\n#&gt;   Species      n\n#&gt;   &lt;fct&gt;    &lt;int&gt;\n#&gt; 1 bobcat      15\n#&gt; 2 coyote       7\n#&gt; 3 gray_fox     7\nWe’ll spend some time on visualization and summary techniques that are helpful when performing exploratory data analysis.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#sec-missing-eda",
    "href": "chapters/missing-data.html#sec-missing-eda",
    "title": "4  Missing Data",
    "section": "\n4.2 Investigating Missing Data",
    "text": "4.2 Investigating Missing Data\nThe naniar package is an excellent tool for missing data. Let’s load it and then get a summary of our training set variables.\n\nlibrary(naniar)\n\nmiss_var_summary(scat_tr_preds) %&gt;% print(n = Inf)\n#&gt; # A tibble: 18 × 3\n#&gt;    variable  n_miss pct_miss\n#&gt;    &lt;chr&gt;      &lt;int&gt;    &lt;num&gt;\n#&gt;  1 Taper         14   17.28 \n#&gt;  2 TI            14   17.28 \n#&gt;  3 Diameter       4    4.938\n#&gt;  4 d13C           2    2.469\n#&gt;  5 d15N           2    2.469\n#&gt;  6 CN             2    2.469\n#&gt;  7 Mass           1    1.235\n#&gt;  8 Month          0    0    \n#&gt;  9 Year           0    0    \n#&gt; 10 Site           0    0    \n#&gt; 11 Location       0    0    \n#&gt; 12 Age            0    0    \n#&gt; 13 Number         0    0    \n#&gt; 14 Length         0    0    \n#&gt; 15 ropey          0    0    \n#&gt; 16 segmented      0    0    \n#&gt; 17 flat           0    0    \n#&gt; 18 scrape         0    0\n\nThe miss_var_summary() function summarizes the missing data in the predictors. The n_miss column is the number of missing values, and the p_miss column is the proportion of missing values.\nFor convenience, let’s make character vectors of column names for predictors with and without missing values.\n\nmiss_cols &lt;- c(\"Taper\", \"TI\", \"Diameter\", \"Mass\", \"d13C\", \"d15N\", \"CN\")\nnon_miss_cols &lt;- c(\"Month\", \"Year\", \"Site\", \"Location\", \"Age\", \"Number\", \"Length\", \n                   \"ropey\", \"segmented\", \"flat\", \"scrape\")\n\nWe can make an upset plot, which visualizes frequently occurring subsets in the high-dimensional Venn diagram where each predictor is encoded as missing/not missing:\n\nlibrary(naniar)\ngg_miss_upset(scat_tr_preds, nsets = 10)\n\n\n\n\n\n\n\nFrom this, we might notice that there might be two different mechanisms causing missing data. First, the laboratory values for predictors (d13C, d15N, and CN) are only missing with one another. This suggests that some laboratory errors may be the cause of their missingness. The second set of predictors that are missing at once are all related to physical properties measured on-site. If the diameter and taper predictors cannot be ascertained, it might be because the scat sample might not have been… solid enough to measure. These assertions, if accurate, help us understand the type of missingness involved and, by extension, how to handle them.\nThe steps we would take to address missingness in the predictors is a preprocessing step; the tidymodels approach is to handle these in a recipes object. Recipes are more thoroughly introduced in Section 5.4. For now, we’ll show their usage and defer the broader information on how recipes work (and why you might want to use them) to the next chapter.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#filtering",
    "href": "chapters/missing-data.html#filtering",
    "title": "4  Missing Data",
    "section": "\n4.3 Filtering",
    "text": "4.3 Filtering\nA recipe consists of an initialized object and a sequence of one or more “steps” that define specific actions/computations that should be done to the data prior to modeling.\nThe initialization consists of a call to recipe::recipe(). The most common interface used there is a formula. This declares which column is the outcome and which are predictors. For example:\n\nscat_rec &lt;- recipe(Species ~ ., data = scat_tr)\n\nAt this point, the recipe catalogs each column’s name, data type, and role (i.e., predictor or outcome). From there, we can add step functions to specify what should be done to what columns.\nTwo recipes steps can be used for filtering missing data: recipes::step_naomit() and recipes::step_filter_missing(). The former removes rows of the training set, and the latter removes predictors if they have too many missing values.\n\n4.3.1 Row Filtering\nLet’s add step_naomit() to the recipe and declare the dplyr selector dplyr::everything() should be used to capture which columns should be checked for missing rows:\n\nna_omit_rec &lt;- \n  scat_rec %&gt;% \n  step_naomit(everything())\n\nTo estimate the recipe (manually), we can use recipes::prep() to process the training set and use these values to decide which rows to omit:\n\nna_omit_rec &lt;- \n  scat_rec %&gt;% \n  step_naomit(everything()) %&gt;% \n  prep()\n\nThe recipes::bake() function can be used to apply the recipe to a data set. Before processing, there are 81 scat samples in the training set. How many remain after applying the recipe?\nTo do this, we can use the bake() function but supply new_data = NULL. This is a shortcut: when preparing the recipe, we must execute all the steps on the entire training set. By default, recipes save the preprocessed version of the training set in the recipe object. There’s no need to re-process the data.\nThe results is that we loose 16 scat samples due to missingness:\n\nall_complete &lt;- bake(na_omit_rec, new_data = NULL)\n\nnrow(all_complete)\n#&gt; [1] 65\n\nHowever, step_naomit() is a bit irregular compared to other recipe steps. It is designed to skip execution on every other data set. This is an important (and appropriate) choice for this method.\nIf we were to apply the recipe to the test set, it would not exclude the missing rows:\n\nbake(na_omit_rec, new_data = scat_te) %&gt;% nrow()\n#&gt; [1] 29\n\nnrow(scat_te)\n#&gt; [1] 29\n\n# but there are missing values:\nsum(!complete.cases(scat_te))\n#&gt; [1] 3\n\n\n\n4.3.2 Column Filtering\nThe sample size of this data set is not large; removing rows might be more problematic than removing columns with a lot of missingness. We can use the step_filter_missing() step to do this. We decide on a threshold representing our “line of dignity” regarding how much missingness is acceptable. That can be specified as a proportion of missing data and is passed to the threshold argument.\nHere’s an example where we determine that more than 10% missingness is too much. Based on our results from the naniar package above, this should eliminate two predictors (Taper and TI).\n\nfilter_features_rec &lt;- \n  scat_rec %&gt;% \n  step_filter_missing(everything(), threshold = 0.10) %&gt;% \n  prep()\n\nncol(scat_tr)\n#&gt; [1] 19\nbake(filter_features_rec, new_data = NULL) %&gt;% ncol()\n#&gt; [1] 17\n\n# use the tody method to determine which were removed: \ntidy(filter_features_rec, number = 1)\n#&gt; # A tibble: 2 × 2\n#&gt;   terms id                  \n#&gt;   &lt;chr&gt; &lt;chr&gt;               \n#&gt; 1 Taper filter_missing_jh0Cw\n#&gt; 2 TI    filter_missing_jh0Cw",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#imputation",
    "href": "chapters/missing-data.html#imputation",
    "title": "4  Missing Data",
    "section": "\n4.4 Imputation",
    "text": "4.4 Imputation\nThe recipes package has several steps for imputing predictors: recipes::step_impute_bag(), recipes::step_impute_knn(), recipes::step_impute_linear(), recipes::step_impute_lower(), recipes::step_impute_mean(), recipes::step_impute_median(), recipes::step_impute_mode(), recipes::step_impute_roll().\n\n4.4.1 Linear Regression\nLet’s consider using linear regression to predict the rows missing their value of Taper. The imputation steps allow you to select which column to impute and which predictors to use as predictors in the imputation model.\nIf we were to predictor Taper as a function of Age, Length, Number, and Location, the code would be:\n\nlin_impute_rec &lt;- \n  scat_rec %&gt;% \n  step_impute_linear(Taper, impute_with = imp_vars(Age, Length, Number, Location)) %&gt;% \n  prep() # &lt;- This estimates the regression\n\n# Imputing the test set: \nlin_impute_rec %&gt;% \n  bake(new_data = scat_te, Taper) %&gt;% \n  filter(is.na(Taper))\n#&gt; # A tibble: 0 × 1\n#&gt; # ℹ 1 variable: Taper &lt;dbl&gt;\n\nThe tidy() methods can extract the model object. We’ll use tidyr::enframe() to get the coefficients:\n\nlm_res &lt;- tidy(lin_impute_rec, number = 1) \nlm_res\n#&gt; # A tibble: 1 × 3\n#&gt;   terms model  id                 \n#&gt;   &lt;chr&gt; &lt;list&gt; &lt;chr&gt;              \n#&gt; 1 Taper &lt;lm&gt;   impute_linear_dtBVd\n\nenframe(coef(lm_res$model[[1]]))\n#&gt; # A tibble: 6 × 2\n#&gt;   name               value\n#&gt;   &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 (Intercept)      38.34  \n#&gt; 2 Age              -1.603 \n#&gt; 3 Length            0.2712\n#&gt; 4 Number           -2.480 \n#&gt; 5 Locationmiddle   -5.366 \n#&gt; 6 Locationoff_edge  5.216\n\nWe might also want to impute the predictor based on its mean value but would like it to be different based on some other grouping column. We can accomplish this by using a single categorical predictor in the formula (such as Location):\n\ngroup_impute_rec &lt;- \n  scat_rec %&gt;% \n  step_mutate(Taper_missing = is.na(Taper)) %&gt;% \n  step_impute_linear(Taper, impute_with = imp_vars(Location)) %&gt;% \n  prep()\n\ngroup_impute_rec %&gt;% \n  bake(new_data = scat_tr, Taper, Taper_missing, Location) %&gt;% \n  filter(Taper_missing) %&gt;% \n  count(Taper, Location)\n#&gt; # A tibble: 3 × 3\n#&gt;   Taper Location     n\n#&gt;   &lt;dbl&gt; &lt;fct&gt;    &lt;int&gt;\n#&gt; 1 24.18 middle      10\n#&gt; 2 29.87 edge         2\n#&gt; 3 30.52 off_edge     2\n\n\n\n4.4.2 Nearest-Neighbor Imputation\nThe syntax for imputation steps is very consistent, so the only change that would be made to move from linear imputation to a nonlinear, nearest-neighbor method would be to change the name.\nThe number of neighbors defaults to five. We can change that using the neighbors option:\n\nknn_impute_rec &lt;- \n  scat_rec %&gt;% \n  step_impute_knn(\n    all_of(miss_cols), \n    impute_with = imp_vars(Age, Length, Number, Location),\n    neighbors = 5) %&gt;% \n  prep()\n\nimputed_train &lt;- \n  knn_impute_rec %&gt;% \n  bake(new_data = scat_tr)\n\nmean(complete.cases(imputed_train))\n#&gt; [1] 1\n\nNote that this step uses Gower distance to define the neighbors. This method does not require the predictors to be numeric or in the same units; they can be left as-is. Also, the function keeps the imputed data in the same format. A categorical predictor being imputed will remain a categorical predictor.\n\n\n4.4.3 Tuning the Preprocessors\nThe syntax to tune parameters will be described in depth in Chapter 9. Let’s briefly show that preprocessing parameters can also be tuned.\nMany tree-based models can naturally handle missingness. Random forest models compute a large number of tree-based models and combine them into an ensemble model. Unfortunately, most implementations of random forests require complete data.\nLet’s use our neighbor-based imputation method but tune the number of neighbors. At the same time, we can tune the random forest \\(n_{min}\\) parameter using a space-filling grid.\nTo do this, we give the neighbors argument of step_impute_knn() a value of tune(). This marks it for optimization. tidymodels knows a lot about these parameters and can make informed decisions about the range and scale of the tuning parameters. With the tune::tune_grid() function, using grid = 15 will automatically create a two-factor grid of candidate models to evaluate.\n\nknn_impute_rec &lt;- \n  scat_rec %&gt;% \n  step_impute_knn(\n    all_of(miss_cols), \n    impute_with = imp_vars(Age, Length, Number, Location),\n    neighbors = tune()) \n\nrf_spec &lt;- \n  rand_forest(min_n = tune(), trees = 1000) %&gt;% \n  set_mode(\"classification\")\n\nknn_rf_wflow &lt;- workflow(knn_impute_rec, rf_spec)\n\nknn_rf_res &lt;- \n  knn_rf_wflow %&gt;% \n  tune_grid(\n    scat_rs,\n    grid = 15\n  )\n\nLooking at the results below, we can see that the number of neighbors does not seem to affect performance (measured via a Brier score). However, for these data, the random forests \\(n_{min}\\) parameter does have a profound effect on model performance.\n\nshow_best(knn_rf_res, metric = \"brier_class\")\n#&gt; # A tibble: 5 × 8\n#&gt;   min_n neighbors .metric     .estimator   mean     n  std_err .config              \n#&gt;   &lt;int&gt;     &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1     2         6 brier_class multiclass 0.1843    50 0.008136 Preprocessor01_Model1\n#&gt; 2     4         2 brier_class multiclass 0.1855    50 0.007938 Preprocessor02_Model1\n#&gt; 3    10         9 brier_class multiclass 0.1900    50 0.007496 Preprocessor04_Model1\n#&gt; 4     7         4 brier_class multiclass 0.1904    50 0.007951 Preprocessor03_Model1\n#&gt; 5    12         1 brier_class multiclass 0.1937    50 0.007449 Preprocessor05_Model1\n\nautoplot(knn_rf_res, metric = \"brier_class\")",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html",
    "href": "chapters/numeric-predictors.html",
    "title": "5  Transforming Numeric Predictors",
    "section": "",
    "text": "5.1 Requirements\nThe corresponding book chapter is concerned with operations to improve how numeric predictor variables are represented in the data prior to modeling. We separate these operations into two categories:\nIn either case, we estimate these transformations exclusively from the training set and apply them to any data (e.g., the training set, test set, and/or new or unknown data). This is generally true and applies to upcoming chapters on categorical data and other transformations.\nIn tidymodels, just about everything you want to do to your predictors can be accomplished using the R formula method or, better still, the recipes package. We shall describe both.\nYou’ll need 4 packages (bestNormalize, embed, splines2, and tidymodels) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"bestNormalize\", \"embed\", \"tidymodels\", \"splines2\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-hotel-data-intro",
    "href": "chapters/numeric-predictors.html#sec-hotel-data-intro",
    "title": "5  Transforming Numeric Predictors",
    "section": "\n5.2 Data Sets",
    "text": "5.2 Data Sets\nThe data sets used here are both in R packages that have already been installed. Let’s work with the primary data set: the Ames Iowa housing data.\nIn the last chapter, our manipulation and splitting code was:\n\ndata(ames, package = \"modeldata\")\n\names &lt;-\n  ames %&gt;%\n  select(Sale_Price, Bldg_Type, Neighborhood, Year_Built, Gr_Liv_Area, Full_Bath,\n         Half_Bath, Year_Sold, Lot_Area, Central_Air, Longitude, Latitude) %&gt;%\n  mutate(\n    Sale_Price = log10(Sale_Price),\n    Baths = Full_Bath  + Half_Bath/2\n  ) %&gt;%\n  select(-Half_Bath, -Full_Bath)\n\nset.seed(3024)\names_split &lt;- initial_split(ames, strata = Sale_Price, breaks = 5)\names_train &lt;- training(ames_split)\names_test  &lt;- testing(ames_split)\n\nWe’ll work with ames_train almost exclusively here.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-r-formulas",
    "href": "chapters/numeric-predictors.html#sec-r-formulas",
    "title": "5  Transforming Numeric Predictors",
    "section": "\n5.3 Standard R Formulas",
    "text": "5.3 Standard R Formulas\nModel formulas in R are identical to those in S, which Chambers and Hastie introduced in Statistical Models in S (1991). A broader discussion can be found in two blog posts (one and two).\nThe formula has a few basic operators:\n\nThe tilde (~) separates the outcome columns from the predictor columns. Anything to the left is considered an outcome, and the right-hand side defines predictors (e.g., outcome ~ predictor\n\nA dot is a wildcard for any columns in the data set that are not outcomes (e.g., y ~ .).\nPlus signs signify the symbolic addition of columns to the formula (typically predictors). For example, y ~ x1 + x2 indicates one outcome and two predictor columns. To indicate arithmetic addition (or any other computations), you can wrap the items in the identity function I() such as y ~ I(x1 + x2).\nYou can use the minus sign to remove columns. This may not be implemented in some modeling functions.\nThe colon indicates interaction terms (described in a future chapter).\n\nThere is further syntax described below.\nHere’s an example of a basic formula that creates two predictor columns by specifying a symbolic formula comprised of two numeric predictors.\n\nf_01 &lt;- Sale_Price ~  Baths + Year_Built\n\nHere’s a short function to show basic results:\n\nshow_columns &lt;- function(f) {\n  model.matrix(f, data = ames_train) %&gt;% \n  tibble::as_tibble() %&gt;% \n  dplyr::slice(c(1, 3, 9))\n}\nshow_columns(f_01)\n#&gt; # A tibble: 3 × 3\n#&gt;   `(Intercept)` Baths Year_Built\n#&gt;           &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1             1   1         1961\n#&gt; 2             1   1.5       1971\n#&gt; 3             1   2         1962\n\nIt does not use row-wise arithmetic additions of the two columns. To do that, you can use the identify function:\n\n# One slope term, not two\nf_02 &lt;- Sale_Price ~  I(Full_Bath + Half_Bath)\n\nSymbolic addition creates separate columns of the data set. In chapter TODO, we’ll discuss main effects and interactions. The main effects are features composed of a single predictor (as in f_01 above). Interaction effects are one or more model terms that combine the information of all the predictors in a multiplicative way. There are a few ways to specify them. Here are three methods for specifying two-factor interactions between predictors:\n\n# `:` is used for specific interactions\nf_03 &lt;- Sale_Price ~  Baths + Year_Built + Baths:Year_Built\n\n# `*` is used to make all interactions of two or more terms\nf_04 &lt;- Sale_Price ~  Baths * Year_Built\n\n# `()^D` makes interactions up to order D of all of the columns\n# within the parenthesis\nf_05 &lt;- Sale_Price ~  (Baths + Year_Built)^2\n\nshow_columns(f_05)\n#&gt; # A tibble: 3 × 4\n#&gt;   `(Intercept)` Baths Year_Built `Baths:Year_Built`\n#&gt;           &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;              &lt;dbl&gt;\n#&gt; 1             1   1         1961              1961 \n#&gt; 2             1   1.5       1971              2956.\n#&gt; 3             1   2         1962              3924\n\nFor this data set, the right-hand side of f_05 could be shortened to (.)^2.\nSince Baths and Year_Built are both numeric, their interactions are created by simply multiplying their values, i.e., I(Baths * Year_Built).\nBy default, the model formula creates an intercept column where the value of each row is 1.0. To prevent the intercept from being added, there are two syntaxes:\n\nf_06 &lt;- Sale_Price ~  Baths - 1 \nf_07 &lt;- Sale_Price ~  Baths + 0\n\nshow_columns(f_07)\n#&gt; # A tibble: 3 × 1\n#&gt;   Baths\n#&gt;   &lt;dbl&gt;\n#&gt; 1   1  \n#&gt; 2   1.5\n#&gt; 3   2\n\nWhat happens with factor predictors? Their specification is the same:\n\nf_08 &lt;- Sale_Price ~  Bldg_Type \n\nHowever, most of the time1, the formula method creates columns of binary 0/1 to replace the original factor column. Since there are 5 possible values of Bldg_Type, the formula creates 4 columns of indicator variables, each corresponding to a specific level. The first factor level is excluded by default. This is discussed more in Working with Categorical Predictors.\n\n# Note that the resulting column names smash the original column\n# name an its factor level together with no delimiter. \nshow_columns(f_08)\n#&gt; # A tibble: 3 × 5\n#&gt;   `(Intercept)` Bldg_TypeTwoFmCon Bldg_TypeDuplex Bldg_TypeTwnhs Bldg_TypeTwnhsE\n#&gt;           &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1             1                 0               0              0               0\n#&gt; 2             1                 0               0              1               0\n#&gt; 3             1                 0               1              0               0\n\nFor interaction terms, the syntax is the same as the one shown above. In the case of categorical predictors, all combinations of the predictors are created. In the following case, Central_Air has two levels. A two-way interaction of these two predictors creates 4 \\(\\times\\) 1 = 4 interaction columns.\n\nf_09 &lt;- Sale_Price ~  (Bldg_Type + Central_Air)^2\n\nshow_columns(f_09)\n#&gt; # A tibble: 3 × 10\n#&gt;   `(Intercept)` Bldg_TypeTwoFmCon Bldg_TypeDuplex Bldg_TypeTwnhs Bldg_TypeTwnhsE\n#&gt;           &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1             1                 0               0              0               0\n#&gt; 2             1                 0               0              1               0\n#&gt; 3             1                 0               1              0               0\n#&gt; # ℹ 5 more variables: Central_AirY &lt;dbl&gt;, `Bldg_TypeTwoFmCon:Central_AirY` &lt;dbl&gt;,\n#&gt; #   `Bldg_TypeDuplex:Central_AirY` &lt;dbl&gt;, `Bldg_TypeTwnhs:Central_AirY` &lt;dbl&gt;,\n#&gt; #   `Bldg_TypeTwnhsE:Central_AirY` &lt;dbl&gt;\n\nWhat happens when you exclude the intercept? All factor levels receive a binary indicator column for a single categorical predictor.\n\nf_10 &lt;- Sale_Price ~  Bldg_Type + 0\n\nshow_columns(f_10)\n#&gt; # A tibble: 3 × 5\n#&gt;   Bldg_TypeOneFam Bldg_TypeTwoFmCon Bldg_TypeDuplex Bldg_TypeTwnhs Bldg_TypeTwnhsE\n#&gt;             &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1               1                 0               0              0               0\n#&gt; 2               0                 0               0              1               0\n#&gt; 3               0                 0               1              0               0\n\nHowever, this may produce unexpected results when multiple factor predictors exist. The first factor in the formula creates all possible indicators (e.g., 5 for Bldg_Type) while the others have all but one factor level created. For example, these two formulas would have different columns:\n\nf_11 &lt;- Sale_Price ~ Bldg_Type + Central_Air + 0\nf_12 &lt;- Sale_Price ~ Central_Air + Bldg_Type + 0\n\nshow_columns(f_11) %&gt;% names() %&gt;% sort()\n#&gt; [1] \"Bldg_TypeDuplex\"   \"Bldg_TypeOneFam\"   \"Bldg_TypeTwnhs\"    \"Bldg_TypeTwnhsE\"  \n#&gt; [5] \"Bldg_TypeTwoFmCon\" \"Central_AirY\"\nshow_columns(f_12) %&gt;% names() %&gt;% sort()\n#&gt; [1] \"Bldg_TypeDuplex\"   \"Bldg_TypeTwnhs\"    \"Bldg_TypeTwnhsE\"   \"Bldg_TypeTwoFmCon\"\n#&gt; [5] \"Central_AirN\"      \"Central_AirY\"\n\nThere model predictions and anova() results will be the same but the interpretation of their coefficients will be very different.\nYou can use in-line functions within a recipe. For example:\n\nlibrary(splines2)\nf_13 &lt;- Sale_Price ~  log(Gr_Liv_Area) + scale(Lot_Area) + naturalSpline(Latitude, df = 3)\n\nshow_columns(f_13)\n#&gt; # A tibble: 3 × 6\n#&gt;   `(Intercept)` `log(Gr_Liv_Area)` `scale(Lot_Area)`\n#&gt;           &lt;dbl&gt;              &lt;dbl&gt;             &lt;dbl&gt;\n#&gt; 1             1              6.798            0.1994\n#&gt; 2             1              6.895           -1.103 \n#&gt; 3             1              7.455            0.4139\n#&gt; # ℹ 3 more variables: `naturalSpline(Latitude, df = 3)1` &lt;dbl&gt;,\n#&gt; #   `naturalSpline(Latitude, df = 3)2` &lt;dbl&gt;,\n#&gt; #   `naturalSpline(Latitude, df = 3)3` &lt;dbl&gt;\n\nuses three in-line functions:\n\nThe first is a simple log transformation of the gross living area.\nThe use of scale() will compute the mean and standard deviation of Lot_Area and use those to center and scale that column.\nThe function splines2::naturalSpline() will create a set of basis functions (described in chapter TODO) that will replace the original Latitude column.\n\nIn the second and third cases, R’s machinery will estimate the relevant statistics and embed them as attributes in the corresponding columns. For each in-line function, the exact same operations are conducted on new data (say when predict() is called).\nFinally, be aware that each formula captures the environment in which it was created. For example:\n\nenvironment(f_12)\n#&gt; &lt;environment: R_GlobalEnv&gt;\n\n# The number of objects in the session used to create this web page (up to now):\nlength(ls(envir = environment(f_12)))\n#&gt; [1] 26\n\nIf an object that used f_12 is saved to disk, it will also contain the 26 objects in the global environment. If any of these objects are large, it can unintentionally make the saved data object large. Note that using the base function object.size() will not take into account anything stored in the environment (so the binary file size is underestimated). lobstr::obj_size() will give a more accurate estimate.\nThe butcher package has tools to strip off these unneeded objects from formulas (or objects that contain formulas). Also, butcher::weigh() returns a tibble with the size of each element contained in the object (if any).",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-recipe-intro",
    "href": "chapters/numeric-predictors.html#sec-recipe-intro",
    "title": "5  Transforming Numeric Predictors",
    "section": "\n5.4 What is a Recipe?",
    "text": "5.4 What is a Recipe?\nA recipe is a set of sequential steps that specify what operations should be conducted on a set of predictors. Operations could include:\n\nModifying a predictor’s encoding (e.g., date to month/day/year columns)\nAdding new features, such as basis expansions.\nStandardizing or transforming individual predictors.\nFeature extraction or embeddings on multiple predictors.\n\nRemoving features.\n\nRecipes can be used by themselves or as part of a modeling pipeline. For illustration, we’ll show how to use them directly. The process is to\nspecify -&gt; estimate -&gt; apply\nthe recipe. In terms of syntax, the analogous functions are:\nrecipe() -&gt; prep() -&gt; bake()\nWe’ll start simply by trying to “unskew” a predictor’s distribution.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-recipe-skewness",
    "href": "chapters/numeric-predictors.html#sec-recipe-skewness",
    "title": "5  Transforming Numeric Predictors",
    "section": "\n5.5 Resolving Asymmetry and Skewness",
    "text": "5.5 Resolving Asymmetry and Skewness\nThe main text mentions that the distribution of the Lot_Area variable is skewed. Let’s see what that looks like.\n\names_train %&gt;% \n  ggplot(aes(Lot_Area)) + \n  geom_histogram(bins = 30, col = \"white\", fill = \"#8E195C\", alpha = 1 / 2) +\n  geom_rug(alpha = 1 / 2, length = unit(0.03, \"npc\"), linewidth = 1) +\n  labs(x = \"Lot Area\")\n\n\n\n\n\n\n\nTo get started, we initialize a recipe with the recipe() function and a data set:\n\nunskew_rec &lt;- recipe(Sale_Price ~ ., data = ames_train)\n\nThe formula method doesn’t do much here: it records the outcome (columns to the left of ~), which are predictors (to the right of ~ ), and their data types. Note that the . in the formula means that all columns, except those to the left, should be considered predictors. When using a formula to start a recipe, keep it simple. It won’t accept any in-line functions (like sqrt() or log()); it wants you to change the variables inside of recipe steps.\nRegarding the data argument: any data set with the appropriate columns could be used. The initial recipe work is just cataloging the columns. You could even use a “zero row slice” such as ames_train[0,] and get the same results. You might want to do something like this if you have a very large training set (to reduce the in-memory footprint). The main advantage of using ames_train is convenience (as we’ll see later).\nWe’ll add different recipe step functions from this initial object to declare what we want to do. Let’s say we will transform the lot area column using the Yeo-Johnsom transformation. To do this:\n\nunskew_rec &lt;- \n  recipe(Sale_Price ~ ., data = ames_train) %&gt;% \n  step_YeoJohnson(Lot_Area)\n\n# or use a dplyr selector:\nunskew_rec &lt;- \n  recipe(Sale_Price ~ ., data = ames_train) %&gt;% \n  step_YeoJohnson(any_of(\"Lot_Area\"))\n\nunskew_rec\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:    1\n#&gt; predictor: 10\n#&gt; \n#&gt; ── Operations\n#&gt; • Yeo-Johnson transformation on: any_of(\"Lot_Area\")\n\nor starts_with(\"Lot_\") and so on.\nThis only specifies what we want to do. Recall that the Yeo-Johnson transformation estimates a transformation parameter from the data. To estimate the recipe, use prep():\n\nunskew_rec &lt;- prep(unskew_rec)\n\n# or, to use a different data set: \nunskew_rec &lt;- prep(unskew_rec, training = ames_train)\n#&gt; Warning in prep(unskew_rec, training = ames_train): ! The previous data will be used by `prep()`.\n#&gt; ℹ The data passed using `training` will be ignored.\nunskew_rec\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:    1\n#&gt; predictor: 10\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 2196 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Yeo-Johnson transformation on: Lot_Area | Trained\n\nNote that the printed recipe shows that Lot_Area was resolved from the original request for any_of(\"Lot_Area\").\nWhat was the estimate of the transformation parameter? The tidy() method can tell us:\n\n# Get the list of steps: \ntidy(unskew_rec)\n#&gt; # A tibble: 1 × 6\n#&gt;   number operation type       trained skip  id              \n#&gt;    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n#&gt; 1      1 step      YeoJohnson TRUE    FALSE YeoJohnson_VL1H3\n\n# Get information about the first step: \ntidy(unskew_rec, number = 1)\n#&gt; # A tibble: 1 × 3\n#&gt;   terms     value id              \n#&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;           \n#&gt; 1 Lot_Area 0.1503 YeoJohnson_VL1H3\n\nNow that we have a trained recipe, we can use it via bake():\n\n# Get the list of steps: \nbake(unskew_rec, new_data = head(ames_train))\n#&gt; # A tibble: 6 × 11\n#&gt;   Bldg_Type Neighborhood    Year_Built Gr_Liv_Area Year_Sold Lot_Area Central_Air\n#&gt;   &lt;fct&gt;     &lt;fct&gt;                &lt;int&gt;       &lt;int&gt;     &lt;int&gt;    &lt;dbl&gt; &lt;fct&gt;      \n#&gt; 1 OneFam    North_Ames            1961         896      2010    20.52 Y          \n#&gt; 2 OneFam    North_Ames            1971         864      2010    20.11 Y          \n#&gt; 3 Twnhs     Briardale             1971         987      2010    13.67 Y          \n#&gt; 4 Twnhs     Briardale             1971        1092      2010    13.67 Y          \n#&gt; 5 Twnhs     Northpark_Villa       1975         836      2010    14.62 Y          \n#&gt; 6 OneFam    Sawyer_West           1920        1012      2010    19.83 N          \n#&gt; # ℹ 4 more variables: Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt;, Baths &lt;dbl&gt;,\n#&gt; #   Sale_Price &lt;dbl&gt;\n\nDid it work? Let’s look at the whole training set:\n\nunskew_rec %&gt;% \n  bake(new_data = ames_train) %&gt;% \n  ggplot(aes(Lot_Area)) +\n  geom_rug(alpha = 1 / 2, length = unit(0.03, \"npc\"), linewidth = 1) + \n  geom_histogram(bins = 30, col = \"white\", fill = \"#8E195C\", alpha = 1 / 2) +\n  labs(x = \"Lot Area\")\n\n\n\n\n\n\n\nOne shortcut we can take: the recipe has to apply each step to the training data after it estimates the step. By default, the recipe object saves the processed version of the data set. This can be turned off using the retain = FALSE option to prep(). Since the training set is already in the recipe, we can get it with no additional computations using\nbake(unskew_rec, new_data = NULL) \nThe main site mentions a few other methods that could be used besides Yeo-Johnson:\n\nBox-Cox: step_BoxCox()\n\nPercentile: step_percentile()\n\norderNorm: step_orderNorm()\n\n\nNote that the last method has its step function in the bestNormalize package; various recipe extension packages can be used. A full set of recipe steps for CRAN packages is available on tidymodels.org.\nThere is also a general step for simple computations that do not need to be estimated. If we were to log transform the data, we would use:\nrecipe(Sale_Price ~ ., data = ames_train) %&gt;% \n  step_mutate(Lot_Area = log10(Lot_Area))\nOther single variable transformations can be found in the following R packages: car, trafo, and Transform.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-recipe-selectors",
    "href": "chapters/numeric-predictors.html#sec-recipe-selectors",
    "title": "5  Transforming Numeric Predictors",
    "section": "\n5.6 More on Recipe Selectors",
    "text": "5.6 More on Recipe Selectors\nThe previous section showed a recipe step that operated on a single column. You can select one or more predictors in a variety of different ways within a recipe:\n\nBare, unquoted column names such as Lot_Area.\n\ndplyr package selectors, including starts_with(), contained(), and so on.\nSpecial, recipe-only selectors:\n\nRole-based: all_predictors(), all_outcomes(), and so on.\nType-based: all_numeric(), all_factor(), …\nCombinations: all_numeric_predictors() etc.\n\n\n\nTwo important dplyr selectors are all_of() and any_of(). These take character vectors of column names as inputs. all_of() will select all of the columns in the vector and will fail if they are not all present when the recipe step is executed. any_of() will select any of the columns that are given and won’t fail, even if none are available.\nThis is important for a few reasons. Some steps can combine or eliminate columns. A recipe should be fault tolerant; if the previous step removed column A and the next step strictly requires it, it will fail. However, if any_of(c(\"A\")) is used, it will not 2.\nThere is a documentation page for recipe selectors as well as the reference page.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-recipe-standardize",
    "href": "chapters/numeric-predictors.html#sec-recipe-standardize",
    "title": "5  Transforming Numeric Predictors",
    "section": "\n5.7 Standardizing to a Common Scale",
    "text": "5.7 Standardizing to a Common Scale\nThe two main steps for standardizing columns to have the same units are step_normalize() and step_range(). A common pattern for the former is:\n\nnorm_rec &lt;- \n  unskew_rec %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\nnorm_rec\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:    1\n#&gt; predictor: 10\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 2196 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Yeo-Johnson transformation on: Lot_Area | Trained\n#&gt; • Zero variance filter on: all_predictors()\n#&gt; • Centering and scaling for: all_numeric_predictors()\n\nstep_zv() is for removing “zero-variance” (zv) predictors. These are columns with a single unique value. Since step_normalize() will try to divide by a column’s standard deviation, this will fail if there is no variation in the column. step_zv() will remove such columns that exist in the training set.\nWe recycled the previous recipe, which has already been trained. Note that in the output above, only the first step is labeled as “Trained”. When we run prep() on this recipe, it only estimates the remaining two steps.\nAgain, once we prep(are) the recipe, we can use bake() to get the normalized data.\nAnother important point is that recipes are designed to utilize different data sets appropriately. The training set is used with prep() and ensures that all the estimations are based on it. There is, as is appropriate, no re-estimation of quantities when new data are processed.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-recipe-spatialp-sign",
    "href": "chapters/numeric-predictors.html#sec-recipe-spatialp-sign",
    "title": "5  Transforming Numeric Predictors",
    "section": "\n5.8 Spatial Sign",
    "text": "5.8 Spatial Sign\nUnsurprisingly, the step to compute the spatial sign is step_spatialsign(). It projects two or more numeric columns onto a multidimensional hypersphere. The resulting data has columns the same name as the input:\n\nlibrary(bestNormalize)\n#&gt; Registered S3 method overwritten by 'butcher':\n#&gt;   method                 from    \n#&gt;   as.character.dev_topic generics\n\nsp_sign_rec &lt;- \n  recipe(Sale_Price ~ Lot_Area + Gr_Liv_Area, data = ames_train) %&gt;% \n  step_YeoJohnson(any_of(c(\"Lot_Area\", \"Gr_Liv_Area\"))) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_orderNorm(all_numeric_predictors()) %&gt;% \n  step_spatialsign(all_numeric_predictors()) %&gt;% \n  prep()\n\nsp_sign_data &lt;- bake(sp_sign_rec, new_data = NULL)\nsp_sign_data\n#&gt; # A tibble: 2,196 × 3\n#&gt;   Lot_Area Gr_Liv_Area Sale_Price\n#&gt;      &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1   0.4496     -0.8932      5.021\n#&gt; 2   0.2349     -0.9720      5.061\n#&gt; 3  -0.9012     -0.4334      4.982\n#&gt; 4  -0.9474     -0.3199      4.944\n#&gt; 5  -0.7424     -0.6700      5.079\n#&gt; 6   0.1537     -0.9881      4.829\n#&gt; # ℹ 2,190 more rows\n\n\nsp_sign_data %&gt;% \n  ggplot(aes(Lot_Area, Gr_Liv_Area)) +\n  geom_point(cex =  2, alpha = 1 / 10, pch = 1) +\n  coord_equal()",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-recipe-resources",
    "href": "chapters/numeric-predictors.html#sec-recipe-resources",
    "title": "5  Transforming Numeric Predictors",
    "section": "\n5.9 Other Resources for Learning About Recipes",
    "text": "5.9 Other Resources for Learning About Recipes\n\n\ntidymodels.org: Preprocess your data with recipes\n\n\nTMwR chapter: Feature Engineering with recipes\n\n\nTMwR chapter: Dimensionality Reduction\n\n2023 Posit conference workshop slides: Intro: Using recipes\n\n2023 Posit conference workshop slides: Feature engineering using recipes\n\nRoles in recipes\nOrdering of steps\nStackoverflow Questions tagged [r-recipes]",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#footnotes",
    "href": "chapters/numeric-predictors.html#footnotes",
    "title": "5  Transforming Numeric Predictors",
    "section": "",
    "text": "Some model functions require these binary indicators, and others do not. You should assume they convert factor predictors to binary indicators; we will alter you when a specific function does not.↩︎\nMore accurately, it will probably be fine. Most steps are permissive; others are not. The previously described step_mutate() would fail if Lot_Area was previously eliminated.↩︎",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html",
    "href": "chapters/categorical-predictors.html",
    "title": "6  Working with Categorical Predictors",
    "section": "",
    "text": "6.1 Requirements\nThe book’s Working with Categorical Predictors chapter focuses on various ways to convert qualitative predictors into better formats for modeling.\nLike the previous chapter, we do most calculations with the recipes package.\nYou’ll need 5 packages (embed, rpart, text2vec, textrecipes, and tidymodels) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"embed\", \"text2vec\", \"textrecipes\", \"tidymodels\", \"rpart\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-hotel-prep",
    "href": "chapters/categorical-predictors.html#sec-hotel-prep",
    "title": "6  Working with Categorical Predictors",
    "section": "\n6.2 The Hotel Rate Data",
    "text": "6.2 The Hotel Rate Data\nThe hotel rate data are used for most examples in the chapter. The original version is in the modeldata package. We’ll split the data in the following way:\n\ndata(hotel_rates, package = \"modeldata\")\n\n# Make the initial split\nhotel_rates &lt;- hotel_rates %&gt;% arrange(arrival_date)\nhotel_rate_split &lt;- initial_time_split(hotel_rates, prop = c(0.75))\nhotel_rate_train &lt;- training(hotel_rate_split)\nhotel_rate_test  &lt;- testing(hotel_rate_split)\n\ninitial_time_split() will reserve that most recent 25% for the test set. It assumes that the data are arranged in time and takes that last part of the data for testing (assumed to be the most recent).",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-indicators",
    "href": "chapters/categorical-predictors.html#sec-indicators",
    "title": "6  Working with Categorical Predictors",
    "section": "\n6.3 Simple Indicator Variables",
    "text": "6.3 Simple Indicator Variables\nWe’ll look at two primary methods for creating indicator (a.k.a. “dummy”) variables.\n\n6.3.1 Base R Formulas\nBase R’s formula method, discussed previously in Section 5.3, automatically creates indicators when the formula includes a factor predictor. For example:\n\ncustomer_types &lt;- \n  hotel_rate_train %&gt;% \n  distinct(customer_type) %&gt;% \n  arrange(customer_type)\n\ncustomer_types %&gt;% \n  model.matrix( ~ customer_type, data = .) %&gt;% \n  as_tibble() %&gt;% \n  select(-`(Intercept)`)\n#&gt; # A tibble: 4 × 3\n#&gt;   customer_typegroup customer_typetransient customer_typetransient_party\n#&gt;                &lt;dbl&gt;                  &lt;dbl&gt;                        &lt;dbl&gt;\n#&gt; 1                  0                      0                            0\n#&gt; 2                  1                      0                            0\n#&gt; 3                  0                      1                            0\n#&gt; 4                  0                      0                            1\n\nNote that the column name and the factor levels are directly concatenated.\nmodel.matrix() is part of the larger base R preprocessing framework and always returns a matrix (by default, with an intercept column). There is also model.frame(). This returns a data frame without creating indicator columns or interactions. It does execute any in-line functions and only returns the columns involved in the formula.\nWhen the factor has missing values, the default behavior is to remove the offending row:\n\nlvls &lt;- levels(hotel_rate_train$customer_type)\n\nwith_missing &lt;- \n  customer_types %&gt;% \n  bind_rows(tibble(customer_type = factor(NA, levels = lvls)))\n\nwith_missing\n#&gt; # A tibble: 5 × 1\n#&gt;   customer_type  \n#&gt;   &lt;fct&gt;          \n#&gt; 1 contract       \n#&gt; 2 group          \n#&gt; 3 transient      \n#&gt; 4 transient_party\n#&gt; 5 &lt;NA&gt;\n\nmodel.matrix( ~ customer_type, data = with_missing) %&gt;% \n  as_tibble() %&gt;% \n  select(-`(Intercept)`)\n#&gt; # A tibble: 4 × 3\n#&gt;   customer_typegroup customer_typetransient customer_typetransient_party\n#&gt;                &lt;dbl&gt;                  &lt;dbl&gt;                        &lt;dbl&gt;\n#&gt; 1                  0                      0                            0\n#&gt; 2                  1                      0                            0\n#&gt; 3                  0                      1                            0\n#&gt; 4                  0                      0                            1\n\nA family of functions can be used to dictate what should be done when missing values occur. The global R option is\n\noptions()$na.action\n#&gt; [1] \"na.omit\"\n\nTo keep the number of rows intact, you can set the global option to be na.pass:\n\norig_options &lt;- options()\noptions(na.action = 'na.pass')\n\nmodel.matrix( ~ customer_type, data = with_missing) %&gt;% \n  as_tibble() %&gt;% \n  select(-`(Intercept)`)\n#&gt; # A tibble: 5 × 3\n#&gt;   customer_typegroup customer_typetransient customer_typetransient_party\n#&gt;                &lt;dbl&gt;                  &lt;dbl&gt;                        &lt;dbl&gt;\n#&gt; 1                  0                      0                            0\n#&gt; 2                  1                      0                            0\n#&gt; 3                  0                      1                            0\n#&gt; 4                  0                      0                            1\n#&gt; 5                 NA                     NA                           NA\n\n# Now reset to original settings:\noptions(orig_options) \noptions()$na.action\n#&gt; [1] \"na.omit\"\n\nIn R, the word “contrast” refers to the algorithm used to create different types of indicators 1. Global options control this (for the most part) and the defaults are:\n\noptions()$contrast\n#&gt;         unordered           ordered \n#&gt; \"contr.treatment\"      \"contr.poly\"\n\n# with possible options: \napropos(\"contr\\\\.\")\n#&gt; [1] \"contr.helmert\"   \"contr.poly\"      \"contr.SAS\"       \"contr.sum\"      \n#&gt; [5] \"contr.treatment\"\n\nMany packages also have additional contrast functions.\n\n6.3.2 Recipes\nWe can also use a recipe to do this (with more functionality):\n\nind_rec &lt;- \n  recipe( ~ customer_type, data = customer_types) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  prep()\n\nbake(ind_rec, customer_types, starts_with(\"customer_type\"))\n#&gt; # A tibble: 4 × 3\n#&gt;   customer_type_group customer_type_transient customer_type_transient_party\n#&gt;                 &lt;dbl&gt;                   &lt;dbl&gt;                         &lt;dbl&gt;\n#&gt; 1                   0                       0                             0\n#&gt; 2                   1                       0                             0\n#&gt; 3                   0                       1                             0\n#&gt; 4                   0                       0                             1\nbake(ind_rec, with_missing, starts_with(\"customer_type\"))\n#&gt; Warning: ! There are new levels in `customer_type`: NA.\n#&gt; ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before `step_dummy()`\n#&gt;   to handle missing values.\n#&gt; # A tibble: 5 × 3\n#&gt;   customer_type_group customer_type_transient customer_type_transient_party\n#&gt;                 &lt;dbl&gt;                   &lt;dbl&gt;                         &lt;dbl&gt;\n#&gt; 1                   0                       0                             0\n#&gt; 2                   1                       0                             0\n#&gt; 3                   0                       1                             0\n#&gt; 4                   0                       0                             1\n#&gt; 5                  NA                      NA                            NA\n\nThere is no need to set the global option for na.action.\nAlso, the naming of features is more rational, with names and levels separated by an underscore. There is also an argument to step_dummy() that controls the naming of new features.\nThere is also an option to produce one-hot encodings called… one_hot.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-recipe-novel",
    "href": "chapters/categorical-predictors.html#sec-recipe-novel",
    "title": "6  Working with Categorical Predictors",
    "section": "\n6.4 Novel Categories",
    "text": "6.4 Novel Categories\nWhen we think the recipe or model will encounter new values of a factor predictor, we can use step_novel() to add a new factor level:\n\nrecipe(avg_price_per_room ~ customer_type, data = hotel_rate_train) %&gt;% \n  step_novel(customer_type) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL) %&gt;% \n  pluck(\"customer_type\") %&gt;% \n  levels()\n#&gt; [1] \"contract\"        \"group\"           \"transient\"       \"transient_party\"\n#&gt; [5] \"new\"\n\nFor the training set, this new level will never have any data associated with it.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-recipe-other",
    "href": "chapters/categorical-predictors.html#sec-recipe-other",
    "title": "6  Working with Categorical Predictors",
    "section": "\n6.5 “Othering”",
    "text": "6.5 “Othering”\nWe can also determine infrequently occurring categories (in the training set) and re-level the factor by converting them to an “other” category. If we chose a frequency of 0.01% as a cutoff, we have far fewer levels:\n\nlength(levels(hotel_rate_train$agent))\n#&gt; [1] 174\n\nrecipe(avg_price_per_room ~ agent, data = hotel_rate_train) %&gt;% \n  step_other(agent, threshold = 0.0001) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL) %&gt;% \n  pluck(\"agent\") %&gt;% \n  levels() %&gt;% \n  length()\n#&gt; [1] 101\n\nIf you are interested in which levels are combined, the tidy() method on the recipe can tell you.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-recipe-hashing",
    "href": "chapters/categorical-predictors.html#sec-recipe-hashing",
    "title": "6  Working with Categorical Predictors",
    "section": "\n6.6 Feature Hashing",
    "text": "6.6 Feature Hashing\nFeature hashing converts qualitative predictors to a set of binary indicators based solely on the value of their category. It is most useful when many categories and/or novel levels might be encountered.\nThe recipe step is in the textrecipes package, so we must load it first2. The main arguments are num_terms and signed. The first sets the number of features to create, and when signed = TRUE, the indicators will have an appropriate sign attached to them (i.e., their values could be -1/0/1).\n\nlibrary(textrecipes)\n\nrecipe(avg_price_per_room ~ agent, data = hotel_rate_train) %&gt;% \n  step_dummy_hash(agent, num_terms = 4) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL, contains(\"agent\")) \n#&gt; # A tibble: 11,551 × 4\n#&gt;   dummyhash_agent_1 dummyhash_agent_2 dummyhash_agent_3 dummyhash_agent_4\n#&gt;               &lt;int&gt;             &lt;int&gt;             &lt;int&gt;             &lt;int&gt;\n#&gt; 1                 0                 1                 0                 0\n#&gt; 2                 0                 0                -1                 0\n#&gt; 3                 0                 1                 0                 0\n#&gt; 4                 0                 1                 0                 0\n#&gt; 5                 0                 1                 0                 0\n#&gt; 6                 0                 0                 0                 1\n#&gt; # ℹ 11,545 more rows",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-recipe-effect-encode",
    "href": "chapters/categorical-predictors.html#sec-recipe-effect-encode",
    "title": "6  Working with Categorical Predictors",
    "section": "\n6.7 Effect Encodings",
    "text": "6.7 Effect Encodings\nEffect embedding is a supervised method to convert qualitative predictors to a numeric column that contains the effect of the category on the outcome. The outcome is a numeric value (the ADR) for these data. Using effect endings here will produce a column with specialized estimates of each category’s mean ADR. Let’s look at the agent predictor again.\nThe embed package has a few recipe steps to do this. This method is sometimes called “likelihood encoding” and the recipe steps all start with step_lencode_*:\n\n\nstep_lencode_glm() produces basic, naive estimates of effect. This is a “no pooling” estimate.\n\nstep_lencode_mixed() uses a non-Bayesian hierarchical model to produce regularized effect estimates.\n\nstep_lencode_bayes() uses a Bayesian model that is more flexible than its non-Bayesian sibling but can take much longer to fit.\n\nWe’ll use the “mixed” function3, For agent:\n\nlibrary(embed)\n\nencoded_agent_rec &lt;- \n  recipe(avg_price_per_room ~ agent, data = hotel_rate_train) %&gt;% \n  step_lencode_mixed(agent, outcome = vars(avg_price_per_room), id = \"effect\") %&gt;% \n  prep() \nencoded_agent_rec\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 1\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 11551 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Linear embedding for factors via mixed effects for: agent | Trained\n\nTo see the actual effect estimates, use the tidy() method:\n\ntidy(encoded_agent_rec, id = \"effect\")\n#&gt; # A tibble: 123 × 4\n#&gt;   level            value terms id    \n#&gt;   &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n#&gt; 1 aaron_marquez    92.22 agent effect\n#&gt; 2 alexander_drake 120.1  agent effect\n#&gt; 3 allen_her        73.70 agent effect\n#&gt; 4 anas_el_bashir   80.41 agent effect\n#&gt; 5 araseli_billy    66.91 agent effect\n#&gt; 6 arhab_al_islam   51.15 agent effect\n#&gt; # ℹ 117 more rows\n\n# The estimate for new agents:\ntidy(encoded_agent_rec, id = \"effect\") %&gt;% \n  slice_tail(n = 1)\n#&gt; # A tibble: 1 × 4\n#&gt;   level value terms id    \n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n#&gt; 1 ..new 75.36 agent effect\n\nWhen the recipe is applied to new data, the agent column is converted to a numeric column with these values:\n\nbake(encoded_agent_rec, hotel_rate_test) %&gt;% \n  bind_cols(hotel_rate_test %&gt;% select(original_col = agent))\n#&gt; # A tibble: 3,851 × 3\n#&gt;    agent avg_price_per_room original_col        \n#&gt;    &lt;dbl&gt;              &lt;dbl&gt; &lt;fct&gt;               \n#&gt; 1 106.9                 100 devin_rivera_borrego\n#&gt; 2 106.9                  60 devin_rivera_borrego\n#&gt; 3  76.38                 45 not_applicable      \n#&gt; 4 106.9                  82 devin_rivera_borrego\n#&gt; 5  76.38                 47 not_applicable      \n#&gt; 6  76.38                 60 not_applicable      \n#&gt; # ℹ 3,845 more rows\n\nFor categorical outcomes, the effect estimate is the log-odds of an event (the first factor level).",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-recipe-collapse",
    "href": "chapters/categorical-predictors.html#sec-recipe-collapse",
    "title": "6  Working with Categorical Predictors",
    "section": "\n6.8 Supervised Combining of Categories",
    "text": "6.8 Supervised Combining of Categories\nTo collapse a large number of factor levels to a smaller set using a supervised model, we can use step_collapse_cart() in the embed package.\nFor example:\n\n# Also needs the embed package loaded (and rpart installed)\nlibrary(embed)\n\ncollapse_agent_rec &lt;- \n  recipe(avg_price_per_room ~ agent, data = hotel_rate_train) %&gt;% \n  step_collapse_cart(agent, outcome = vars(avg_price_per_room), id = \"collapse\") %&gt;% \n  prep() \ncollapse_agent_rec\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 1\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 11551 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Collapsing factor levels using CART: agent | Trained\n\nThe step converts 122 unique values of agent in the training set to a smaller set of 12 categories. To see the conversion key, use the tidy() methods:\n\ntidy(collapse_agent_rec, id = \"collapse\")\n#&gt; # A tibble: 122 × 4\n#&gt;   terms old               new      id      \n#&gt;   &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 1 agent araseli_billy     agent_01 collapse\n#&gt; 2 agent arhab_al_islam    agent_01 collapse\n#&gt; 3 agent brayan_guerrero   agent_01 collapse\n#&gt; 4 agent daifallah_el_sami agent_01 collapse\n#&gt; 5 agent dante_merritt     agent_01 collapse\n#&gt; 6 agent derrick_barger    agent_01 collapse\n#&gt; # ℹ 116 more rows\n\ntidy(collapse_agent_rec, id = \"collapse\") %&gt;% \n  count(new)\n#&gt; # A tibble: 12 × 2\n#&gt;   new          n\n#&gt;   &lt;chr&gt;    &lt;int&gt;\n#&gt; 1 agent_01    17\n#&gt; 2 agent_02    14\n#&gt; 3 agent_03    14\n#&gt; 4 agent_04    10\n#&gt; 5 agent_05     7\n#&gt; 6 agent_06    13\n#&gt; # ℹ 6 more rows\n\nThere are two main tuning parameters (described later in section TODO):\n\ncost complexity (a.k.a. \\(C_p\\)): smaller values result in more groups. Values typically range between zero and 0.1.\nminimum n: the minimum number of rows in a group to enable it to keep splitting. Smaller values should result in more groupings.\n\nThese values can be tuned.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-recipe-ordinal",
    "href": "chapters/categorical-predictors.html#sec-recipe-ordinal",
    "title": "6  Working with Categorical Predictors",
    "section": "\n6.9 Encodings for Ordinal Predictors",
    "text": "6.9 Encodings for Ordinal Predictors\nAs reported in the section on ordinal data, the default for R is to encode ordinal values with \\(p\\) values is to create a set of \\(p - 1\\) orthogonal polynomial features. That is what step_dummy() does by default.\n\nquality_vals &lt;- c('excellent', 'fair', 'good', 'typical', 'poor')\nquality &lt;- tibble(quality = ordered(quality_vals, levels = quality_vals))\nstr(quality)\n#&gt; tibble [5 × 1] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ quality: Ord.factor w/ 5 levels \"excellent\"&lt;\"fair\"&lt;..: 1 2 3 4 5\n\nrecipe(~ quality, data = quality) %&gt;% \n  step_dummy(quality) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\n#&gt; # A tibble: 5 × 4\n#&gt;    quality_1 quality_2  quality_3 quality_4\n#&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 -6.325e- 1    0.5345 -3.162e- 1    0.1195\n#&gt; 2 -3.162e- 1   -0.2673  6.325e- 1   -0.4781\n#&gt; 3 -3.288e-17   -0.5345  9.637e-17    0.7171\n#&gt; 4  3.162e- 1   -0.2673 -6.325e- 1   -0.4781\n#&gt; 5  6.325e- 1    0.5345  3.162e- 1    0.1195\n\nWe can convert the ordered factor to an unordered factor:\n\nrecipe(~ quality, data = quality) %&gt;% \n  step_unorder(quality) %&gt;% \n  step_dummy(quality) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\n#&gt; # A tibble: 5 × 4\n#&gt;   quality_fair quality_good quality_typical quality_poor\n#&gt;          &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1            0            0               0            0\n#&gt; 2            1            0               0            0\n#&gt; 3            0            1               0            0\n#&gt; 4            0            0               1            0\n#&gt; 5            0            0               0            1\n\nAnother strategy is mapping the ordinal factor levels to a set of numeric values that make sense for their modeling problem. step_ordinalscore() can do that with a user-supplied conversion function:\n\nconvert_to_prime &lt;-  function(x) {\n  primes &lt;- c(2, 3, 7, 11, 13)\n  primes[as.numeric(x)]\n}\n\nrecipe(~ quality, data = quality) %&gt;% \n  step_ordinalscore(quality, convert = convert_to_prime) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\n#&gt; # A tibble: 5 × 1\n#&gt;   quality\n#&gt;     &lt;int&gt;\n#&gt; 1       2\n#&gt; 2       3\n#&gt; 3       7\n#&gt; 4      11\n#&gt; 5      13\n\nstep_integer() does the same for either type of factor but converts them to consecutive one-based integers.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-other-recipe-steps",
    "href": "chapters/categorical-predictors.html#sec-other-recipe-steps",
    "title": "6  Working with Categorical Predictors",
    "section": "\n6.10 Other Relevant Recipe Steps",
    "text": "6.10 Other Relevant Recipe Steps\nThere are a variety of other steps that can be used with qualitative predictors (a list of relevant recipe steps in recipes)",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#footnotes",
    "href": "chapters/categorical-predictors.html#footnotes",
    "title": "6  Working with Categorical Predictors",
    "section": "",
    "text": "In statistics, “contrasts” typically indicates a combination of parameters. For example, if you wanted to test that the average of two parameters was equal to a third, the contrast would be \\(\\beta_1 + \\beta_2 - 2\\beta_3\\) and the contrast coefficients would be c(1, 1, -2).↩︎\nIt also requires another package (text2vec) to be installed but not loaded.↩︎\nFor this function, we all need the lme4 package to be installed.↩︎",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html",
    "href": "chapters/embeddings.html",
    "title": "7  Embeddings",
    "section": "",
    "text": "7.1 Requirements\nThe corresponding chapter on the main site focuses on finding ways to combine or distill a set of features into a smaller set that captures important information. Like the previous chapters, this. one will also focus on the recipes package.\nYou’ll need 13 packages (bestNormalize, dimRed, embed, fastICA, igraph, mixOmics, modeldatatoo, patchwork, RANN, RSpectra, tidymodels, uwot, viridis) for this chapter. The mixOmics is a Bioconductor package and is not on CRAN. For the others, we can install them as usual but we’ll get mixOmics from GitHub:\nreq_pkg &lt;- c(\"bestNormalize\", \"dimRed\", \"embed\", \"fastICA\", \"igraph\", \n             \"mixOmics\", \"modeldatatoo\", \"patchwork\", \"RANN\", \"RSpectra\", \n             \"tidymodels\", \"uwot\", \"viridis\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  \n  # mixOmics is not on CRAN\n  cran_install_list &lt;- install_list[install_list != \"mixOmics\"]\n  if ( length(cran_install_list) &gt; 0 ) {\n    pak::pak(cran_install_list)\n  }\n  \n  # Get mixOmics from github\n  if ( \"mixOmics\" %in% install_list ) {\n    pak::pak(\"mixOmicsTeam/mixOmics\")\n  }\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\nlibrary(viridis)\nlibrary(embed) # for umap\nlibrary(patchwork)\n\ntidymodels_prefer()\ntheme_set(theme_bw())",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-barley",
    "href": "chapters/embeddings.html#sec-barley",
    "title": "7  Embeddings",
    "section": "\n7.2 Example: Predicting Barley Amounts",
    "text": "7.2 Example: Predicting Barley Amounts\nThe data are in the modeldatatoo package. Let’s load the data, remove two outcome columns that will not be analyzed here, and conduct a three-way split of the data:\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/setup_chemometrics.R\")\n\nThe column names for the predictors are wvlgth_001 through wvlgth_550.\nThe primary recipe used for almost all of the embedding methods is:\n\nlibrary(bestNormalize) # for ORD transformation\n#&gt; Registered S3 method overwritten by 'butcher':\n#&gt;   method                 from    \n#&gt;   as.character.dev_topic generics\n\nbarley_rec &lt;-\n  recipe(barley ~ ., data = barley_train) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;%\n  # Pre-compute to save time later\n  prep()\n\nbarley_rec\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:     1\n#&gt; predictor: 550\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 4839 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • orderNorm transformation on: wvlgth_001, wvlgth_002, wvlgth_003, ... | Trained\n\nIf you use a recipe, most of the embedding methods can be computed with a common interface. The recipe step functions are mostly in the recipes package, although some live in “side packages,” such as the embed package. We’ll be clear about which package is needed for each.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-linear-embed",
    "href": "chapters/embeddings.html#sec-linear-embed",
    "title": "7  Embeddings",
    "section": "\n7.3 Linear Transformations",
    "text": "7.3 Linear Transformations\nWe’ll look at the three linear methods described in the text.\n\n7.3.1 Principal Component Analysis\nUnsurprisingly, the recipe step needed here is called step_pca(). We’ll add an id argument to more easily reference the step of interest.\n\nbarley_pca_rec &lt;-\n  barley_rec %&gt;%\n  step_pca(all_numeric_predictors(), num_comp = 2, id = \"pca\") %&gt;% \n  prep()\n\nbarley_pca_rec\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:     1\n#&gt; predictor: 550\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 4839 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • orderNorm transformation on: wvlgth_001, wvlgth_002, wvlgth_003, ... | Trained\n#&gt; • PCA extraction with: wvlgth_001, wvlgth_002, wvlgth_003, ... | Trained\n\nTo further investigate the results, the tidy() method can extract elements of the computations. For example, you can return how variance each component captures using the argument type = \"variance\". Note that when the PCA recipe step was added, we used the option id = \"pca\". This is not required, but it makes it easier to specify what step the tidy() method should consider:\n\npca_scree &lt;- tidy(barley_pca_rec, id = \"pca\", type = \"variance\")\npca_scree\n#&gt; # A tibble: 2,200 × 4\n#&gt;   terms       value component id   \n#&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;\n#&gt; 1 variance 507.5            1 pca  \n#&gt; 2 variance  35.84           2 pca  \n#&gt; 3 variance   3.395          3 pca  \n#&gt; 4 variance   1.511          4 pca  \n#&gt; 5 variance   0.6940         5 pca  \n#&gt; 6 variance   0.4265         6 pca  \n#&gt; # ℹ 2,194 more rows\n\npca_scree %&gt;% count(terms)\n#&gt; # A tibble: 4 × 2\n#&gt;   terms                           n\n#&gt;   &lt;chr&gt;                       &lt;int&gt;\n#&gt; 1 cumulative percent variance   550\n#&gt; 2 cumulative variance           550\n#&gt; 3 percent variance              550\n#&gt; 4 variance                      550\n\nNote that there are 550 entries for each since there are 550 predictor columns.\nThe default option for the tidy() method with PCA is to return the estimated loadings. This can help untangle which predictors influence the PCA components the most (or least).\n\npca_loadings &lt;- tidy(barley_pca_rec, id = \"pca\")\npca_loadings\n#&gt; # A tibble: 302,500 × 4\n#&gt;   terms         value component id   \n#&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;\n#&gt; 1 wvlgth_001 -0.01696 PC1       pca  \n#&gt; 2 wvlgth_002 -0.01704 PC1       pca  \n#&gt; 3 wvlgth_003 -0.01713 PC1       pca  \n#&gt; 4 wvlgth_004 -0.01723 PC1       pca  \n#&gt; 5 wvlgth_005 -0.01734 PC1       pca  \n#&gt; 6 wvlgth_006 -0.01748 PC1       pca  \n#&gt; # ℹ 302,494 more rows\n\nThere are 550^2 = 302500 possible loadings.\nTo get the component values for new data, such as the validation set, the bake() method can be used. Using new_data = NULL returns the training set points:\n\nbarley_pca_rec %&gt;% \n  bake(new_data = NULL, starts_with(\"PC\"))\n#&gt; # A tibble: 4,839 × 2\n#&gt;       PC1   PC2\n#&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2.546  6.650\n#&gt; 2  0.1477 6.821\n#&gt; 3 -3.638  5.551\n#&gt; 4 -5.344  5.264\n#&gt; 5 -5.064  4.263\n#&gt; 6  8.857  8.426\n#&gt; # ℹ 4,833 more rows\n\nSince we used num_comp = 2, two new features were generated.\nWe can also pass new data in, such as the validation set:\n\npca_score_plot &lt;- \n  barley_pca_rec %&gt;% \n  bake(new_data = barley_val) %&gt;% \n  ggplot(aes(PC1, PC2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\")\n\npca_score_plot\n\n\n\n\n\n\n\nNote the difference in the axis ranges. If we are considering how much the PCA components explain the original predictors (i.e., not the outcome), it can be very helpful to keep the axis scales common:\n\npca_score_plot + coord_obs_pred()\n\n\n\n\n\n\n\nThis helps avoid over-interpreting proportionally small patterns in the later components.\n\n\n\n\n\n\nNote\n\n\n\nAs mentioned in the main text, PCA (and PLS) components are unique up to their sign. This means that the embedded features have the same shape, but their values may be flipped in the North/South and/or East/West directions.\n\n\nThe functions embed::step_pca_sparse() and embed::step_pca_sparse_bayes() have sparse/regularized estimation methods for PCA. Each has an argument called predictor_prop() that attempts to control how much sparsity should be used. predictor_prop = 0 should approximate regular PCA, and values near 1.0 would produce very few non-zero loadings.\n\n\n7.3.2 Independent Component Analysis\nAn ICA recipe step can also be found in the recipes package. The syntax is virtually identical:\n\nset.seed(538)\nbarley_ica_rec &lt;-\n  recipe(barley ~ ., data = barley_train) %&gt;% \n  step_ica(all_numeric_predictors(), num_comp = 2, id = \"ica\") %&gt;% \n  prep()\n\nSimilarly, the tidy() method returns the ICA loadings:\n\ntidy(barley_ica_rec, id = \"ica\")\n#&gt; # A tibble: 1,100 × 4\n#&gt;   terms      component    value id   \n#&gt;   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1 wvlgth_001 IC1       -0.02197 ica  \n#&gt; 2 wvlgth_001 IC2        0.9340  ica  \n#&gt; 3 wvlgth_002 IC1       -0.02196 ica  \n#&gt; 4 wvlgth_002 IC2        0.9307  ica  \n#&gt; 5 wvlgth_003 IC1       -0.02196 ica  \n#&gt; 6 wvlgth_003 IC2        0.9270  ica  \n#&gt; # ℹ 1,094 more rows\n\nMost other dimension reduction techniques (but not PCA and PLS) depend on random numbers. We’ll set them when needed, but it is worth pointing out that you will likely get different results each time you run them.\nFor example, when two ICA components are used, the results are not the same but close when using a different random number seed.\n\nset.seed(955)\nica_redo &lt;- \n  recipe(barley ~ ., data = barley_train) %&gt;% \n  step_ica(all_numeric_predictors(), num_comp = 2, id = \"ica\") %&gt;% \n  prep()\n\nica_redo %&gt;% tidy(id = \"ica\")\n#&gt; # A tibble: 1,100 × 4\n#&gt;   terms      component    value id   \n#&gt;   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1 wvlgth_001 IC1        0.9341  ica  \n#&gt; 2 wvlgth_001 IC2       -0.01989 ica  \n#&gt; 3 wvlgth_002 IC1        0.9307  ica  \n#&gt; 4 wvlgth_002 IC2       -0.01989 ica  \n#&gt; 5 wvlgth_003 IC1        0.9270  ica  \n#&gt; 6 wvlgth_003 IC2       -0.01990 ica  \n#&gt; # ℹ 1,094 more rows\n\nThe individual loading values are different between runs, and components one and two are swapped between invocations with different seeds:\n\nica_1 &lt;- \n  barley_ica_rec %&gt;% \n  bake(new_data = barley_val) %&gt;% \n  ggplot(aes(IC1, IC2, col = barley)) + \n  geom_point(alpha = 1 / 4, show.legend = FALSE) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred() +\n  labs(title = \"seed = 538\")\n\nica_2 &lt;- \n  ica_redo %&gt;% \n  bake(new_data = barley_val) %&gt;% \n  ggplot(aes(IC1, IC2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred() +\n  labs(title = \"seed = 955\")\n\nica_1 + ica_2\n\n\n\n\n\n\n\nThis might not cause a difference in performance when the features are used in a predictive model, but if the model uses slopes and intercepts, the parameter estimates will be different each time it is run.\n\n\n7.3.3 Partial Least Squares\nThe syntax for PLS is also very similar. However, it is a supervised method, so we need to specify the column containing the outcome (the outcome column is not needed after model training). The code below uses dplyr::vars() to declare the column name, but a simple character string can also be used.\n\nbarley_pls_rec &lt;-\n  barley_rec %&gt;%\n  step_pls(all_numeric_predictors(), outcome = vars(barley), num_comp = 2,\n           id = \"pls\") %&gt;% \n  prep()\n\n# Loadings: \ntidy(barley_pls_rec, id = \"pls\")\n#&gt; # A tibble: 1,100 × 4\n#&gt;   terms         value component id   \n#&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;\n#&gt; 1 wvlgth_001 -0.05632 PLS1      pls  \n#&gt; 2 wvlgth_001 -0.1572  PLS2      pls  \n#&gt; 3 wvlgth_002 -0.05637 PLS1      pls  \n#&gt; 4 wvlgth_002 -0.1571  PLS2      pls  \n#&gt; 5 wvlgth_003 -0.05642 PLS1      pls  \n#&gt; 6 wvlgth_003 -0.1570  PLS2      pls  \n#&gt; # ℹ 1,094 more rows",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-mds",
    "href": "chapters/embeddings.html#sec-mds",
    "title": "7  Embeddings",
    "section": "\n7.4 Multidimensional Scaling",
    "text": "7.4 Multidimensional Scaling\ntidymodels contains recipe steps for Isomap and UMAP. The latter is accessible via the embed package.\n\n7.4.1 Isomap\nAgain, the syntax is very similar to the previous unsupervised methods. The main two tuning parameters are num_terms and neighbors. We should also set the seed before execution.\n\nset.seed(221)\nbarley_isomap_rec &lt;-\n  barley_rec %&gt;%\n  step_isomap(all_numeric_predictors(), neighbors = 10, num_terms = 2) %&gt;% \n  prep()\n\nWe can project this preprocessing model onto new data:\n\nbarley_isomap_rec %&gt;% \n  bake(new_data = barley_val) %&gt;% \n  ggplot(aes(Isomap1, Isomap2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred()\n\n\n\n\n\n\n\n\n\n7.4.2 UMAP\nstep_umap(), in the embed package, has a number of tuning parameters: neighbors, num_comp, min_dist, learn_rate, epochs, initial (initialization method, e.g. “pca”), and the optional target_weight.\nFor an unsupervised embedding:\n\nset.seed(724)\nbarley_umap_rec &lt;-\n  barley_rec %&gt;%\n  step_umap(all_numeric_predictors(), neighbors = 10, num_comp = 2) %&gt;% \n  prep()\n\nProjection on new data has the same syntax:\n\nbarley_umap_rec %&gt;% \n  bake(new_data = barley_val) %&gt;% \n  ggplot(aes(UMAP1, UMAP2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred()\n\n\n\n\n\n\n\nFor a supervised embedding, the target_weight argument is used. A value of zero is unsupervised, and values near 1.0 are completely supervised. As with PLS, the argument for the outcome column is called outcome and can be a string of an unquoted name wrapped in vars().",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-centroids",
    "href": "chapters/embeddings.html#sec-centroids",
    "title": "7  Embeddings",
    "section": "\n7.5 Centroid-Based Methods",
    "text": "7.5 Centroid-Based Methods\nThere are two steps in recipes for this:\n\n\nstep_classdist(): basic “distance to centroid” calculations and,\n\nstep_classdist_shrunken(): nearest shrunken centroids\n\nThese steps are for classification data, so we’ll use some example data from the modeldata package:\n\ntwo_class_dat %&gt;% \n  ggplot(aes(A, B, col = Class)) + \n  geom_point(alpha = 1 / 2) +\n  coord_obs_pred()\n\n\n\n\n\n\n\nHere’s an example of creating a recipe with the basic class distance computations:\n\ncentroid_rec &lt;-\n  recipe(Class ~ ., data = two_class_dat) %&gt;%\n  step_classdist(all_numeric_predictors(), class = \"Class\") %&gt;% \n  prep()\n\nThe outcome argument is called \"class\" and takes a string value for the column name.\nThe processed data has a default naming convention of \"classdist_{class level}\" and you get one column per class:\n\nbake(centroid_rec, new_data = NULL)\n#&gt; # A tibble: 791 × 5\n#&gt;       A     B Class  classdist_Class1 classdist_Class2\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;             &lt;dbl&gt;            &lt;dbl&gt;\n#&gt; 1 2.070 1.632 Class1         -0.05795          -0.5526\n#&gt; 2 2.016 1.037 Class1         -1.026             1.647 \n#&gt; 3 1.689 1.367 Class2         -0.8454            0.2437\n#&gt; 4 3.435 1.980 Class2          1.367             1.678 \n#&gt; 5 2.885 1.976 Class1          0.9208            0.3913\n#&gt; 6 3.314 2.406 Class2          1.708             0.4739\n#&gt; # ℹ 785 more rows\n\nThe shrunken version of this step has an additional argument that is the fraction of the complete solutions. The argument name is threshold:\n\ncentroid_shrunk_rec &lt;-\n  recipe(Class ~ ., data = two_class_dat) %&gt;%\n  step_classdist_shrunken(all_numeric_predictors(), threshold = 1 / 6, class = \"Class\") %&gt;% \n  prep()",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html",
    "href": "chapters/interactions-nonlinear.html",
    "title": "8  Interactions and Nonlinear Features",
    "section": "",
    "text": "8.1 Requirements\nThis chapter is focused on how predictors can enter the model in a nonlinear fashion. The three approaches discussed are interaction effects, basis expansions, and discretization.\nYou’ll need 5 packages (aorsf, embed, gt, hstats, and tidymodels) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"aorsf\", \"embed\", \"gt\", \"hstats\", \"tidymodels\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\nlibrary(embed)\ntidymodels_prefer()\ntheme_set(theme_bw())",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-interactions",
    "href": "chapters/interactions-nonlinear.html#sec-interactions",
    "title": "8  Interactions and Nonlinear Features",
    "section": "\n8.2 Interactions",
    "text": "8.2 Interactions\nAs the text mentions, interactions involve two or more predictors (of any data type). An interaction means that the relationship between the outcome and the predictors involved cannot be articulated by looking at one predictor at a time; they act in concert.\nTo get started, let’s once again load the food delivery data and use the same split:\n\ndata(deliveries, package = \"modeldata\")\n\nset.seed(991)\ndelivery_split &lt;- initial_validation_split(deliveries, prop = c(0.6, 0.2), strata = time_to_delivery)\ndelivery_train &lt;- training(delivery_split)\ndelivery_test  &lt;- testing(delivery_split)\ndelivery_val   &lt;- validation(delivery_split)\ndelivery_rs    &lt;- validation_set(delivery_split)\n\n## Some functions to faciltate the content\n\ndec_to_time &lt;- function(x) {\n  mins &lt;- floor(x)\n  dec &lt;- floor((x - mins) * 60)\n  res &lt;- cli::pluralize(\"{mins} minutes{?s} and {dec} second{?s}\")\n  res &lt;- as.character(res)\n  res &lt;- gsub(\" and 0 seconds\", \"\", res)\n  res &lt;- gsub(\"1 minutes\", \"1 minute\", res) \n  res\n}\n\ndec_to_time_rs &lt;- function(x) {\n  collect_metrics(x) %&gt;% \n    filter(.metric == \"mae\") %&gt;% \n    pluck(\"mean\") %&gt;% \n    dec_to_time\n}\n\nWe’ll consider two mechanisms to encode interaction columns: via the base R formula method and with recipes.\n\n8.2.1 Interactions with Model Formulas\nThis was briefly discussed in Section 5.3.\nThe main operator that creates interactions is the colon. Using a:b within a formula will create the appropriate columns that encode the interactions. The specifics depend on what type of data are in columns a and b:\n\nIf both are numeric, an additional column, which is their product, is added to the model matrix. By default, the base R formula method gives it the name \"a:b\".\nIf one is numeric and the other is categorical, R first converts the categorical (i.e., factor) column into binary indicator variables, then makes columns that are the produce of each indicator column and the original numeric column.\nIf both are categorical, indicators are made for both and then their corresponding product pairs are created (binary times binary columns).\n\nLet’s make a small example to demonstrate:\n\nlibrary(gt)\n\ninteraction_example &lt;- \n  delivery_test %&gt;% \n  slice(1, .by = day) %&gt;% \n  select(day, hour, distance) %&gt;% \n  arrange(day)\n\ninteraction_example %&gt;% gt()\n\n\n\n\n\nday\nhour\ndistance\n\n\n\nMon\n19.38\n2.59\n\n\nTue\n12.14\n2.40\n\n\nWed\n18.03\n2.96\n\n\nThu\n16.22\n3.25\n\n\nFri\n12.88\n2.88\n\n\nSat\n12.94\n2.47\n\n\nSun\n12.96\n3.80\n\n\n\n\n\n\nFor two numeric predictors:\n\nmodel.matrix(~ hour + distance + hour:distance, interaction_example) %&gt;% \n  as_tibble() %&gt;% \n  select(-`(Intercept)`) %&gt;% \n  gt()\n\n\n\n\n\nhour\ndistance\nhour:distance\n\n\n\n19.38\n2.59\n50.19\n\n\n12.14\n2.40\n29.14\n\n\n18.03\n2.96\n53.36\n\n\n16.22\n3.25\n52.71\n\n\n12.88\n2.88\n37.09\n\n\n12.94\n2.47\n31.97\n\n\n12.96\n3.80\n49.26\n\n\n\n\n\n\nOne numeric and one factor predictor:\n\nmodel.matrix(~ day + distance + day:distance, interaction_example) %&gt;% \n  as_tibble() %&gt;% \n  select(-`(Intercept)`) %&gt;% \n  gt() |&gt; \n  tab_style_body(style = cell_text(color = \"gray70\"), values = 0)\n\n\n\n\n\ndayTue\ndayWed\ndayThu\ndayFri\ndaySat\ndaySun\ndistance\ndayTue:distance\ndayWed:distance\ndayThu:distance\ndayFri:distance\ndaySat:distance\ndaySun:distance\n\n\n\n0\n0\n0\n0\n0\n0\n2.59\n0.0\n0.00\n0.00\n0.00\n0.00\n0.0\n\n\n1\n0\n0\n0\n0\n0\n2.40\n2.4\n0.00\n0.00\n0.00\n0.00\n0.0\n\n\n0\n1\n0\n0\n0\n0\n2.96\n0.0\n2.96\n0.00\n0.00\n0.00\n0.0\n\n\n0\n0\n1\n0\n0\n0\n3.25\n0.0\n0.00\n3.25\n0.00\n0.00\n0.0\n\n\n0\n0\n0\n1\n0\n0\n2.88\n0.0\n0.00\n0.00\n2.88\n0.00\n0.0\n\n\n0\n0\n0\n0\n1\n0\n2.47\n0.0\n0.00\n0.00\n0.00\n2.47\n0.0\n\n\n0\n0\n0\n0\n0\n1\n3.80\n0.0\n0.00\n0.00\n0.00\n0.00\n3.8\n\n\n\n\n\n\nIf you want to make all possible interactions, you can use the dot and exponent operator:\n\n# All possible two-way interactions:\nmodel.matrix(~ (.)^2, interaction_example)\n\n# All possible two- and three-way interactions, etc:\nmodel.matrix(~ (.)^3, interaction_example)\n\n\n8.2.2 Interactions from Recipes\nThe recipes has step_interact(). This step is very atypical since it uses a formula to specify the inputs (rather than a set of dplyr selectors). It also requires all columns used to be already converted to indicators (perhaps using step_dummy()).\nThe formula passed to step_interact() also uses colons to declare interactions, but it has two special differences from base R formulas:\n\nyou can use dplyr selectors to select the columns to interact with and\nthe resulting interaction columns use _x_ as the default seperator in the names.\n\nFor continuous/continuous interactions:\n\nrecipe(~ hour + distance, data = interaction_example) %&gt;% \n  step_interact(~ hour:distance) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL) %&gt;% \n  gt()\n\n\n\n\n\nhour\ndistance\nhour_x_distance\n\n\n\n19.38\n2.59\n50.19\n\n\n12.14\n2.40\n29.14\n\n\n18.03\n2.96\n53.36\n\n\n16.22\n3.25\n52.71\n\n\n12.88\n2.88\n37.09\n\n\n12.94\n2.47\n31.97\n\n\n12.96\n3.80\n49.26\n\n\n\n\n\n\nFor categorical/continuous combinations, we use step_dummy() first and then a selector to make the interactions\n\nrecipe(~ day + hour, data = interaction_example) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  step_interact(~ starts_with(\"day_\"):hour) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL) %&gt;% \n  gt() |&gt; \n  tab_style_body(style = cell_text(color = \"gray70\"), values = 0)\n\n\n\n\n\nhour\nday_Tue\nday_Wed\nday_Thu\nday_Fri\nday_Sat\nday_Sun\nday_Tue_x_hour\nday_Wed_x_hour\nday_Thu_x_hour\nday_Fri_x_hour\nday_Sat_x_hour\nday_Sun_x_hour\n\n\n\n19.38\n0\n0\n0\n0\n0\n0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n12.14\n1\n0\n0\n0\n0\n0\n12.14\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n18.03\n0\n1\n0\n0\n0\n0\n0.00\n18.03\n0.00\n0.00\n0.00\n0.00\n\n\n16.22\n0\n0\n1\n0\n0\n0\n0.00\n0.00\n16.22\n0.00\n0.00\n0.00\n\n\n12.88\n0\n0\n0\n1\n0\n0\n0.00\n0.00\n0.00\n12.88\n0.00\n0.00\n\n\n12.94\n0\n0\n0\n0\n1\n0\n0.00\n0.00\n0.00\n0.00\n12.94\n0.00\n\n\n12.96\n0\n0\n0\n0\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n12.96\n\n\n\n\n\n\n\n\n8.2.3 Detecting Interactions\nA few different packages can compute \\(H\\)-statistics (and similar quantities):\n\nhstats\n\npre, specifically the interact() and bsnullinteract() functions\n\nbartMachine has interaction_investigator()\n\n\naorsf has orsf_vint()\n\n\nand so on. We’ll focus on the first since the other functions are tied to specific models.\nFor this chapter, we fit an oblique random forest model to compute the \\(H\\)-statistics. We’ll defer the details of that model fit until a later chapter but the code was:\n\nlibrary(aorsf)\n\np &lt;- ncol(delivery_train) - 1\n\nset.seed(1)\norf_fit &lt;- orsf(time_to_delivery ~ ., data = delivery_train, mtry = p, n_tree = 50)\n\nAny ML model can be used with the hstats package. We’ll use the validation set to compute the values; this lends more validity to the values (although we could have used the training set). There are a lot of computations here; this may take a bit to compute:\n\nlibrary(hstats)\n\nset.seed(218)\norf_hstats &lt;-\n  hstats(orf_fit,\n         X = delivery_val %&gt;% dplyr::select(-time_to_delivery),\n         # Prioritize the top 10 individual predictors for computing potential\n         # pairwise interactions.\n         pairwise_m = 10,\n         # We can run a little faster by using quantiles to approximate the \n         # predictor distributions. \n         approx = TRUE,\n         # How many random data points are used for the computations.\n         n_max = 1000,\n         verbose = FALSE)\n\norf_two_way_int_obj &lt;- h2_pairwise(orf_hstats, zero = TRUE)\norf_two_way_int_obj\n#&gt; Pairwise H^2 (normalized)\n#&gt;                       [,1]\n#&gt; hour:day         6.639e-02\n#&gt; item_01:item_10  2.627e-02\n#&gt; day:distance     1.540e-02\n#&gt; hour:distance    4.622e-03\n#&gt; item_10:item_24  4.023e-03\n#&gt; day:item_10      3.999e-03\n#&gt; day:item_24      1.885e-03\n#&gt; day:item_01      1.734e-03\n#&gt; item_07:item_24  1.214e-03\n#&gt; item_01:item_24  1.123e-03\n#&gt; distance:item_24 1.052e-03\n#&gt; item_01:item_07  1.037e-03\n#&gt; item_08:item_10  9.799e-04\n#&gt; item_02:item_03  8.466e-04\n#&gt; distance:item_10 7.546e-04\n#&gt; day:item_02      7.292e-04\n#&gt; item_03:item_07  6.366e-04\n#&gt; day:item_03      5.975e-04\n#&gt; item_01:item_08  5.908e-04\n#&gt; day:item_07      5.823e-04\n#&gt; item_08:item_24  5.744e-04\n#&gt; item_02:item_24  5.673e-04\n#&gt; distance:item_02 5.592e-04\n#&gt; distance:item_03 5.282e-04\n#&gt; item_07:item_10  5.219e-04\n#&gt; distance:item_07 4.920e-04\n#&gt; distance:item_01 4.794e-04\n#&gt; item_03:item_24  4.073e-04\n#&gt; item_03:item_10  3.571e-04\n#&gt; hour:item_10     3.524e-04\n#&gt; hour:item_01     2.630e-04\n#&gt; hour:item_24     2.169e-04\n#&gt; item_07:item_08  1.976e-04\n#&gt; day:item_08      1.834e-04\n#&gt; hour:item_08     1.758e-04\n#&gt; item_02:item_10  1.753e-04\n#&gt; distance:item_08 1.694e-04\n#&gt; hour:item_02     1.512e-04\n#&gt; item_03:item_08  1.490e-04\n#&gt; hour:item_03     1.383e-04\n#&gt; item_02:item_07  1.368e-04\n#&gt; item_01:item_03  9.934e-05\n#&gt; item_01:item_02  8.855e-05\n#&gt; item_02:item_08  4.405e-05\n#&gt; hour:item_07     3.428e-05\n\nFrom these results, we could add more terms by adding another layer of step_interact() to our recipe.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-polynomials",
    "href": "chapters/interactions-nonlinear.html#sec-polynomials",
    "title": "8  Interactions and Nonlinear Features",
    "section": "\n8.3 Polynomial Basis Expansions",
    "text": "8.3 Polynomial Basis Expansions\nThere are two main functions that produce different types of orthogonal polynomials:\n\n\nstep_poly() wrapping stats::poly()\n\n\nstep_poly_bernstein() wrapping splines2::bernsteinPoly().\n\nWe can pass the columns of interest to each step:\n\nrecipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_poly(hour, degree = 4) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL, starts_with(\"hour\"))\n#&gt; # A tibble: 6,004 × 4\n#&gt;   hour_poly_1 hour_poly_2 hour_poly_3 hour_poly_4\n#&gt;         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1   -0.02318    0.02240     -0.01758     0.008000\n#&gt; 2    0.01558    0.01027     -0.003123   -0.01343 \n#&gt; 3    0.01105   -0.0003742   -0.01174    -0.01039 \n#&gt; 4   -0.002365  -0.01319      0.004819    0.01182 \n#&gt; 5   -0.01761    0.006275     0.009360   -0.01725 \n#&gt; 6   -0.02321    0.02249     -0.01777     0.008254\n#&gt; # ℹ 5,998 more rows\n\n# or\n\nrecipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_poly_bernstein(hour, degree = 4) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL, starts_with(\"hour\"))\n#&gt; # A tibble: 6,004 × 4\n#&gt;    hour_1  hour_2   hour_3     hour_4\n#&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 0.2593  0.03588 0.002207 0.00005090\n#&gt; 2 0.01665 0.1209  0.3899   0.4717    \n#&gt; 3 0.05106 0.2201  0.4216   0.3028    \n#&gt; 4 0.2658  0.3742  0.2342   0.05495   \n#&gt; 5 0.4047  0.1437  0.02267  0.001341  \n#&gt; 6 0.2582  0.03549 0.002169 0.00004969\n#&gt; # ℹ 5,998 more rows\n\nIf we would like different polynomial degrees for different columns, we would call the step multiple times.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-splines",
    "href": "chapters/interactions-nonlinear.html#sec-splines",
    "title": "8  Interactions and Nonlinear Features",
    "section": "\n8.4 Spline Functions",
    "text": "8.4 Spline Functions\nrecipes has several spline steps:\n\napropos(\"step_spline\")\n#&gt; [1] \"step_spline_b\"           \"step_spline_convex\"      \"step_spline_monotone\"   \n#&gt; [4] \"step_spline_natural\"     \"step_spline_nonnegative\"\n\nThe syntax is almost identical to the polynomial steps. For example, for natural splines:\n\nrecipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_spline_natural(hour, deg_free = 4) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL, starts_with(\"hour\"))\n#&gt; # A tibble: 6,004 × 4\n#&gt;   hour_1   hour_2  hour_3  hour_4\n#&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 0.1970 0.004302 0       0      \n#&gt; 2 0      0.06326  0.4307  0.3122 \n#&gt; 3 0      0.2167   0.4816  0.2611 \n#&gt; 4 0.2030 0.6655   0.03618 0.01668\n#&gt; 5 0.3939 0.05003  0       0      \n#&gt; 6 0.1959 0.004225 0       0      \n#&gt; # ℹ 5,998 more rows\n\nSome steps also have arguments for the spline degree called degree. Also, most basis expansion function steps have an option called options to pass additional specifications, such as a vector of knots.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#discretization",
    "href": "chapters/interactions-nonlinear.html#discretization",
    "title": "8  Interactions and Nonlinear Features",
    "section": "\n8.5 Discretization",
    "text": "8.5 Discretization\nThere is one step in recipes that conducts unsupervised binning called step_discretize() and the embed package has two for supervised methods: step_discretize_cart() and step_discretize_xgb().\nFor unsupervised binning:\n\nbasic_binning &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_discretize(hour, num_breaks = 4) %&gt;% \n  prep()\n\nbake(basic_binning, new_data = NULL, hour)\n#&gt; # A tibble: 6,004 × 1\n#&gt;   hour \n#&gt;   &lt;fct&gt;\n#&gt; 1 bin1 \n#&gt; 2 bin4 \n#&gt; 3 bin4 \n#&gt; 4 bin2 \n#&gt; 5 bin1 \n#&gt; 6 bin1 \n#&gt; # ℹ 5,998 more rows\n\nThe hour column has been converted to a factor. There is also an argument called min_unique that defines the “line of dignity” for the binning. If this value is less than the number of data points per bin that would occur for the value of num_breaks, binning is not used.\nTo understand where the breakpoints reside, the tidy() method will show them:\n\ntidy(basic_binning, number = 1)\n#&gt; # A tibble: 5 × 3\n#&gt;   terms   value id              \n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;           \n#&gt; 1 hour  -Inf    discretize_eoInz\n#&gt; 2 hour    14.48 discretize_eoInz\n#&gt; 3 hour    16.57 discretize_eoInz\n#&gt; 4 hour    18.21 discretize_eoInz\n#&gt; 5 hour   Inf    discretize_eoInz\n\n# note: \nquantile(delivery_train$hour, probs = (0:4) / 4)\n#&gt;    0%   25%   50%   75%  100% \n#&gt; 11.07 14.48 16.57 18.21 20.92\n\nThe use of infinite bounds allows data falling outside of the range of the training set to be processed.\nIf you want to use custom breakpoints, recipes::step_cut() is also available.\nFor supervised methods, the syntax is different in two ways. First, the outcome column is specified using the outcome argument. Secondly, additional parameters control the complexity of the results (i.e., the number of bins). These depend on the binning models. for CART binning, the arguments are cost_complexity, tree_depth, and min_n. They are different for the step that uses boosted trees.\nThe format of the results is very similar though:\n\nlibrary(embed)\ncart_binning &lt;-\n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;%\n  step_discretize_cart(hour,\n                       outcome = vars(time_to_delivery),\n                       cost_complexity = 0.001) %&gt;%\n  prep()\n\nbake(cart_binning, new_data = NULL, hour)\n#&gt; # A tibble: 6,004 × 1\n#&gt;   hour         \n#&gt;   &lt;fct&gt;        \n#&gt; 1 [-Inf,12.28) \n#&gt; 2 [18.97,19.68)\n#&gt; 3 [16.63,18.97)\n#&gt; 4 [15.73,16.24)\n#&gt; 5 [12.28,13.23)\n#&gt; 6 [-Inf,12.28) \n#&gt; # ℹ 5,998 more rows\n\ntidy(cart_binning, number = 1)\n#&gt; # A tibble: 11 × 3\n#&gt;   terms value id                   \n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1 hour  12.28 discretize_cart_ChWJu\n#&gt; 2 hour  13.23 discretize_cart_ChWJu\n#&gt; 3 hour  13.96 discretize_cart_ChWJu\n#&gt; 4 hour  14.77 discretize_cart_ChWJu\n#&gt; 5 hour  15.73 discretize_cart_ChWJu\n#&gt; 6 hour  16.24 discretize_cart_ChWJu\n#&gt; # ℹ 5 more rows\n\nAs you can see, the breakpoints are built into the factor levels.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html",
    "href": "chapters/overfitting.html",
    "title": "9  Overfitting",
    "section": "",
    "text": "9.1 Requirements\nSince overfitting is often caused by poor values of tuning parameters, we’ll focus on how to work with these values.\nYou’ll need 2 packages (bestNormalize and tidymodels) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"bestNormalize\", \"tidymodels\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\nlibrary(bestNormalize)\ntidymodels_prefer()",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-tuning-parameters",
    "href": "chapters/overfitting.html#sec-tuning-parameters",
    "title": "9  Overfitting",
    "section": "\n9.2 Tuning Paramters",
    "text": "9.2 Tuning Paramters\nThere are currently two main components in a model pipeline:\n\na preprocessing method\na supervised model fit.\n\nIn tidymodels, the type of object that can hold these two components is called a workflow. Each has arguments, many of which are for tuning parameters.\nThere are standardized arguments for most model parameters. For example, regularization in glmnet models and neural networks use the argument name penalty even though the latter model would refer to this as weight decay.\ntidymodels differentiates between two varieties of tuning parameters:\n\nmain arguments are used by most engines for a model type.\nengine arguments represent more niche values specific to a few engines.\n\nLet’s look at an example. In the embeddings chapter, the barley data had a high degree of correlation between the predictors. We discussed PCA, PLS, and other methods to deal with this (via recipe steps). We might try a neural network (say, using the brulee engine) for a model. The code to specify this pipeline would be:\n\npls_rec &lt;-\n  recipe(barley ~ ., data = barley_train) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;%\n  step_pls(all_numeric_predictors(),\n           outcome = \"barley\",\n           num_comp = 20) %&gt;%\n  step_normalize(all_predictors())\n\nnnet_spec &lt;-\n  mlp(\n    hidden_units = 10,\n    activation = \"relu\",\n    penalty = 0.01,\n    epochs = 1000,\n    learn_rate = 0.1\n  ) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"brulee\")\n\npls_nnet_wflow &lt;- workflow(pls_rec, nnet_spec)\n\nWe’ve filled in specific values for each of these arguments, although we don’t know if these are best that we can do.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-tag-for-tuning",
    "href": "chapters/overfitting.html#sec-tag-for-tuning",
    "title": "9  Overfitting",
    "section": "\n9.3 Marking Parameters for Optimization",
    "text": "9.3 Marking Parameters for Optimization\nTo tune these parameters, we can give them a value of the function tune(). This special function just returns an expression with the value “tune()”. For example:\n\npls_rec &lt;-\n  recipe(barley ~ ., data = barley_train) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;%\n  step_pls(all_numeric_predictors(),\n           outcome = \"barley\",\n           # For demonstration, we'll use a label\n           num_comp = tune(\"pca comps\")) %&gt;%\n  step_normalize(all_predictors())\n\nnnet_spec &lt;-\n  mlp(\n    hidden_units = tune(),\n    activation = tune(),\n    penalty = tune(),\n    epochs = tune(),\n    learn_rate = tune()\n  ) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"brulee\")\n\npls_nnet_wflow &lt;- workflow(pls_rec, nnet_spec)\n\nOptionally, we can give a label as an argument to the function:\n\nstr(tune(\"#PCA components\"))\n#&gt;  language tune(\"#PCA components\")\n\nThis is useful when the pipeline has two arguments with the same name. For example, if you wanted to use splines for two predictors but allow them to have different degrees of freedom, the resulting set of parameters would not be unique since both of them would have the default label of deg_free. In this case, one recipe step could use tune(\"predictor 1 deg free\") and another could be tune(\"predictor 2 deg free\").\nEngine arguments are set by set_engine(). For example:\n\nnnet_spec &lt;-\n  mlp(\n    hidden_units = tune(),\n    activation = tune(),\n    penalty = tune(),\n    epochs = tune(),\n    learn_rate = tune()\n  ) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"brulee\", stop_iter = 5, rate_schedule = tune()) \n\npls_nnet_wflow &lt;- workflow(pls_rec, nnet_spec)",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-param-functions",
    "href": "chapters/overfitting.html#sec-param-functions",
    "title": "9  Overfitting",
    "section": "\n9.4 Parameter Functions",
    "text": "9.4 Parameter Functions\nEach tuning parameter has a corresponding function from the dials package containing information on the parameter type, parameter ranges (or possible values), and other data.\nFor example, the function for the penalty argument is:\n\npenalty()\n#&gt; Amount of Regularization (quantitative)\n#&gt; Transformer: log-10 [1e-100, Inf]\n#&gt; Range (transformed scale): [-10, 0]\n\nThis parameter has a default range from 10-10 to 1.0. It also has a corresponding transformation function (log base 10). This means that when values are created, they are uniformly distributed on the log scale. This is common for parameters that have values that span several orders of magnitude and cannot be negative.\nWe can change these defaults via arguments:\n\npenalty(range = c(0, 1), trans = scales::transform_identity())\n#&gt; Amount of Regularization (quantitative)\n#&gt; Transformer: identity [-Inf, Inf]\n#&gt; Range (transformed scale): [0, 1]\n\nIn some cases, we can’t know the range a priori. Parameters like the number of possible PCA components or random forest’s \\(m_{try}\\) depend on the data dimensions. In the case of \\(m_{try}\\) , the default has an unknown in its range:\n\nmtry()\n#&gt; # Randomly Selected Predictors (quantitative)\n#&gt; Range: [1, ?]\n\nWe would need to set this range to use the parameter.\nIn a few situations, the argument name to a recipe step or model function will use a dials function that has a different name than the argument. For example, there are a few different types of “degrees”. There is (real-valued) polynomial exponent degree:\n\ndegree()\n#&gt; Polynomial Degree (quantitative)\n#&gt; Range: [1, 3]\n\n# Data type: \ndegree()$type\n#&gt; [1] \"double\"\n\nbut for the spline recipe steps we need an integer value:\n\n# Data type: \nspline_degree()$type\n#&gt; [1] \"integer\"\n\nIn some cases, tidymodels has methods for automatically changing the parameter function to be used, the range of values, and so on. We’ll see that in a minute.\nThere are also functions to manipulate individual parameters:\n\n# Function list:\napropos(\"^value_\")\n#&gt; [1] \"value_inverse\"   \"value_sample\"    \"value_seq\"       \"value_set\"      \n#&gt; [5] \"value_transform\" \"value_validate\"\n\nvalue_seq(hidden_units(), n = 4)\n#&gt; [1]  1  4  7 10",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-param-sets",
    "href": "chapters/overfitting.html#sec-param-sets",
    "title": "9  Overfitting",
    "section": "\n9.5 Sets of Parameters",
    "text": "9.5 Sets of Parameters\nFor our pipeline pls_nnet_wflow, we can extract a parameter set that collects all of the parameters and their suggested information. There is a function to do this:\n\npls_nnet_param &lt;- extract_parameter_set_dials(pls_nnet_wflow)\n\nclass(pls_nnet_param)\n#&gt; [1] \"parameters\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnames(pls_nnet_param)\n#&gt; [1] \"name\"         \"id\"           \"source\"       \"component\"    \"component_id\"\n#&gt; [6] \"object\"\n\npls_nnet_param\n#&gt; Collection of 7 parameters for tuning\n#&gt; \n#&gt;     identifier          type    object\n#&gt;   hidden_units  hidden_units nparam[+]\n#&gt;        penalty       penalty nparam[+]\n#&gt;         epochs        epochs nparam[+]\n#&gt;     activation    activation dparam[+]\n#&gt;     learn_rate    learn_rate nparam[+]\n#&gt;  rate_schedule rate_schedule dparam[+]\n#&gt;      pca comps      num_comp nparam[+]\n#&gt; \n\nThe difference in the type and identifier columns only occurs when the tune() value has a label (as with the final row).\nThe output \"nparam[+]\" indicates a numeric parameter, and the plus indicates that it is fully specified. If our pipeline had used \\(m_{try}\\), that value would show \"nparam[?]\". The rate schedule is a qualitative parameter and has a label of \"dparam[+]\" (“d” for discrete).\nLet’s look at the information for the learning rate parameter by viewing the parameter information set by tidymodels. It is different than the default:\n\npls_nnet_param %&gt;% \n  filter(id == \"learn_rate\") %&gt;% \n  pluck(\"object\")\n#&gt; [[1]]\n#&gt; Learning Rate (quantitative)\n#&gt; Transformer: log-10 [1e-100, Inf]\n#&gt; Range (transformed scale): [-3, -0.2]\n\n# The defaults: \nlearn_rate()\n#&gt; Learning Rate (quantitative)\n#&gt; Transformer: log-10 [1e-100, Inf]\n#&gt; Range (transformed scale): [-10, -1]\n\nWhy are they different? The main function has a wider range since it can be used by boosted trees, neural networks, UMAP, and other tools. The range is more narrow for this pipeline since we know that neural networks tend to work better with faster learning rates (so we set a different default).\nSuppose we want to change the range to be even more narrow. We can use the update() function to change defaults or to use a different dials parameter function:\n\nnew_rate &lt;- \n  pls_nnet_param %&gt;% \n  update(learn_rate = learn_rate(c(-2, -1/2)))\n\nnew_rate %&gt;% \n  filter(id == \"learn_rate\") %&gt;% \n  pluck(\"object\")\n#&gt; [[1]]\n#&gt; Learning Rate (quantitative)\n#&gt; Transformer: log-10 [1e-100, Inf]\n#&gt; Range (transformed scale): [-2, -0.5]\n\nYou don’t always have to extract or modify a parameter set; this is an optional tool in case you want to change default values.\nThe parameter set is sometimes passed as an argument to grid creation functions or to iterative optimization functions that need to simulate/sample random candidates. For example, to create a random grid with 4 candidate values:\n\nset.seed(220)\ngrid_random(pls_nnet_param, size = 4)\n#&gt; # A tibble: 4 × 7\n#&gt;   hidden_units  penalty epochs activation  learn_rate rate_schedule `pca comps`\n#&gt;          &lt;int&gt;    &lt;dbl&gt;  &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;               &lt;int&gt;\n#&gt; 1           36 1.883e-3    112 elu            0.04998 cyclic                  3\n#&gt; 2            7 3.417e-2    362 tanhshrink     0.05164 cyclic                  2\n#&gt; 3            7 1.593e-9    444 log_sigmoid    0.03668 cyclic                  1\n#&gt; 4           38 4.803e-8    150 tanhshrink     0.02776 step                    3",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html",
    "href": "chapters/resampling.html",
    "title": "10  Measuring Performance with Resampling",
    "section": "",
    "text": "10.1 Requirements\nThis chapter outlines how to create objects to facilitate resampling. At the end, Section 10.11 illustrates how to use the resampling objects to produce good estimates of performance for a model.\nWe will use the ames, concrete, and Chicago data sets from the modeldata package (which is automatically loaded below) to illustrate some of the methods. The other data, Orthodont, can be obtained in the nlme package which comes with each R installation.\nYou’ll need 8 packages (Cubist, future.mirai, parallelly, probably, rules, spatialsample, tidymodels, and tidysdm) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"Cubist\", \"future.mirai\", \"parallelly\", \"probably\", \"rules\", \"spatialsample\", \n             \"tidymodels\", \"tidysdm\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-resampling-basics",
    "href": "chapters/resampling.html#sec-resampling-basics",
    "title": "10  Measuring Performance with Resampling",
    "section": "\n10.2 Basic Methods and Data Structures",
    "text": "10.2 Basic Methods and Data Structures\nThe rsample package provides many functions to facilitate resampling. For now, we’ll assume an initial split into a training and test set has been made (but see Section 10.4 below for three-way splits).\nLet’s use the concrete data for illustration:\n\nset.seed(82)\nconcrete_split &lt;- initial_split(concrete)\nconcrete_tr &lt;- training(concrete_split)\nconcrete_te &lt;- testing(concrete_split)\n\nAll resampling methods use the training set as the substrate for creating resamples. We’ll demonstrate the main tools and functions using basic bootstrap resampling and then discuss the other resampling methods.\nThe function that we’ll use to demonstrate is rsample::bootstraps(). Its main option, times, describes how many resamples to create. Let’s make five resamples:\n\nset.seed(380)\nconcrete_rs &lt;- bootstraps(concrete, times = 5)\n\nThis creates a tibble with five rows and two columns:\n\nconcrete_rs\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 5 × 2\n#&gt;   splits             id        \n#&gt;   &lt;list&gt;             &lt;chr&gt;     \n#&gt; 1 &lt;split [1030/355]&gt; Bootstrap1\n#&gt; 2 &lt;split [1030/408]&gt; Bootstrap2\n#&gt; 3 &lt;split [1030/369]&gt; Bootstrap3\n#&gt; 4 &lt;split [1030/375]&gt; Bootstrap4\n#&gt; 5 &lt;split [1030/378]&gt; Bootstrap5\n\nWe’ll investigate the splits column in a bit. The id column gives each row a name (corresponding to a resample). Some resampling methods, such as repeated cross-validation, can have additional identification columns.\nThe object has two additional classes bootstraps and rset that differentiate it from a standard tibble.\nThis collection of splits is called an \"rset\" and has specific rules. The object above is defined as a set of five bootstrap samples. If we delete a row, it breaks that definition, and the class drops back down to a basic tibble:\n\nconcrete_rs[-1,]\n#&gt; # A tibble: 4 × 2\n#&gt;   splits             id        \n#&gt;   &lt;list&gt;             &lt;chr&gt;     \n#&gt; 1 &lt;split [1030/408]&gt; Bootstrap2\n#&gt; 2 &lt;split [1030/369]&gt; Bootstrap3\n#&gt; 3 &lt;split [1030/375]&gt; Bootstrap4\n#&gt; 4 &lt;split [1030/378]&gt; Bootstrap5\n\nThe “A tibble: 4 × 2” title is different from the original title of “Bootstrap sampling.” This, and the class values, give away the difference. You can add columns without violating the definition.\n\n10.2.1 Split Objects\nNote the splits column in the output above. This is a list column in the data frame. It contains an rsplit object that tells us which rows of the training set go into our analysis and assessment sets. As a reminder:\n\nThe analysis set (of size \\(n_{fit}\\)) estimates quantities associated with preprocessing, model training, and postprocessing.\nThe assessment set (\\(n_{pred}\\)) is only used for prediction so that we can compute measures of model effectiveness (e.g., RMSE, classification accuracy, etc).\n\nWhen we see output “&lt;split [1030/408]&gt;” this indicates a binary split where the analysis set has 1030 rows, and the assessment set has 408 rows.\nTo get the analysis and assessment sets, there are two eponymous functions. For a specific split:\n\nex_split &lt;- concrete_rs$splits[[1]]\nex_split\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;1030/355/1030&gt;\n\nanalysis(ex_split)   %&gt;% dim()\n#&gt; [1] 1030    9\nassessment(ex_split) %&gt;% dim()\n#&gt; [1] 355   9\n\nIf we want to get the specific row indices of the training set:\n\nex_ind &lt;- as.integer(ex_split, data = \"assessment\")\nhead(ex_ind)\n#&gt; [1]  2  5  6 14 15 17\nlength(ex_ind)\n#&gt; [1] 355\n\nYou shouldn’t really have to interact with these objects at all.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-resampling-todo",
    "href": "chapters/resampling.html#sec-resampling-todo",
    "title": "10  Measuring Performance with Resampling",
    "section": "\n10.3 Basic Resampling Tools",
    "text": "10.3 Basic Resampling Tools\nrsample contains several functions that resample data whose rows are thought to be statistically independent of one another. Almost all of these functions contain options for stratified resampling.\nWe’ll show an example with basic 10-fold cross-validation below in Section 10.11.\nLet’s examine each flavor of resampling mentioned in the book chapter.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-validation-sets",
    "href": "chapters/resampling.html#sec-validation-sets",
    "title": "10  Measuring Performance with Resampling",
    "section": "\n10.4 Validation Sets",
    "text": "10.4 Validation Sets\nTo create a validation set, the first split should use rsample::initial_validation_split():\n\nset.seed(426)\nconcrete_split &lt;- initial_validation_split(concrete, prop = c(.8, .1))\nconcrete_tr &lt;- training(concrete_split)\nconcrete_vl &lt;- validation(concrete_split)\nconcrete_te &lt;- testing(concrete_split)\n\nTo make an rset object that can be used with most of tidymodel’s resampling machinery, we can use the rsample::validation_set() function to produce one (taking the initial three-way split as input):\n\nconcrete_rs &lt;- validation_set(concrete_split)\nconcrete_rs\n#&gt; # A tibble: 1 × 2\n#&gt;   splits            id        \n#&gt;   &lt;list&gt;            &lt;chr&gt;     \n#&gt; 1 &lt;split [824/103]&gt; validation\n\nAt this point, we can use concrete_rs as if it were any other rset object.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-cv-mc",
    "href": "chapters/resampling.html#sec-cv-mc",
    "title": "10  Measuring Performance with Resampling",
    "section": "\n10.5 Monte Carlo Cross-Validation",
    "text": "10.5 Monte Carlo Cross-Validation\nThe relevant function here is mc_cv() with two main arguments:\n\n\ntimes is the number of resamples\n\nprop is the proportion of the data that is allocated to the analysis set.\n\nFor example:\n\nset.seed(380)\nmc_cv(concrete_tr, times = 3, prop = 9 / 10)\n#&gt; # Monte Carlo cross-validation (0.9/0.1) with 3 resamples \n#&gt; # A tibble: 3 × 2\n#&gt;   splits           id       \n#&gt;   &lt;list&gt;           &lt;chr&gt;    \n#&gt; 1 &lt;split [741/83]&gt; Resample1\n#&gt; 2 &lt;split [741/83]&gt; Resample2\n#&gt; 3 &lt;split [741/83]&gt; Resample3\n\n# or \n\nmc_cv(concrete_tr, times = 2, prop = 2 /  3)\n#&gt; # Monte Carlo cross-validation (0.67/0.33) with 2 resamples \n#&gt; # A tibble: 2 × 2\n#&gt;   splits            id       \n#&gt;   &lt;list&gt;            &lt;chr&gt;    \n#&gt; 1 &lt;split [549/275]&gt; Resample1\n#&gt; 2 &lt;split [549/275]&gt; Resample2",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-cv",
    "href": "chapters/resampling.html#sec-cv",
    "title": "10  Measuring Performance with Resampling",
    "section": "\n10.6 V-Fold Cross-Validation",
    "text": "10.6 V-Fold Cross-Validation\nBasic V-fold cross-validation is performed using vfold_cv(). The v argument defines the number of folds and defaults to v = 10.\n\nset.seed(380)\nconcrete_rs &lt;- vfold_cv(concrete_tr)\nconcrete_rs\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 × 2\n#&gt;   splits           id    \n#&gt;   &lt;list&gt;           &lt;chr&gt; \n#&gt; 1 &lt;split [741/83]&gt; Fold01\n#&gt; 2 &lt;split [741/83]&gt; Fold02\n#&gt; 3 &lt;split [741/83]&gt; Fold03\n#&gt; 4 &lt;split [741/83]&gt; Fold04\n#&gt; 5 &lt;split [742/82]&gt; Fold05\n#&gt; 6 &lt;split [742/82]&gt; Fold06\n#&gt; # ℹ 4 more rows\n\nAs with the other tools, the strata argument can balance the outcome distributions across folds. It takes a single column as input.\nAnother argument of note is repeats, which describes how many sets of V resamples should be created. This generated an additional column called id2:\n\nset.seed(380)\nconcrete_rs &lt;- vfold_cv(concrete_tr, repeats = 2)\nconcrete_rs\n#&gt; #  10-fold cross-validation repeated 2 times \n#&gt; # A tibble: 20 × 3\n#&gt;   splits           id      id2   \n#&gt;   &lt;list&gt;           &lt;chr&gt;   &lt;chr&gt; \n#&gt; 1 &lt;split [741/83]&gt; Repeat1 Fold01\n#&gt; 2 &lt;split [741/83]&gt; Repeat1 Fold02\n#&gt; 3 &lt;split [741/83]&gt; Repeat1 Fold03\n#&gt; 4 &lt;split [741/83]&gt; Repeat1 Fold04\n#&gt; 5 &lt;split [742/82]&gt; Repeat1 Fold05\n#&gt; 6 &lt;split [742/82]&gt; Repeat1 Fold06\n#&gt; # ℹ 14 more rows",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-bootstrap",
    "href": "chapters/resampling.html#sec-bootstrap",
    "title": "10  Measuring Performance with Resampling",
    "section": "\n10.7 The Bootstrap",
    "text": "10.7 The Bootstrap\nFirst, there are special occasions when the regular set of bootstrap samples needs to be supplemented with an additional resample that can be used to measure the resubstitution rate (predicting the analysis set after fitting the data on the same analysis set). The function that produces this extra row is called apparent(), which is the same name as the function argument:\n\nset.seed(380)\nconcrete_rs &lt;- bootstraps(concrete, times = 5, apparent = TRUE)\n\nNote that the id column reflects this, and the split label shows that the analysis and assessment sets are the same size.\nThe tidymodels resampling functions are aware of the potential presence of the “apparent” sample and will not include it at inappropriate times. For example, if we resample a model with and without using apparent = TRUE, we’ll get the same results as long as we use the same random number seed to make each set of resamples.\nSecondly, there is a strata argument for this function. That enables different bootstrap samples to be taken within each stratum, which are combined into the final resampling set. Technically, this isn’t a bootstrap sample, but it is probably close enough to be useful.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-time-series-resampling",
    "href": "chapters/resampling.html#sec-time-series-resampling",
    "title": "10  Measuring Performance with Resampling",
    "section": "\n10.8 Time Series Data",
    "text": "10.8 Time Series Data\nUsually, the most recent data are used to evaluate performance for time series data. The function initial_time_split() can be used to make the initial split. We’ll use the Chicago data to demonstrate:\n\nn &lt;- nrow(Chicago)\n# To get 30 days of data\nprop_30 &lt;- (n - 30) / n\n\nchi_split &lt;- initial_time_split(Chicago, prop = prop_30)\nchi_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;5668/30/5698&gt;\n\nchi_tr &lt;- training(chi_split)\nchi_te &lt;- testing(chi_split)\n\nLet’s say that we want\n\n5,000 days in our analysis set,\n30 day assessment sets\nshift the 30-day window 30 days ahead\n\nThe data set has a column with the Date class. We can use this to partition the data in case there is an unequal number of data points in our 30-day period. The sliding_period() can be used with a date or date/time input, while sliding_index() can be used for equally spaced data.\nLet’s use sliding_period() for this example, annotate the argument logic in comments, and then compute a few columns for the start/stop dates for the analysis and assessment sets (for illustration):\n\nchi_rs &lt;- \n  sliding_period(\n    chi_tr,\n    index = date,\n    period = \"day\",   # Could be \"year\", \"quarter\", \"month\", \"week\", or \"day\"\n    lookback = 5000,  # 5000 days in the analysis sets\n    assess_start = 1, # Start the assessment set 1 day after analysis set\n    assess_stop = 30, # Stop the assessment set 20 days after analysis set\n    step = 30         # Jump ahead 30 days between resamples; no assessment overlap in assessments\n  ) %&gt;% \n  mutate(\n    fit_start =  map_vec(splits, ~ min(analysis(.x)$date)),\n    fit_stop =   map_vec(splits, ~ max(analysis(.x)$date)),\n    perf_start = map_vec(splits, ~ min(assessment(.x)$date)),\n    perf_stop =  map_vec(splits, ~ max(assessment(.x)$date))\n  )\nchi_rs\n#&gt; # Sliding period resampling \n#&gt; # A tibble: 22 × 6\n#&gt;   splits            id      fit_start  fit_stop   perf_start perf_stop \n#&gt;   &lt;list&gt;            &lt;chr&gt;   &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 &lt;split [5001/30]&gt; Slice01 2001-01-22 2014-10-01 2014-10-02 2014-10-31\n#&gt; 2 &lt;split [5001/30]&gt; Slice02 2001-02-21 2014-10-31 2014-11-01 2014-11-30\n#&gt; 3 &lt;split [5001/30]&gt; Slice03 2001-03-23 2014-11-30 2014-12-01 2014-12-30\n#&gt; 4 &lt;split [5001/30]&gt; Slice04 2001-04-22 2014-12-30 2014-12-31 2015-01-29\n#&gt; 5 &lt;split [5001/30]&gt; Slice05 2001-05-22 2015-01-29 2015-01-30 2015-02-28\n#&gt; 6 &lt;split [5001/30]&gt; Slice06 2001-06-21 2015-02-28 2015-03-01 2015-03-30\n#&gt; # ℹ 16 more rows\n\nThe first analysis set starts on 2001-01-22 and ends 5,000 days later on 2014-10-01. The next day (2014-10-02), the analysis set includes 30 days and stops on 2014-10-31.\nFor the second resample, the analysis and assessment sets both start 30 days later.\nHere is a visualization of the date periods defined by the resampling scheme that illustrates why the method is sometimes called rolling origin forecast resampling. The figure also shows that the assessment sets are very small compared to the analysis sets.\n\nchi_rs %&gt;% \n  ggplot(aes(y = id)) + \n  geom_segment(aes(x = fit_start,  xend = fit_stop,  yend = id), col = \"grey\", linewidth = 1) +\n  geom_segment(aes(x = perf_start, xend = perf_stop, yend = id), col = \"red\", linewidth = 3) +\n  labs(y = NULL, x = \"Date\")\n\n\n\n\n\n\n\nOne variation of this approach is to cumulately increase the analysis set by keeping the starting date the same (inside of sliding/rolling). For this, we can make the “lookback” infinite but use the skip argument to remove the large number of resamples that contain fewer than 5,000 days in the analysis sets:\n\nchi_rs &lt;- \n  sliding_period(\n    chi_tr,\n    index = date,\n    period = \"day\", \n    lookback = Inf,   # Use all data before assessment\n    assess_start = 1,\n    assess_stop = 30,\n    step = 30,\n    skip = 5000       # Drop first 5000 results so assessment starts at same time \n  ) %&gt;% \n  mutate(\n    fit_start =  map_vec(splits, ~ min(analysis(.x)$date)),\n    fit_stop =   map_vec(splits, ~ max(analysis(.x)$date)),\n    perf_start = map_vec(splits, ~ min(assessment(.x)$date)),\n    perf_stop =  map_vec(splits, ~ max(assessment(.x)$date))\n  )\nchi_rs\n#&gt; # Sliding period resampling \n#&gt; # A tibble: 22 × 6\n#&gt;   splits            id      fit_start  fit_stop   perf_start perf_stop \n#&gt;   &lt;list&gt;            &lt;chr&gt;   &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 &lt;split [5001/30]&gt; Slice01 2001-01-22 2014-10-01 2014-10-02 2014-10-31\n#&gt; 2 &lt;split [5031/30]&gt; Slice02 2001-01-22 2014-10-31 2014-11-01 2014-11-30\n#&gt; 3 &lt;split [5061/30]&gt; Slice03 2001-01-22 2014-11-30 2014-12-01 2014-12-30\n#&gt; 4 &lt;split [5091/30]&gt; Slice04 2001-01-22 2014-12-30 2014-12-31 2015-01-29\n#&gt; 5 &lt;split [5121/30]&gt; Slice05 2001-01-22 2015-01-29 2015-01-30 2015-02-28\n#&gt; 6 &lt;split [5151/30]&gt; Slice06 2001-01-22 2015-02-28 2015-03-01 2015-03-30\n#&gt; # ℹ 16 more rows\n\nNote that the values in the fit_stop column are the same. Visually:\n\nchi_rs %&gt;% \n  ggplot(aes(y = id)) + \n  geom_segment(aes(x = fit_start,  xend = fit_stop,  yend = id), col = \"grey\", linewidth = 1) +\n  geom_segment(aes(x = perf_start, xend = perf_stop, yend = id), col = \"red\", linewidth = 3) +\n  labs(y = NULL, x = \"Date\")",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-spatial-resampling",
    "href": "chapters/resampling.html#sec-spatial-resampling",
    "title": "10  Measuring Performance with Resampling",
    "section": "\n10.9 Spatial Data",
    "text": "10.9 Spatial Data\nWe split the Ames data into a training and testing set back in Section 3.8 using this code:\n\nlibrary(sf)\n#&gt; Linking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\nlibrary(spatialsample)\nlibrary(tidysdm)\n\names_sf &lt;-\n  ames %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n\nset.seed(318)\names_block_buff_split &lt;-\n  spatial_initial_split(\n    ames_sf, \n    prop = 0.2, \n    strategy = spatial_block_cv,\n    method = \"continuous\",\n    n = 25, \n    square = FALSE,\n    buffer = 250)\n\names_tr &lt;- training(ames_block_buff_split)\names_te &lt;- testing(ames_block_buff_split)\n\nThe options for resampling are basically the same as the initial split. For example, with block resampling, we create a grid on the training set and allocate specific grids to specific assessment sets. Buffering can also be used for each resample. The spatial_block_cv() function in the spatialsample package and has a v argument for the number of resamples:\n\nset.seed(652)\names_rs &lt;-\n  spatial_block_cv(\n    ames_tr, \n    v = 10,\n    method = \"continuous\",\n    n = 25, \n    square = FALSE,\n    buffer = 250)\names_rs\n#&gt; #  10-fold spatial block cross-validation \n#&gt; # A tibble: 10 × 2\n#&gt;   splits             id    \n#&gt;   &lt;list&gt;             &lt;chr&gt; \n#&gt; 1 &lt;split [1047/167]&gt; Fold01\n#&gt; 2 &lt;split [1098/182]&gt; Fold02\n#&gt; 3 &lt;split [1225/147]&gt; Fold03\n#&gt; 4 &lt;split [1130/174]&gt; Fold04\n#&gt; 5 &lt;split [1123/155]&gt; Fold05\n#&gt; 6 &lt;split [1235/95]&gt;  Fold06\n#&gt; # ℹ 4 more rows\n\nThere is an overall autoplot() method that can be used to show the grid:\n\nautoplot(ames_rs, cex = 1 / 3, show_grid = TRUE)\n\n\n\n\n\n\n\nWe can also autoplot() individual splits to see the analysis and assessment set.\n\nautoplot(ames_rs$splits[[1]], cex = 1 / 2)",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-multilevel-resampling",
    "href": "chapters/resampling.html#sec-multilevel-resampling",
    "title": "10  Measuring Performance with Resampling",
    "section": "\n10.10 Grouped or Multi-Level Data",
    "text": "10.10 Grouped or Multi-Level Data\nReturning to the orthodontal data from Section 3.7, we use the initial split:\n\ndata(Orthodont, package = \"nlme\")\n\nset.seed(93)\north_split &lt;- group_initial_split(Orthodont, group = Subject, prop = 2 / 3)\north_tr &lt;- training(orth_split)\north_te &lt;- testing(orth_split)\n\nThere are several resampling functions for these data in the rsample package, including: group_vfold_cv(), group_bootstraps(), and group_mc_cv(). For example:\n\nlibrary(vctrs)\n\n# Subjects in the training set: \nvec_unique_count(orth_tr$Subject)\n#&gt; [1] 18\n\nset.seed(714)\north_rs &lt;- \n  group_vfold_cv(orth_tr, group = Subject, v = 10) %&gt;% \n  mutate(num_subjects = map_int(splits, ~ vec_unique_count(assessment(.x)$Subject)))\n\north_rs\n#&gt; # Group 10-fold cross-validation \n#&gt; # A tibble: 10 × 3\n#&gt;   splits         id         num_subjects\n#&gt;   &lt;list&gt;         &lt;chr&gt;             &lt;int&gt;\n#&gt; 1 &lt;split [64/8]&gt; Resample01            2\n#&gt; 2 &lt;split [64/8]&gt; Resample02            2\n#&gt; 3 &lt;split [64/8]&gt; Resample03            2\n#&gt; 4 &lt;split [64/8]&gt; Resample04            2\n#&gt; 5 &lt;split [64/8]&gt; Resample05            2\n#&gt; 6 &lt;split [64/8]&gt; Resample06            2\n#&gt; # ℹ 4 more rows\n\nTo leave a single subject out for each resample, we could have set v to be 18.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-resampled-models",
    "href": "chapters/resampling.html#sec-resampled-models",
    "title": "10  Measuring Performance with Resampling",
    "section": "\n10.11 Estimating Performance",
    "text": "10.11 Estimating Performance\nNow that we can create different types of resamples for our training set, how do we actually resample a model to get accurate performance statistics?\ntidymodels contains high-level functions for this purpose, so there is typically no need to loop over rows of the resampling objects to get the data sets, train the model, etc.\nThe fit_resamples() function can do all of this for you. It takes a model (or workflow) in conjunction with a resampling object as inputs.\nTo demonstrate, let’s re-use the concrete data and create an object for a simple 10-fold cross-validation:\n\nset.seed(426)\nconcrete_split &lt;- initial_split(concrete, prop = 3 / 4)\nconcrete_tr &lt;- training(concrete_split)\nconcrete_te &lt;- testing(concrete_split)\n\nconcrete_rs &lt;- vfold_cv(concrete_tr)\n\nLet’s use a Cubist model for the data. It creates a set of rules from the data (derived from a regression tree) and, for each rule, creates a corresponding linear regression for the training set points covered by the rule. In the end, a sample is predicted, perhaps using multiple rules, and the average of the linear regression models is used as the prediction.\nUsually, we use a boosting-like process called _model committees _to create an ensemble of rule sets. Instead, we will make a single rule set. We’ll need to load the rules package to load this type of model into the parsnip model database.\n\nlibrary(rules)\nrules_spec &lt;- cubist_rules(committees = 1)\n\nTo specify the model in fit_resamples(), there are two options:\n\nThe first two arguments can be a model specification and a preprocessor (in that order). The preprocessor could be a recipe or a standard R formula.\nThe first argument can be a workflow.\n\nAfter the model specification, the resamples argument takes the resamping object.\nFrom here, we can run fit_resamples(). Note that the Cubist model does not use any random numbers. If it did, we would probably want to set the random number seed before using fit_resamples().\nOur code:\n\nconcrete_res &lt;- fit_resamples(rules_spec, compressive_strength ~ ., resamples = concrete_rs)\nconcrete_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 × 4\n#&gt;   splits           id     .metrics         .notes          \n#&gt;   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n#&gt; 1 &lt;split [694/78]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n#&gt; 2 &lt;split [694/78]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n#&gt; 3 &lt;split [695/77]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n#&gt; 4 &lt;split [695/77]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n#&gt; 5 &lt;split [695/77]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n#&gt; 6 &lt;split [695/77]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n#&gt; # ℹ 4 more rows\n\nThis looks a lot like our resampling object. There are some new columns. .metrics contains data frames with performance statistics for the particular resample. The .notes column contains any warnings or error messages that the model produced; none were produced by these 10 model fits. Note that, if there are errors, fit_resamples() does not stop computations.\nHow can we get our performance estimates? To aggregate the data in this object, there is a set of collect_*() functions. The first is collect_metrics(). By default, it returns the averages of the resampled estimates:\n\ncollect_metrics(concrete_res)\n#&gt; # A tibble: 2 × 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard   6.503     10 0.2396  Preprocessor1_Model1\n#&gt; 2 rsq     standard   0.8517    10 0.01170 Preprocessor1_Model1\n\nNote the n and std_err columns. To get the per-resample estimates:\n\ncollect_metrics(concrete_res, summarize = FALSE)\n#&gt; # A tibble: 20 × 5\n#&gt;   id     .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 Fold01 rmse    standard      6.764  Preprocessor1_Model1\n#&gt; 2 Fold01 rsq     standard      0.8462 Preprocessor1_Model1\n#&gt; 3 Fold02 rmse    standard      6.925  Preprocessor1_Model1\n#&gt; 4 Fold02 rsq     standard      0.8044 Preprocessor1_Model1\n#&gt; 5 Fold03 rmse    standard      6.652  Preprocessor1_Model1\n#&gt; 6 Fold03 rsq     standard      0.8298 Preprocessor1_Model1\n#&gt; # ℹ 14 more rows\n\nIf there were issues with the computations, collect_notes(concrete_res) would print a catalog of messages.\nNext, let’s look at a few customizations for fit_resamples().\n\n10.11.1 Parallel Processing\nWe fit 10 different Cubist models to 10 slightly different data sets. None of these computations depend on one another. This is the case of an “embarrassingly parallel” computing issue. We can increase our computational efficiency by running training the models on multiple “worker” processes on our computer(s).\nThe future package can run the resamples in parallel. The plan() function sets the parallel processing engine. There are a few plans that can be used:\n\nThe most common approach is “multisession”. This uses a parallel socket cluster (akak “psock cluster”) on your local computer. It is available for all operating systems.\nAnother option (not available on Windows) is “multicore”. The forks the current R session into different worker processes.\nThe “cluster” option is most useful for having worker processes on different machines.\nThe “sequential” plan is regular, non-parallel computing.\n\nThere are several other packages with plans: future.batchtools, future.callr, and future.mirai.\nOnce we know a plan, we run the following code (once) before running operations that can be done in parallel:\n\nlibrary(future)\nparallelly::availableCores()\nplan(multisession)\n\nAlternatively, the more recent mirai “engine” for parallel processing can also be used for additional efficiency.\n\nlibrary(future.mirai)\n#&gt; Loading required package: future\nplan(mirai_multisession)\n\nThis will generally increase the efficiency of the resampling process.\n\n10.11.2 Other Options\nLet’s look at some other options. First, note that the results did not include the trained models or the out-of-sample predicted results. This is the default because there is no way of knowing how much memory will be required to keep these values.\nWe’ll talk about accessing the fitted models in the next sections.\nTo save the predictions, we can use an R convention of a “control function.” These functions are reserved for specifying ancillary aspects of the computations. For fit_resamples() the control function is called control_resamples(), and it has an option called save_pred. When set to TRUE, the out-of-sample predictions are retained.\n\nctrl &lt;- control_resamples(save_pred = TRUE)\n\nLet’s also estimate different metrics. As mentioned back in Section 2.5, a metric set is used to specify statistics for model efficacy. fit_resamples() has an option called metrics that we can use to pass in a metric set.\nLet’s re-run our model with these two changes:\n\nreg_mtr &lt;- metric_set(rmse, rsq, ccc, mae)\n\nconcrete_opts_res &lt;- fit_resamples(\n  rules_spec,\n  compressive_strength ~ .,\n  resamples = concrete_rs,\n  metrics = reg_mtr,\n  control = ctrl\n)\nconcrete_opts_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 × 5\n#&gt;   splits           id     .metrics         .notes           .predictions     \n#&gt;   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;           \n#&gt; 1 &lt;split [694/78]&gt; Fold01 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [78 × 4]&gt;\n#&gt; 2 &lt;split [694/78]&gt; Fold02 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [78 × 4]&gt;\n#&gt; 3 &lt;split [695/77]&gt; Fold03 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [77 × 4]&gt;\n#&gt; 4 &lt;split [695/77]&gt; Fold04 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [77 × 4]&gt;\n#&gt; 5 &lt;split [695/77]&gt; Fold05 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [77 × 4]&gt;\n#&gt; 6 &lt;split [695/77]&gt; Fold06 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [77 × 4]&gt;\n#&gt; # ℹ 4 more rows\n\nThe expanded set of metrics:\n\ncollect_metrics(concrete_opts_res)\n#&gt; # A tibble: 4 × 6\n#&gt;   .metric .estimator   mean     n  std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 ccc     standard   0.9198    10 0.006753 Preprocessor1_Model1\n#&gt; 2 mae     standard   4.621     10 0.1657   Preprocessor1_Model1\n#&gt; 3 rmse    standard   6.503     10 0.2396   Preprocessor1_Model1\n#&gt; 4 rsq     standard   0.8517    10 0.01170  Preprocessor1_Model1\n\nNotice that there is now a column named .predictions. The number of rows in the tibbles matches the sizes of the assessment sets shown in the splits column; these are the held-out predicted values.\nTo obtain these values, there are collect_predictions() and augment():\n\nheldout_pred &lt;- collect_predictions(concrete_opts_res)\nheldout_pred\n#&gt; # A tibble: 772 × 5\n#&gt;   .pred id      .row compressive_strength .config             \n#&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;                &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 40.56 Fold01     8                40.76 Preprocessor1_Model1\n#&gt; 2 29.34 Fold01    14                29.22 Preprocessor1_Model1\n#&gt; 3 32.65 Fold01    27                37.36 Preprocessor1_Model1\n#&gt; 4 24.10 Fold01    38                20.73 Preprocessor1_Model1\n#&gt; 5 61.61 Fold01    53                55.16 Preprocessor1_Model1\n#&gt; 6 12.30 Fold01    71                 9.74 Preprocessor1_Model1\n#&gt; # ℹ 766 more rows\n\n# Same but merges them with the original training data\naugment(concrete_opts_res)\n#&gt; # A tibble: 772 × 11\n#&gt;    .pred    .resid cement blast_furnace_slag fly_ash water superplasticizer\n#&gt;    &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;\n#&gt; 1 45.54  -12.69     168                 42.1   163.8 121.8              5.7\n#&gt; 2 54.82   -1.131    190                190       0   228                0  \n#&gt; 3 32.77    0.07400  160                188     146   203               11  \n#&gt; 4  9.688   2.862    190.3                0     125.2 166.6              9.9\n#&gt; 5 47.36  -10.09     218.9                0     124.1 158.5             11.3\n#&gt; 6 16.93   -9.526    168.9               42.2   124.3 158.3             10.8\n#&gt; # ℹ 766 more rows\n#&gt; # ℹ 4 more variables: coarse_aggregate &lt;dbl&gt;, fine_aggregate &lt;dbl&gt;, age &lt;int&gt;,\n#&gt; #   compressive_strength &lt;dbl&gt;\n\nFrom here, we can do exploratory data analysis to understand where our model can be improved.\nOne other option: the probably package has a simple interface for obtaining plots of the observed and predicted values (i.e., a regression “calibration” plot):\n\nlibrary(probably)\ncal_plot_regression(concrete_opts_res)\n\n\n\n\n\n\n\nNot too bad but there are fairly large outliers that seem to occur more with mixtures corresponding to larger observed outcomes.\n\n10.11.3 Extracting Results\nHow can we get the 10 trained models? The control function has an option for extract that takes a user-defined function. The argument to this function (say x) is the fitted workflow. If you want the whole model, you can return x. Otherwise, we can run computations on the model and return whatever elements or statistics associated with the model that we are interested in.\nFor example, a tidy() method for Cubist models will save information on the rules, the regression function for each rule, and various statistics. To get this, we need to pull the Cubist model out of the workflow and then run the tidy method on it. Here is an example of that:\n\nextract_rules &lt;- function(x) {\n  x %&gt;% \n    extract_fit_engine() %&gt;%\n    tidy()\n}\n\nNow we update our control object:\n\nctrl &lt;- control_resamples(save_pred = TRUE, extract = extract_rules)\n\nand pass it into fit_resamples():\n\nconcrete_ext_res &lt;- fit_resamples(rules_spec,\n                                  compressive_strength ~ .,\n                                  resamples = concrete_rs,\n                                  control = ctrl)\nconcrete_ext_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 × 6\n#&gt;   splits           id     .metrics         .notes           .extracts .predictions\n#&gt;   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;      \n#&gt; 1 &lt;split [694/78]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;  &lt;tibble&gt;    \n#&gt; 2 &lt;split [694/78]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;  &lt;tibble&gt;    \n#&gt; 3 &lt;split [695/77]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;  &lt;tibble&gt;    \n#&gt; 4 &lt;split [695/77]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;  &lt;tibble&gt;    \n#&gt; 5 &lt;split [695/77]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;  &lt;tibble&gt;    \n#&gt; 6 &lt;split [695/77]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;  &lt;tibble&gt;    \n#&gt; # ℹ 4 more rows\n\nThe new output has an .extracts column. The rows contain a tibble that has the resampling information and another tibble, also with the name .extracts. We can pull that column out via:\n\nrule_extract &lt;- collect_extracts(concrete_ext_res)\nrule_extract\n#&gt; # A tibble: 10 × 3\n#&gt;   id     .extracts         .config             \n#&gt;   &lt;chr&gt;  &lt;list&gt;            &lt;chr&gt;               \n#&gt; 1 Fold01 &lt;tibble [10 × 5]&gt; Preprocessor1_Model1\n#&gt; 2 Fold02 &lt;tibble [13 × 5]&gt; Preprocessor1_Model1\n#&gt; 3 Fold03 &lt;tibble [13 × 5]&gt; Preprocessor1_Model1\n#&gt; 4 Fold04 &lt;tibble [11 × 5]&gt; Preprocessor1_Model1\n#&gt; 5 Fold05 &lt;tibble [13 × 5]&gt; Preprocessor1_Model1\n#&gt; 6 Fold06 &lt;tibble [13 × 5]&gt; Preprocessor1_Model1\n#&gt; # ℹ 4 more rows\n\nWhat is in a specific result?\n\nrule_extract$.extracts[[1]]\n#&gt; # A tibble: 10 × 5\n#&gt;   committee rule_num rule                                         estimate statistic\n#&gt;       &lt;int&gt;    &lt;int&gt; &lt;chr&gt;                                        &lt;list&gt;   &lt;list&gt;   \n#&gt; 1         1        1 ( cement &lt;= 218.89999 ) & ( blast_furnace_s… &lt;tibble&gt; &lt;tibble&gt; \n#&gt; 2         1        2 ( cement &lt;= 218.89999 ) & ( blast_furnace_s… &lt;tibble&gt; &lt;tibble&gt; \n#&gt; 3         1        3 ( water &gt; 183.8 ) & ( cement &gt; 218.89999 ) … &lt;tibble&gt; &lt;tibble&gt; \n#&gt; 4         1        4 ( age &lt;= 7 ) & ( water &lt;= 183.8 ) & ( blast… &lt;tibble&gt; &lt;tibble&gt; \n#&gt; 5         1        5 ( age &lt;= 28 ) & ( age &gt; 7 ) & ( cement &gt; 21… &lt;tibble&gt; &lt;tibble&gt; \n#&gt; 6         1        6 ( age &gt; 28 ) & ( superplasticizer &lt;= 7.8000… &lt;tibble&gt; &lt;tibble&gt; \n#&gt; # ℹ 4 more rows\n\nTo “flatten out” these results, we’ll unnest() the column of extracted results:\n\nrule_extract &lt;- \n  collect_extracts(concrete_ext_res) %&gt;% \n  unnest(col = .extracts)\nrule_extract\n#&gt; # A tibble: 124 × 7\n#&gt;   id     committee rule_num rule                          estimate statistic .config\n#&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;int&gt; &lt;chr&gt;                         &lt;list&gt;   &lt;list&gt;    &lt;chr&gt;  \n#&gt; 1 Fold01         1        1 ( cement &lt;= 218.89999 ) & ( … &lt;tibble&gt; &lt;tibble&gt;  Prepro…\n#&gt; 2 Fold01         1        2 ( cement &lt;= 218.89999 ) & ( … &lt;tibble&gt; &lt;tibble&gt;  Prepro…\n#&gt; 3 Fold01         1        3 ( water &gt; 183.8 ) & ( cement… &lt;tibble&gt; &lt;tibble&gt;  Prepro…\n#&gt; 4 Fold01         1        4 ( age &lt;= 7 ) & ( water &lt;= 18… &lt;tibble&gt; &lt;tibble&gt;  Prepro…\n#&gt; 5 Fold01         1        5 ( age &lt;= 28 ) & ( age &gt; 7 ) … &lt;tibble&gt; &lt;tibble&gt;  Prepro…\n#&gt; 6 Fold01         1        6 ( age &gt; 28 ) & ( superplasti… &lt;tibble&gt; &lt;tibble&gt;  Prepro…\n#&gt; # ℹ 118 more rows\n\nWhat could we do with these results? Let’s look at what is in the statistics tibble:\n\nrule_extract$statistic[[1]]\n#&gt; # A tibble: 1 × 6\n#&gt;   num_conditions coverage  mean   min   max error\n#&gt;            &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1              3       74 22.71   7.4 55.51 4.062\n\nIt might be interesting to know how complex each rule was. For example, the rule\n\n#&gt; (water &lt;= 174.8) & (cement &gt; 255.5) & (blast_furnace_slag &lt;= \n#&gt;     139.89999) & (age &gt; 7) & (age &lt;= 56) & (fly_ash &lt;= 126.5)\n\nhas 6 conditions.\nWe can use a mutate() and a map_int() to pull out the frequency of terms included in the rules (captured by the num_conditions column).\n\nconditions &lt;- \n  rule_extract %&gt;% \n  mutate(conditions = map_int(statistic, ~ .x$num_conditions)) %&gt;% \n  select(id, rule_num, conditions)\n\nHere is the distribution of the number of logical conditions that make up the rules:\n\nconditions %&gt;% \n  count(conditions)\n#&gt; # A tibble: 6 × 2\n#&gt;   conditions     n\n#&gt;        &lt;int&gt; &lt;int&gt;\n#&gt; 1          1     7\n#&gt; 2          2    13\n#&gt; 3          3    53\n#&gt; 4          4    36\n#&gt; 5          5    14\n#&gt; 6          6     1",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/grid.html",
    "href": "chapters/grid.html",
    "title": "11  Grid Search",
    "section": "",
    "text": "11.1 Requirements\nThe previous chapters have discussed how to estimate performance using resampling as well as how to tag arguments for optimization (via tune()). This page will illustrate how to use similar tools to optimize models via grid search.\nAs with the previous chapter, we will use the concrete data set from the modeldata package (which is automatically loaded below) to illustrate some of the methods.\nYou’ll need 5 packages (Cubist, finetune, future.mirai, rules, and tidymodels) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"Cubist\", \"finetune\", \"future.mirai\", \"rules\", \"tidymodels\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid.html#sec-grid-creation",
    "href": "chapters/grid.html#sec-grid-creation",
    "title": "11  Grid Search",
    "section": "\n11.2 Creating Grids",
    "text": "11.2 Creating Grids\nThe dials package has several grid creation functions, whose names all start with grid_. The primary input is a dials parameter set object, which can be created from a model, recipe, or workflow. The primary functions are:\n\n\ngrid_regular() for regular grids. The argument for specifying the size of the grid is called levels. This can be a single number (recycled across parameters) or a vector of sizes for each tuning parameter.\n\ngrid_random() creates random uniform parameter values. The argument size dictates how many candidate combinations are created.\n\ngrid_space_filling() can produce different types of space-filling designs (via the type argument). It also uses a size argument.\n\nLet’s pick back up from the Cubist example in Section 10.11. We can tag two of the Cubist models’s parameters for tuning:\n\nThe number of committee members in the ensemble (usually ranging from one to 100).\nThe number of neighbors to use in a post hoc model adjustment phase, ranging from zero neighbors (i.e., no adjustment) to nine.\n\nBoth of these parameters are described more in a blog post on “Modern Rule-Based Models”.\nWe need to load the rules package to enable access to the model, mark these parameters for tuning, and then extract the parameter set needed to make the grids.\n\nlibrary(rules)\n\ncubist_spec &lt;- cubist_rules(committees = tune(), neighbors = tune())\n\ncubist_param &lt;- \n  cubist_spec %&gt;% \n  extract_parameter_set_dials()\n\ncubist_param\n#&gt; Collection of 2 parameters for tuning\n#&gt; \n#&gt;  identifier       type    object\n#&gt;  committees committees nparam[+]\n#&gt;   neighbors  neighbors nparam[+]\n#&gt; \n\nLet’s make a uniform space-filling design with 25 candidate models:\n\ncubist_grid &lt;- grid_space_filling(cubist_param, size = 25) \n\ncubist_grid %&gt;% \n  ggplot(aes(committees, neighbors)) + \n  geom_point() + \n  coord_fixed(ratio = 10)\n\n\n\n\n\n\n\nRecall from Section 9.2, we can manipulate the ranges and values of the tuning parameters in the parameter set using update().\nNote that:\n\nIf we labeled any of our parameters (e.g., neighbors = tune(\"K\")), that label is used as the column name.\nSome parameters are associated with a transformation, and, by default, the values are created on that scale and then transformed back to the original units when the grid is returned.\nThe size argument should be considered the maximum size; redundant combinations are removed. For example:\n\n\n# Default range is 0L to 9L:\ncubist_rules(neighbors = tune(\"K\")) %&gt;% \n  extract_parameter_set_dials() %&gt;%\n  grid_space_filling(size = 50) %&gt;% \n  arrange(K)\n#&gt; # A tibble: 10 × 1\n#&gt;       K\n#&gt;   &lt;int&gt;\n#&gt; 1     0\n#&gt; 2     1\n#&gt; 3     2\n#&gt; 4     3\n#&gt; 5     4\n#&gt; 6     5\n#&gt; # ℹ 4 more rows\n\nYou can also make grid manually as long as they are in a data frame and the column names match the parameter types/labels of the parameters:\n\ncrossing(committees = c(1, 8, 100), neighbors = c(0, 9))\n#&gt; # A tibble: 6 × 2\n#&gt;   committees neighbors\n#&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1          1         0\n#&gt; 2          1         9\n#&gt; 3          8         0\n#&gt; 4          8         9\n#&gt; 5        100         0\n#&gt; 6        100         9\n\nFinally, as a reminder, a workflow can contain preprocessing arguments that were tagged for optimization via tune(). These values are treated the same as model arguments when it comes to extracting the parameter set and creating grids.\n (regular grids)  (irregular)",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid.html#sec-grid-tuning",
    "href": "chapters/grid.html#sec-grid-tuning",
    "title": "11  Grid Search",
    "section": "\n11.3 Tuning Models with Grids",
    "text": "11.3 Tuning Models with Grids\nFor grids, the three main functions are tune::tune_grid() and the two racing functions in the finetune package: finetune::tune_race_anova() and finetune::tune_race_winloss(). The syntax for these is nearly identical and also closely follows the previously described code for fit_resamples() from Section 10.11.\nThe primary arguments for these tuning functions in tidymodels are:\n\n\ngrid: Either a data frame or an integer value. The latter choice will trigger tidymodels to make a space-filling design for you.\n\nparam_info: The parameter set object. This is only needed if grid is an integer and you request nonstandard ranges/values for one or more parameters.\n\nOther arguments, such as metrics, are the same. The control function for these functions are named differently (e.g., tune_race()).\nTo get started, let’s recreate the objects for the concrete data that match those from the previous chapter:\n\nset.seed(426)\nconcrete_split &lt;- initial_split(concrete, prop = 3 / 4)\nconcrete_tr &lt;- training(concrete_split)\nconcrete_te &lt;- testing(concrete_split)\nconcrete_rs &lt;- vfold_cv(concrete_tr)\n\nWe will reuse the cubist_spec and cubist_grid objects created above.\nLet’s do basic grid search:\n\ncubist_res &lt;- \n  cubist_spec %&gt;% \n  tune_grid(\n    compressive_strength ~ .,\n    resamples = concrete_rs,\n    grid = cubist_grid,\n    control = control_grid(save_pred = TRUE, save_workflow = TRUE)\n  )\n\nThe option to save the workflow for our model will be references below.\nThis object is similar to the one produced by fit_resamples except that the .metrics and .predictions columns have more rows since their values contain the results for the 25 candidates. We have our previous functions to rely on:\n\ncollect_metrics(cubist_res)\n#&gt; # A tibble: 50 × 8\n#&gt;   committees neighbors .metric .estimator   mean     n  std_err .config             \n#&gt;        &lt;int&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1          1         4 rmse    standard   5.785     10 0.2373   Preprocessor1_Model…\n#&gt; 2          1         4 rsq     standard   0.8828    10 0.01024  Preprocessor1_Model…\n#&gt; 3          5         2 rmse    standard   5.004     10 0.2625   Preprocessor1_Model…\n#&gt; 4          5         2 rsq     standard   0.9103    10 0.009437 Preprocessor1_Model…\n#&gt; 5          9         7 rmse    standard   5.011     10 0.1992   Preprocessor1_Model…\n#&gt; 6          9         7 rsq     standard   0.9105    10 0.007576 Preprocessor1_Model…\n#&gt; # ℹ 44 more rows\ncollect_predictions(cubist_res)\n#&gt; # A tibble: 19,300 × 7\n#&gt;    .pred id      .row committees neighbors compressive_strength .config             \n#&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;      &lt;int&gt;     &lt;int&gt;                &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 41.63  Fold01     8          1         4                40.76 Preprocessor1_Model…\n#&gt; 2 29.84  Fold01    14          1         4                29.22 Preprocessor1_Model…\n#&gt; 3 37.52  Fold01    27          1         4                37.36 Preprocessor1_Model…\n#&gt; 4 24.99  Fold01    38          1         4                20.73 Preprocessor1_Model…\n#&gt; 5 62.37  Fold01    53          1         4                55.16 Preprocessor1_Model…\n#&gt; 6  9.835 Fold01    71          1         4                 9.74 Preprocessor1_Model…\n#&gt; # ℹ 19,294 more rows\n\nThere are a few additional methods that we can apply here. First, we can visualize the results using autoplot():\n\nautoplot(cubist_res)\n\n\n\n\n\n\n\nThis function has a metric argument in case you want to plot a selection of metrics. Also, for regular grids, the visualization can look very different.\nFrom these results, both tuning parameters have an effect on performance. A small number of committees or neighbors have poor performance. How can we tell which one was best for either metric?\nThere are also show_best() and select_*() functions to select the best results for a given metric:\n\nshow_best(cubist_res, metric = \"rmse\")\n#&gt; # A tibble: 5 × 8\n#&gt;   committees neighbors .metric .estimator  mean     n std_err .config              \n#&gt;        &lt;int&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1         95         2 rmse    standard   4.592    10  0.2151 Preprocessor1_Model24\n#&gt; 2         75         3 rmse    standard   4.610    10  0.2004 Preprocessor1_Model19\n#&gt; 3         29         2 rmse    standard   4.617    10  0.2122 Preprocessor1_Model08\n#&gt; 4         91         4 rmse    standard   4.619    10  0.1931 Preprocessor1_Model23\n#&gt; 5         54         3 rmse    standard   4.620    10  0.1908 Preprocessor1_Model14\n\nshow_best(cubist_res, metric = \"rsq\", n = 3)\n#&gt; # A tibble: 3 × 8\n#&gt;   committees neighbors .metric .estimator   mean     n  std_err .config             \n#&gt;        &lt;int&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1         95         2 rsq     standard   0.9244    10 0.007673 Preprocessor1_Model…\n#&gt; 2         75         3 rsq     standard   0.9241    10 0.006887 Preprocessor1_Model…\n#&gt; 3         54         3 rsq     standard   0.9239    10 0.006690 Preprocessor1_Model…\n\nTo return the candidate with the smallest RMSE:\n\ncubist_best &lt;- select_best(cubist_res, metric = \"rmse\")\ncubist_best\n#&gt; # A tibble: 1 × 3\n#&gt;   committees neighbors .config              \n#&gt;        &lt;int&gt;     &lt;int&gt; &lt;chr&gt;                \n#&gt; 1         95         2 Preprocessor1_Model24\n\nThere are a few things that we can do with this candidate value. We can use it to subset other results. For example, we can get the out-of-sample predictions for just this model via:\n\ncubist_res %&gt;% \n  collect_predictions() %&gt;% \n  nrow()\n#&gt; [1] 19300\n\n# Just for the best:\ncubist_res %&gt;% \n  collect_predictions(parameters = cubist_best) %&gt;% \n  nrow()\n#&gt; [1] 772\n\n# augment() returns the numerically best by default: \ncubist_res %&gt;% \n  augment() %&gt;% \n  nrow()\n#&gt; [1] 772\n\nWe can also give these values for the calibration plot produced by probably:\n\nlibrary(probably)\ncal_plot_regression(cubist_res, parameters = cubist_best)\n\n\n\n\n\n\n\nIf these candidate points appear to be optimal, we can also update our model specification (or workflow) using a finalize_*() function:\n\nfinalize_model(cubist_spec, cubist_best)\n#&gt; Cubist Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   committees = 95\n#&gt;   neighbors = 2\n#&gt; \n#&gt; Computational engine: Cubist\n\nIf we used the save_workflow = TRUE control option, we could get fit on the entire training set for this model via fit_best(), which serves as a shortcut\n\nfit_best(cubist_res)\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: cubist_rules()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────────────\n#&gt; compressive_strength ~ .\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; cubist.default(x = x, y = y, committees = 95L)\n#&gt; \n#&gt; Number of samples: 772 \n#&gt; Number of predictors: 8 \n#&gt; \n#&gt; Number of committees: 95 \n#&gt; Number of rules per committee: 11, 13, 10, 10, 12, 10, 11, 12, 4, 13, 8, 16, 5, 23, 5, 14, 4, 15, 12, 13 ...\n\nFinally, as previously seen in Section 10.11.1, we can parallel process these model fits using the same syntax as shown there.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid.html#racing",
    "href": "chapters/grid.html#racing",
    "title": "11  Grid Search",
    "section": "\n11.4 Racing",
    "text": "11.4 Racing\nThe syntax for these optimization methods is virtually the same. Besides the different function names, the control function has a few options of note:\n\n\nverbose_elim should be a logical for whether a log of the candidate eliminations should be shown.\n\nburn_in requires an integer and represents the earliest the parameter filter should be applied.\n\nnum_ties, also an integer value, decides when tie-breaking should occur when only two candidates are remaining.\n\nalpha is the numeric value for the hypothesis testing false positive rate (for one-sided hypothesis tests).\n\nrandomize accepts a logical value for whether the resamples should be randomly ordered.\n\nLet’s run the same Cubist grid using the ANOVA method:\n\nlibrary(finetune)\n\n# Since resamples are randomized, set the seed:\nset.seed(11)\ncubist_race_res &lt;- \n  cubist_spec %&gt;% \n  tune_race_anova(\n    compressive_strength ~ .,\n    resamples = concrete_rs,\n    grid = cubist_grid,\n    control = control_race(verbose_elim = TRUE)\n  )\n#&gt; ℹ Evaluating against the initial 3 burn-in resamples.\n#&gt; ℹ Racing will minimize the rmse metric.\n#&gt; ℹ Resamples are analyzed in a random order.\n#&gt; ℹ Fold05: 12 eliminated; 13 candidates remain.\n#&gt; \n#&gt; ℹ Fold07: 8 eliminated; 5 candidates remain.\n#&gt; \n#&gt; ℹ Fold10: 1 eliminated; 4 candidates remain.\n#&gt; \n#&gt; ℹ Fold01: 0 eliminated; 4 candidates remain.\n#&gt; \n#&gt; ℹ Fold08: 0 eliminated; 4 candidates remain.\n#&gt; \n#&gt; ℹ Fold03: 0 eliminated; 4 candidates remain.\n#&gt; \n#&gt; ℹ Fold09: 0 eliminated; 4 candidates remain.\n\nIt is important to note that the helpful functions for racing results mostly filter their output to disregard candidates who did not finish the race. For example, if we ask show_best() to provide results for more candidate than those that finished, it will curtail its output:\n\nshow_best(cubist_race_res, metric = \"rmse\", n = 10)\n#&gt; # A tibble: 4 × 8\n#&gt;   committees neighbors .metric .estimator  mean     n std_err .config              \n#&gt;        &lt;int&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1         95         2 rmse    standard   4.592    10  0.2151 Preprocessor1_Model24\n#&gt; 2         75         3 rmse    standard   4.610    10  0.2004 Preprocessor1_Model19\n#&gt; 3         29         2 rmse    standard   4.617    10  0.2122 Preprocessor1_Model08\n#&gt; 4         54         3 rmse    standard   4.620    10  0.1908 Preprocessor1_Model14\n\nTo visualize the results, there is also a plot_race() function:\n\nplot_race(cubist_race_res)\n\n\n\n\n\n\n\nEach line corresponds to a candidate.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid.html#sec-nested-resampling",
    "href": "chapters/grid.html#sec-nested-resampling",
    "title": "11  Grid Search",
    "section": "\n11.5 Nested Resampling",
    "text": "11.5 Nested Resampling\nThere are tidymodels experimental APIs for nested resampling (and analytical bias correction). We’ll fill this section in when these are finalized.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html",
    "href": "chapters/iterative-search.html",
    "title": "12  Iterative Search",
    "section": "",
    "text": "12.1 Requirements\nThis book chapter discusses several search procedures for finding optimal (or at least acceptable) tuning parameter values.\nThis chapter requires 6 packages (finetune, future.mirai, GA, probably, tidymodels, xgboost). To install:\nreq_pkg &lt;- c(\"finetune\", \"GA\", \"probably\", \"tidymodels\", \"xgboost\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the packages and set some preferences:\nlibrary(GA)\nlibrary(tidymodels)\nlibrary(finetune)\nlibrary(probably)\nlibrary(future.mirai)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\nplan(mirai_multisession)\nTo reduce the complexity of the example, we’ll use a simulated classification data set containing numeric predictors. We’ll simulate 1,000 samples using a simulation system, the details of which can be found in the modeldata documentation. The data set has linear, nonlinear, and interacting features, and the classes are fairly balanced. We’ll use a 3:1 split for training and testing as well as 10-fold cross-validation:\nset.seed(2783)\nsim_dat &lt;- sim_classification(1000)\n\nset.seed(101)\nsim_split &lt;- initial_split(sim_dat)\nsim_train &lt;- training(sim_split)\nsim_test &lt;- testing(sim_split)\nsim_rs &lt;- vfold_cv(sim_train)\nWe’ll tune a boosted classification model using the xgboost package, described in a later chapter. We tune multiple parameters and set an additional parameter, validation, to be zero. This is used when early stopping, which we will not use:\nbst_spec &lt;-\n  boost_tree(\n    mtry = tune(),\n    tree_depth = tune(),\n    trees = tune(),\n    learn_rate = tune(),\n    min_n = tune(),\n    loss_reduction = tune(),\n    sample_size = tune()\n  ) %&gt;%\n  set_engine('xgboost', validation = 0) %&gt;%\n  set_mode('classification')\nTree-based models require little to no preprocessing so we will use a simple R formula to define the roles of variables:\nbst_wflow &lt;- workflow(class ~ ., bst_spec)\nFrom the workflow, we create a parameters object and set the ranges for two parameters. mtry requires an upper bound to be set since it depends on the number of model terms in the data set. We’ll need parameter information since most iterative methods need to know the possible ranges as well as the type of parameter (e.g., integer, character, etc.) and/or any transformations of the values.\nbst_param &lt;-\n  bst_wflow %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  update(\n    mtry = mtry(c(3, 15)),\n    trees = trees(c(10, 500))\n  )\nWe can now fit and/or tune models. We’ll declare what metrics should be collected and then create a small space-filling design that is used as the starting point for simulated annealing and Bayesian optimization. We could let the system make these initial values for us, but we’ll create them now so that we can reuse the results and have a common starting place.\ncls_mtr &lt;- metric_set(brier_class, roc_auc)\n\ninit_grid &lt;- grid_space_filling(bst_param, size = 6)\n\nset.seed(21)\ninitial_res &lt;-\n  bst_wflow %&gt;%\n  tune_grid(\n    resamples = sim_rs,\n    grid = init_grid,\n    metrics = cls_mtr,\n    control = control_grid(save_pred = TRUE)\n  )\nFrom these six candidates, the smallest Brier score was 0.117, a mediocre value:\nshow_best(initial_res, metric = \"brier_class\") %&gt;% \n  select(-.estimator, -.config, -.metric) %&gt;% \n  relocate(mean)\n#&gt; # A tibble: 5 × 10\n#&gt;     mean  mtry trees min_n tree_depth learn_rate loss_reduction sample_size     n  std_err\n#&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1 0.1172    12   402    17          1   0.1           1    e-10        0.46    10 0.005269\n#&gt; 2 0.1201    15   304    24          9   0.03162       3.162e+ 1        1       10 0.004319\n#&gt; 3 0.1504     5   206    32         15   0.3162        1.995e- 8        0.64    10 0.006183\n#&gt; 4 0.1786     3   108     2          3   0.01          7.943e- 4        0.82    10 0.002977\n#&gt; 5 0.2441     7   500     9         12   0.003162      1.585e- 1        0.1     10 0.001748\nWe will show how to use three iterative search methods.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-sim-anneal",
    "href": "chapters/iterative-search.html#sec-sim-anneal",
    "title": "12  Iterative Search",
    "section": "\n12.2 Simulated Annealing",
    "text": "12.2 Simulated Annealing\nThe finetune package contains finetune::tune_sim_anneal() that can incrementally search the parameter space in a non-greedy way. Its syntax is very similar to tune_grid() with two additional arguments of note:\n\n\ninitial: Either:\n\nAn integer that declares how many points in a space-filling design should be created and evaluated before proceeding.\nAn object from a previous run of tune_grid() or one of the other tune_*() functions.\n\n\n\niter: An integer for the maximum search iterations.\n\nAlso of note is control_sim_anneal(), which helps save additional results and controls logging, and if restarts or early stopping should be used.\nOne important note: the first metric in the metric set guides the optimization. All of the other metric values are recorded for each iteration but only one is used to improve the model fit.\nHere’s some example code:\n\nset.seed(381)\nsa_res &lt;-\n  bst_wflow %&gt;%\n  tune_sim_anneal(\n    resamples = sim_rs,\n    param_info = bst_param,\n    metrics = cls_mtr,\n    # Additional options:\n    initial = initial_res,\n    iter = 50,\n    # Prevent early stopping, save out-of-sample predictions, \n    # and log the process to the console: \n    control = control_sim_anneal(\n      no_improve = Inf,\n      verbose_iter = TRUE,\n      save_pred = TRUE\n    )\n  )\n#&gt; Optimizing brier_class\n#&gt; Initial best: 0.11724\n#&gt; 1 ◯ accept suboptimal  brier_class=0.12584 (+/-0.005567)\n#&gt; 2 ♥ new best           brier_class=0.11088 (+/-0.006034)\n#&gt; 3 ◯ accept suboptimal  brier_class=0.12186 (+/-0.006986)\n#&gt; 4 + better suboptimal  brier_class=0.11355 (+/-0.0077)\n#&gt; 5 + better suboptimal  brier_class=0.11204 (+/-0.005044)\n#&gt; 6 ─ discard suboptimal brier_class=0.18772 (+/-0.006055)\n#&gt; 7 ─ discard suboptimal brier_class=0.14709 (+/-0.004402)\n#&gt; 8 ─ discard suboptimal brier_class=0.15341 (+/-0.006266)\n#&gt; 9 ♥ new best           brier_class=0.10271 (+/-0.005368)\n#&gt; 10 ♥ new best           brier_class=0.098399 (+/-0.005912)\n#&gt; 11 ─ discard suboptimal brier_class=0.11041 (+/-0.006706)\n#&gt; 12 ♥ new best           brier_class=0.089953 (+/-0.004929)\n#&gt; 13 ─ discard suboptimal brier_class=0.094707 (+/-0.005409)\n#&gt; 14 ─ discard suboptimal brier_class=0.10833 (+/-0.006363)\n#&gt; 15 ♥ new best           brier_class=0.088551 (+/-0.005869)\n#&gt; 16 ◯ accept suboptimal  brier_class=0.092395 (+/-0.006208)\n#&gt; 17 + better suboptimal  brier_class=0.091379 (+/-0.006153)\n#&gt; 18 ♥ new best           brier_class=0.080126 (+/-0.005732)\n#&gt; 19 ♥ new best           brier_class=0.078878 (+/-0.005375)\n#&gt; 20 ─ discard suboptimal brier_class=0.088738 (+/-0.00475)\n#&gt; 21 ─ discard suboptimal brier_class=0.088662 (+/-0.004205)\n#&gt; 22 ◯ accept suboptimal  brier_class=0.079888 (+/-0.005168)\n#&gt; 23 ─ discard suboptimal brier_class=0.083144 (+/-0.004481)\n#&gt; 24 ◯ accept suboptimal  brier_class=0.080998 (+/-0.005145)\n#&gt; 25 ─ discard suboptimal brier_class=0.089208 (+/-0.00424)\n#&gt; 26 ─ discard suboptimal brier_class=0.083758 (+/-0.004778)\n#&gt; 27 ✖ restart from best  brier_class=0.080229 (+/-0.005623)\n#&gt; 28 ─ discard suboptimal brier_class=0.07948 (+/-0.005617)\n#&gt; 29 ─ discard suboptimal brier_class=0.088518 (+/-0.004385)\n#&gt; 30 ◯ accept suboptimal  brier_class=0.080155 (+/-0.004802)\n#&gt; 31 ─ discard suboptimal brier_class=0.085765 (+/-0.004087)\n#&gt; 32 ─ discard suboptimal brier_class=0.09274 (+/-0.004826)\n#&gt; 33 ◯ accept suboptimal  brier_class=0.082697 (+/-0.004478)\n#&gt; 34 ─ discard suboptimal brier_class=0.1074 (+/-0.004515)\n#&gt; 35 ✖ restart from best  brier_class=0.095289 (+/-0.004477)\n#&gt; 36 ─ discard suboptimal brier_class=0.08412 (+/-0.005034)\n#&gt; 37 ─ discard suboptimal brier_class=0.082724 (+/-0.004939)\n#&gt; 38 ─ discard suboptimal brier_class=0.081199 (+/-0.006138)\n#&gt; 39 ─ discard suboptimal brier_class=0.084084 (+/-0.005246)\n#&gt; 40 ─ discard suboptimal brier_class=0.082772 (+/-0.005205)\n#&gt; 41 ─ discard suboptimal brier_class=0.082996 (+/-0.004645)\n#&gt; 42 ─ discard suboptimal brier_class=0.081317 (+/-0.004638)\n#&gt; 43 ✖ restart from best  brier_class=0.084592 (+/-0.004923)\n#&gt; 44 ─ discard suboptimal brier_class=0.085653 (+/-0.004442)\n#&gt; 45 ─ discard suboptimal brier_class=0.085816 (+/-0.004273)\n#&gt; 46 ─ discard suboptimal brier_class=0.085154 (+/-0.004607)\n#&gt; 47 ◯ accept suboptimal  brier_class=0.084356 (+/-0.004371)\n#&gt; 48 ─ discard suboptimal brier_class=0.10018 (+/-0.004477)\n#&gt; 49 + better suboptimal  brier_class=0.080555 (+/-0.005406)\n#&gt; 50 ─ discard suboptimal brier_class=0.084699 (+/-0.004644)\n\nThe Brier score has been reduced from the initial value of 0.117 to a new best of 0.079. We’ll estimate:\n\nshow_best(sa_res, metric = \"brier_class\") %&gt;% \n  select(-.estimator, -.config, -.metric) %&gt;% \n  relocate(mean)\n#&gt; # A tibble: 5 × 11\n#&gt;      mean  mtry trees min_n tree_depth learn_rate loss_reduction sample_size     n\n#&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 0.07888    12   243     2          6    0.02247   0.000001299       0.3711    10\n#&gt; 2 0.07948    11   243     3          8    0.02518   0.0000002770      0.4750    10\n#&gt; 3 0.07989    13   228     2          5    0.01815   0.000009236       0.3659    10\n#&gt; 4 0.08013    11   291     2          6    0.04033   0.000001875       0.4259    10\n#&gt; 5 0.08015    14   278     2          4    0.01628   0.000005399       0.3251    10\n#&gt; # ℹ 2 more variables: std_err &lt;dbl&gt;, .iter &lt;int&gt;\n\nThere are several ways to use autoplot() to investigate the results. The default methods plots the metric(s) versus the parameters. Here is it for just the Brier score:\n\nautoplot(sa_res, metric = \"brier_class\")\n\n\n\n\n\n\n\nNext, we can see how the parameter values change over the search by adding type = \"parameters\":\n\nautoplot(sa_res, metric = \"brier_class\", type = \"parameters\")\n\n\n\n\n\n\n\nFinally, a plot of performance metrics can be used via type = \"performance\":\n\nautoplot(sa_res, metric = \"brier_class\", type = \"performance\")\n\n\n\n\n\n\n\nIf we had used control_sim_anneal(save_worflow = TRUE), we could use fit_best() to determine the candidate with the best metric value and then fit that model to the training set.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-genetic-algo",
    "href": "chapters/iterative-search.html#sec-genetic-algo",
    "title": "12  Iterative Search",
    "section": "\n12.3 Genetic Algorithms",
    "text": "12.3 Genetic Algorithms\ntidymodels has no API or function for optimizing models using genetic algorithms. However, there is unsupported code (below) for doing this as long as the tuning parameters are all numeric. We’ll use the GA package for the computations, and this will require:\n\nThe upper and lower bounds of the parameters\nCode to transform the parameter values (if needed)\nA means to resample/evaluate a model on an out-of-sample data set.\nA method to compute a single performance metric such that larger values are more desirable.\n\nTo get started, let’s work with the parameter object named bst_param. We can use purrr::map_dbl() to get vectors of the minimum and maximum values. These should be in the transformed space (if needed):\n\nmin_vals &lt;- map_dbl(bst_param$object, ~ .x$range[[1]])\nmax_vals &lt;- map_dbl(bst_param$object, ~ .x$range[[2]])\n\nThe remainder of the tasks should occur within the GA’s processing. This function shows code with comments to help understand:\n\nyardstick_fitness &lt;- function(values, wflow, param_info, metrics, ...) {\n  # Quietly load required packages if run in parallel\n  shhh &lt;- purrr::quietly(require)\n  loaded &lt;- lapply(c(\"tidymodels\", required_pkgs(wflow)), shhh)\n\n  info &lt;- as_tibble(metrics)\n\n  # Check to see if there are any qualitative parameters and stop if so.\n  qual_check &lt;- map_lgl(param_info$object, ~ inherits(.x, \"qual_param\"))\n  if (any(qual_check)) {\n    cli::cli_abort(\n      \"The function only works for quantitative tuning parameters.\"\n    )\n  }\n\n  # Back-transform parameters if they use a transformation (inputs are in\n  # transformed scales)\n  values &lt;- purrr::map2_dbl(\n    values,\n    param_info$object,\n    ~ dials::value_inverse(.y, .x)\n  )\n\n  # Convert integer parameters to integers\n  is_int &lt;- map_lgl(param_info$object, ~ .x$type == \"integer\")\n  int_param &lt;- param_info$id[is_int]\n  for (i in int_param) {\n    ind &lt;- which(param_info$id == i)\n    values[[ind]] &lt;- floor(values[[ind]])\n  }\n\n  # Convert from vector to a tibble\n  values &lt;- matrix(values, nrow = 1)\n  colnames(values) &lt;- param_info$id\n  values &lt;- as_tibble(values)\n\n  # We could run _populations_ within a generation in parallel. If we do,\n  # let's make sure to turn off parallelization of resamples here:\n  # ctrl &lt;- control_grid(allow_par = FALSE)\n  \n  ctrl &lt;- control_grid()\n\n  # Resample / validate metrics\n  res &lt;- tune_grid(\n    wflow,\n    metrics = metrics,\n    param_info = param_info,\n    grid = values,\n    control = ctrl,\n    ...\n  )\n\n  # Fitness is to be maximized so change direction if needed\n  best_res &lt;- show_best(res, metric = info$metric[1])\n  if (info$direction[1] == \"minimize\") {\n    obj_value &lt;- -best_res$mean\n  } else {\n    obj_value &lt;- best_res$mean\n  }\n  obj_value\n}\n\nNow, let’s initialize the search using a space-filling design (with 10 candidates per population):\n\npop_size &lt;- 10\ngrid_ga &lt;- grid_space_filling(bst_param, size = pop_size, original = FALSE)\n\n# We apply the GA operators on the transformed scale of the parameters (if any).\n# For this example, two use a log-transform: \ngrid_ga$learn_rate &lt;- log10(grid_ga$learn_rate)\ngrid_ga$loss_reduction &lt;- log10(grid_ga$loss_reduction)\n\nNow we can run GA::ga() to begin the process:\n\nset.seed(158)\nga_res &lt;-\n  ga(\n    # ga() options:\n    type = \"real-valued\",\n    fitness = yardstick_fitness,\n    lower = min_vals,\n    upper = max_vals,\n    popSize = pop_size,\n    suggestions = as.matrix(grid_ga),\n    maxiter = 25,\n    # Save the best solutions at each iteration\n    keepBest = TRUE,\n    seed = 39,\n    # Here we can signal to run _populations_ within a generation in parallel\n    parallel = FALSE,\n    # Now options to pass to the `...` in yardstick_fitness()\n    wflow = bst_wflow,\n    param_info = bst_param,\n    metrics = cls_mtr,\n    resamples = sim_rs\n  )\n\nHere is a plot of the best results per population and the mean result (both are Brier scores):\n\n# Negate the fitness value since the Brier score should be minimized.\n-attr(ga_res,\"summary\") %&gt;% \n  as_tibble() %&gt;% \n  mutate(generation = row_number()) %&gt;% \n  select(best = max, mean = mean, generation) %&gt;% \n  pivot_longer(c(best, mean), names_to = \"summary\", values_to = \"fitness\") %&gt;% \n  ggplot(aes(generation, fitness, col = summary, pch = summary)) + \n  geom_point() + \n  labs(x = \"Generation\", y = \"Brier Score (CV)\")\n\n\n\n\n\n\n\nThe best results are in a slot called solution. Let’s remap that to the original parameter values:\n\nga_best &lt;- \n  # There could be multiple solutions for the same fitness; we take the first. \n  ga_res@solution[1,] %&gt;% \n  # Back-transform\n  map2(bst_param$object, ~ value_inverse(.y, .x)) %&gt;% \n  as_tibble() %&gt;% \n  set_names(bst_param$id) %&gt;% \n  # Attach fitness and coerce to integer if needed.\n  mutate(\n    mtry = floor(mtry),\n    trees = floor(trees),\n    min_n = floor(min_n),\n    tree_depth = floor(tree_depth),\n    brier = -ga_res@fitnessValue\n  ) %&gt;% \n  relocate(brier)\n\nga_best\n#&gt; # A tibble: 1 × 8\n#&gt;     brier  mtry trees min_n tree_depth learn_rate loss_reduction sample_size\n#&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 0.07793     9   410     2          6    0.01235   0.0000001197      0.6159",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-bayes-opt",
    "href": "chapters/iterative-search.html#sec-bayes-opt",
    "title": "12  Iterative Search",
    "section": "\n12.4 Bayesian Optimization",
    "text": "12.4 Bayesian Optimization\nNumerous packages use Bayesian optimization:\n\nBayesGP\nBayesGPfit\nFastGP\nGauPro\nGPBayes\nGPfit\n\nand many others. The book Gaussian process modeling, design and optimization for the applied sciences also contains descriptions of many other GO packages.\nCurrently, tidymodels uses GPfit.\nThe tune package contains tune_bayes() for Bayesian optimization. The syntax is identical to what we’ve already seen with tune_sim_anneal().\n\nset.seed(221)\nbo_res &lt;- bst_wflow %&gt;%\n  tune_bayes(\n    resamples = sim_rs,\n    param_info = bst_param,\n    metrics = cls_mtr,\n    # These options work as before: \n    initial = initial_res,\n    iter = 50,\n    control = control_bayes(\n      no_improve = Inf,\n      verbose_iter = TRUE,\n      save_pred = TRUE,\n    )\n  )\n#&gt; ! There are 7 tuning parameters and 6 grid points were requested.\n#&gt; • There are more tuning parameters than there are initial points. This is likely to\n#&gt;   cause numerical issues in the first few search iterations.\n#&gt; Optimizing brier_class using the expected improvement\n#&gt; \n#&gt; ── Iteration 1 ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.1172 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; ! The Gaussian process model is being fit using 7 features but only has 6 data points\n#&gt;   to do so. This may cause errors or a poor model fit.\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=5, trees=489, min_n=26, tree_depth=14, learn_rate=0.0582,\n#&gt;   loss_reduction=9.86e-09, sample_size=0.114\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.2499 (+/-0.000157)\n#&gt; \n#&gt; ── Iteration 2 ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.1172 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; ! The Gaussian process model is being fit using 7 features but only has 7 data points\n#&gt;   to do so. This may cause errors or a poor model fit.\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=3, trees=96, min_n=39, tree_depth=14, learn_rate=0.0768,\n#&gt;   loss_reduction=2.53e-07, sample_size=0.513\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.2189 (+/-0.00405)\n#&gt; \n#&gt; ── Iteration 3 ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.1172 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; ! The Gaussian process model is being fit using 7 features but only has 8 data points\n#&gt;   to do so. This may cause errors or a poor model fit.\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=5, trees=286, min_n=7, tree_depth=1, learn_rate=0.0278, loss_reduction=4.26e-05,\n#&gt;   sample_size=0.432\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ♥ Newest results:    brier_class=0.1153 (+/-0.00444)\n#&gt; \n#&gt; ── Iteration 4 ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.1153 (@iter 3)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=7, trees=338, min_n=24, tree_depth=1, learn_rate=0.00178, loss_reduction=0.108,\n#&gt;   sample_size=0.702\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.2166 (+/-0.00133)\n#&gt; \n#&gt; ── Iteration 5 ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.1153 (@iter 3)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=13, trees=384, min_n=28, tree_depth=11, learn_rate=0.254,\n#&gt;   loss_reduction=0.000924, sample_size=0.936\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.1163 (+/-0.00601)\n#&gt; \n#&gt; ── Iteration 6 ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.1153 (@iter 3)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=7, trees=208, min_n=30, tree_depth=15, learn_rate=0.0206, loss_reduction=3.24,\n#&gt;   sample_size=0.81\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.1245 (+/-0.00436)\n#&gt; \n#&gt; ── Iteration 7 ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.1153 (@iter 3)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=15, trees=229, min_n=6, tree_depth=15, learn_rate=0.266, loss_reduction=2.85,\n#&gt;   sample_size=0.982\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ♥ Newest results:    brier_class=0.08313 (+/-0.00674)\n#&gt; \n#&gt; ── Iteration 8 ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.08313 (@iter 7)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=9, trees=296, min_n=12, tree_depth=15, learn_rate=0.176,\n#&gt;   loss_reduction=9.48e-08, sample_size=0.999\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.09177 (+/-0.00532)\n#&gt; \n#&gt; ── Iteration 9 ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.08313 (@iter 7)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=10, trees=199, min_n=3, tree_depth=14, learn_rate=0.0622, loss_reduction=0.125,\n#&gt;   sample_size=0.959\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ♥ Newest results:    brier_class=0.0824 (+/-0.00654)\n#&gt; \n#&gt; ── Iteration 10 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.0824 (@iter 9)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=3, trees=48, min_n=6, tree_depth=15, learn_rate=0.0012, loss_reduction=9.11e-10,\n#&gt;   sample_size=0.934\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.2413 (+/-0.000398)\n#&gt; \n#&gt; ── Iteration 11 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.0824 (@iter 9)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=12, trees=255, min_n=4, tree_depth=8, learn_rate=0.164, loss_reduction=0.00354,\n#&gt;   sample_size=0.352\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.0903 (+/-0.00684)\n#&gt; \n#&gt; ── Iteration 12 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.0824 (@iter 9)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=14, trees=205, min_n=3, tree_depth=14, learn_rate=0.0699,\n#&gt;   loss_reduction=7.86e-05, sample_size=0.977\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ♥ Newest results:    brier_class=0.08224 (+/-0.00684)\n#&gt; \n#&gt; ── Iteration 13 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.08224 (@iter 12)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=8, trees=357, min_n=5, tree_depth=5, learn_rate=0.308, loss_reduction=4.44e-10,\n#&gt;   sample_size=0.385\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.1102 (+/-0.00737)\n#&gt; \n#&gt; ── Iteration 14 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.08224 (@iter 12)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=15, trees=220, min_n=9, tree_depth=2, learn_rate=0.113, loss_reduction=5.26,\n#&gt;   sample_size=0.405\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.09564 (+/-0.00507)\n#&gt; \n#&gt; ── Iteration 15 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.08224 (@iter 12)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=7, trees=249, min_n=4, tree_depth=8, learn_rate=0.134, loss_reduction=1.86,\n#&gt;   sample_size=0.854\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ♥ Newest results:    brier_class=0.07948 (+/-0.00598)\n#&gt; \n#&gt; ── Iteration 16 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=11, trees=249, min_n=3, tree_depth=6, learn_rate=0.0989,\n#&gt;   loss_reduction=2.55e-10, sample_size=0.896\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08295 (+/-0.00608)\n#&gt; \n#&gt; ── Iteration 17 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=11, trees=244, min_n=4, tree_depth=6, learn_rate=0.0662, loss_reduction=13.2,\n#&gt;   sample_size=0.976\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08762 (+/-0.00476)\n#&gt; \n#&gt; ── Iteration 18 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=12, trees=238, min_n=3, tree_depth=1, learn_rate=0.166, loss_reduction=9.55e-07,\n#&gt;   sample_size=0.966\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08199 (+/-0.00546)\n#&gt; \n#&gt; ── Iteration 19 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=6, trees=239, min_n=5, tree_depth=14, learn_rate=0.188, loss_reduction=3.6e-09,\n#&gt;   sample_size=0.975\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08804 (+/-0.00716)\n#&gt; \n#&gt; ── Iteration 20 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=10, trees=205, min_n=3, tree_depth=2, learn_rate=0.118, loss_reduction=0.37,\n#&gt;   sample_size=0.845\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08471 (+/-0.00557)\n#&gt; \n#&gt; ── Iteration 21 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=13, trees=326, min_n=2, tree_depth=14, learn_rate=0.107, loss_reduction=0.348,\n#&gt;   sample_size=0.822\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08266 (+/-0.00683)\n#&gt; \n#&gt; ── Iteration 22 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=11, trees=285, min_n=4, tree_depth=2, learn_rate=0.164, loss_reduction=7.6,\n#&gt;   sample_size=0.966\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08542 (+/-0.0049)\n#&gt; \n#&gt; ── Iteration 23 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=7, trees=251, min_n=3, tree_depth=2, learn_rate=0.0766, loss_reduction=7.89e-09,\n#&gt;   sample_size=0.919\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08063 (+/-0.00555)\n#&gt; \n#&gt; ── Iteration 24 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=8, trees=213, min_n=2, tree_depth=14, learn_rate=0.128, loss_reduction=2.15e-10,\n#&gt;   sample_size=0.831\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08654 (+/-0.00632)\n#&gt; \n#&gt; ── Iteration 25 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=15, trees=357, min_n=4, tree_depth=8, learn_rate=0.177, loss_reduction=0.00166,\n#&gt;   sample_size=0.901\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08988 (+/-0.00743)\n#&gt; \n#&gt; ── Iteration 26 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=15, trees=203, min_n=2, tree_depth=14, learn_rate=0.183,\n#&gt;   loss_reduction=3.94e-08, sample_size=0.922\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08778 (+/-0.00755)\n#&gt; \n#&gt; ── Iteration 27 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=3, trees=342, min_n=4, tree_depth=2, learn_rate=0.0774, loss_reduction=1.98,\n#&gt;   sample_size=0.748\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08631 (+/-0.00513)\n#&gt; \n#&gt; ── Iteration 28 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=11, trees=327, min_n=7, tree_depth=14, learn_rate=0.103,\n#&gt;   loss_reduction=5.45e-10, sample_size=0.979\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08946 (+/-0.00573)\n#&gt; \n#&gt; ── Iteration 29 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=14, trees=266, min_n=2, tree_depth=6, learn_rate=0.136, loss_reduction=0.000997,\n#&gt;   sample_size=0.853\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08579 (+/-0.00728)\n#&gt; \n#&gt; ── Iteration 30 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=6, trees=317, min_n=9, tree_depth=5, learn_rate=0.13, loss_reduction=1.5,\n#&gt;   sample_size=0.914\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08558 (+/-0.00593)\n#&gt; \n#&gt; ── Iteration 31 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=15, trees=356, min_n=21, tree_depth=7, learn_rate=0.145, loss_reduction=0.122,\n#&gt;   sample_size=0.222\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.2315 (+/-0.0046)\n#&gt; \n#&gt; ── Iteration 32 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=3, trees=29, min_n=5, tree_depth=13, learn_rate=0.13, loss_reduction=18.1,\n#&gt;   sample_size=0.672\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.1201 (+/-0.00443)\n#&gt; \n#&gt; ── Iteration 33 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=4, trees=394, min_n=15, tree_depth=8, learn_rate=0.0998,\n#&gt;   loss_reduction=5.21e-10, sample_size=0.804\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.0944 (+/-0.00564)\n#&gt; \n#&gt; ── Iteration 34 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=13, trees=460, min_n=3, tree_depth=13, learn_rate=0.0819, loss_reduction=0.113,\n#&gt;   sample_size=0.464\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08671 (+/-0.00593)\n#&gt; \n#&gt; ── Iteration 35 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=12, trees=290, min_n=4, tree_depth=15, learn_rate=0.137, loss_reduction=14,\n#&gt;   sample_size=0.542\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.09203 (+/-0.00477)\n#&gt; \n#&gt; ── Iteration 36 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=14, trees=99, min_n=15, tree_depth=12, learn_rate=0.11, loss_reduction=2.86e-09,\n#&gt;   sample_size=0.967\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08659 (+/-0.00509)\n#&gt; \n#&gt; ── Iteration 37 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=9, trees=268, min_n=3, tree_depth=6, learn_rate=0.109, loss_reduction=0.297,\n#&gt;   sample_size=0.437\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08782 (+/-0.00678)\n#&gt; \n#&gt; ── Iteration 38 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=13, trees=34, min_n=2, tree_depth=1, learn_rate=0.143, loss_reduction=0.0201,\n#&gt;   sample_size=0.995\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.1141 (+/-0.00432)\n#&gt; \n#&gt; ── Iteration 39 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=10, trees=225, min_n=13, tree_depth=11, learn_rate=0.0965,\n#&gt;   loss_reduction=3.44e-06, sample_size=0.925\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08963 (+/-0.00498)\n#&gt; \n#&gt; ── Iteration 40 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=9, trees=17, min_n=2, tree_depth=13, learn_rate=0.146, loss_reduction=0.026,\n#&gt;   sample_size=0.359\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.09176 (+/-0.00485)\n#&gt; \n#&gt; ── Iteration 41 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=13, trees=157, min_n=17, tree_depth=11, learn_rate=0.314,\n#&gt;   loss_reduction=6.95e-08, sample_size=0.957\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.09725 (+/-0.00662)\n#&gt; \n#&gt; ── Iteration 42 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=7, trees=313, min_n=2, tree_depth=6, learn_rate=0.127, loss_reduction=0.109,\n#&gt;   sample_size=0.813\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.09222 (+/-0.00767)\n#&gt; \n#&gt; ── Iteration 43 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=12, trees=493, min_n=12, tree_depth=13, learn_rate=0.0622,\n#&gt;   loss_reduction=0.000261, sample_size=0.637\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.094 (+/-0.00595)\n#&gt; \n#&gt; ── Iteration 44 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=7, trees=18, min_n=32, tree_depth=13, learn_rate=0.089, loss_reduction=8.1e-09,\n#&gt;   sample_size=0.995\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.1396 (+/-0.00449)\n#&gt; \n#&gt; ── Iteration 45 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=7, trees=187, min_n=9, tree_depth=15, learn_rate=0.114, loss_reduction=0.00238,\n#&gt;   sample_size=0.868\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08754 (+/-0.00559)\n#&gt; \n#&gt; ── Iteration 46 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=10, trees=11, min_n=3, tree_depth=4, learn_rate=0.197, loss_reduction=3.75e-10,\n#&gt;   sample_size=0.137\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.1147 (+/-0.00564)\n#&gt; \n#&gt; ── Iteration 47 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=9, trees=432, min_n=6, tree_depth=9, learn_rate=0.0675, loss_reduction=6.32e-09,\n#&gt;   sample_size=0.521\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08992 (+/-0.00517)\n#&gt; \n#&gt; ── Iteration 48 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=15, trees=443, min_n=4, tree_depth=15, learn_rate=0.0621, loss_reduction=0.682,\n#&gt;   sample_size=0.838\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.08458 (+/-0.0066)\n#&gt; \n#&gt; ── Iteration 49 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=8, trees=482, min_n=9, tree_depth=2, learn_rate=0.0937, loss_reduction=0.00332,\n#&gt;   sample_size=0.534\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.09804 (+/-0.00584)\n#&gt; \n#&gt; ── Iteration 50 ─────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; i Current best:      brier_class=0.07948 (@iter 15)\n#&gt; i Gaussian process model\n#&gt; ✓ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i mtry=15, trees=71, min_n=21, tree_depth=3, learn_rate=0.064, loss_reduction=1.1e-10,\n#&gt;   sample_size=0.858\n#&gt; i Estimating performance\n#&gt; ✓ Estimating performance\n#&gt; ⓧ Newest results:    brier_class=0.1003 (+/-0.00459)\n\nThe same helper functions are used to interrogate the results and to create diagnostic plots:\n\nshow_best(bo_res, metric = \"brier_class\") %&gt;% \n  select(-.estimator, -.config, -.metric) %&gt;% \n  relocate(mean)\n#&gt; # A tibble: 5 × 11\n#&gt;      mean  mtry trees min_n tree_depth learn_rate loss_reduction sample_size     n\n#&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 0.07948     7   249     4          8    0.1343        1.859e+0      0.8543    10\n#&gt; 2 0.08063     7   251     3          2    0.07663       7.895e-9      0.9194    10\n#&gt; 3 0.08199    12   238     3          1    0.1658        9.551e-7      0.9661    10\n#&gt; 4 0.08224    14   205     3         14    0.06994       7.861e-5      0.9767    10\n#&gt; 5 0.08240    10   199     3         14    0.06220       1.247e-1      0.9593    10\n#&gt; # ℹ 2 more variables: std_err &lt;dbl&gt;, .iter &lt;int&gt;\n\nThese results are about the same as those of the SA search. We can plot the data and see that some parameters (number of trees, learning rate, minimum node size, and the sampling proportion) appear to converge to specific values:\n\nautoplot(bo_res, metric = \"brier_class\")\n\n\n\n\n\n\n\nHere we see that the learning rate and the minumum node size reach a steady-state:\n\nautoplot(bo_res, metric = \"brier_class\", type = \"parameters\")\n\n\n\n\n\n\n\nA plot of the overall progress:\n\nautoplot(bo_res, metric = \"brier_class\", type = \"performance\")\n\n\n\n\n\n\n\nOther packages use Bayesian optimization:\n\nmlr3mbo\nmlrMBO\nParBayesianOptimization\nrBayesianOptimization",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html",
    "href": "chapters/feature-selection.html",
    "title": "13  Feature Selection",
    "section": "",
    "text": "13.1 Requirements\nThe book’s Feature Selection chapter focuses on different methods to reduce the number of predictors used by the model\nWe will use the following package in this chapter: You’ll need 9 packages (bestNormalize, future.mirai, janitor, kernlab, partykit, QSARdata, ranger, rpart, and tidymodels) for this chapter. You can install them via:. To install them:\nreq_pkg &lt;- c(\"bestNormalize\", \"future.mirai\", \"janitor\", \"kernlab\", \"partykit\", \n             \"QSARdata\", \"ranger\", \"rpart\", \"tidymodels\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\n\n# For coliono, install from GitHub\npak::pak(\"stevenpawley/colino\")\nWe’ll demonstrate these tools using the phospholipidosis (PLD) data set from the QSARdata package. These data are an example of a data set used in drug discovery to help predict when compounds have unwanted side effects (e.g., toxicity).\nWe’ll consider two predictor sets. One consists of a series of descriptors of molecules (such as size or weight) and a set of predictors that are binary indicators for important sub-structures in the equations that make up the molecules. The response has two classes (“clean” and toxic outcomes).\nEven before initial splitting, there are more predictors (1345) than data points (324), making feature selection an essential task for these data.\nLet’s load the meta package, manage some between-package function conflicts, and initialize parallel processing:\nlibrary(tidymodels)\nlibrary(colino)\nlibrary(bestNormalize)\nlibrary(future.mirai)\nlibrary(janitor)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\nplan(mirai_multisession)\nThere are a few different data frames for these data. We’ll merge two and clean up and shorten the variable names, use a 4:1 split of the data, and then initialize multiple repeats of 10-fold cross-validation:\ndata(PLD, package = \"QSARdata\")\n\ndrug_data &lt;- \n  # Merge the outcome data with a subset of possible predictors.\n  PLD_Outcome %&gt;% \n  full_join(PLD_VolSurfPlus, by = \"Molecule\") %&gt;% \n  full_join(PLD_PipelinePilot_FP %&gt;% select(1, contains(\"FCFP\")), \n            by = \"Molecule\") %&gt;% \n  select(-Molecule) %&gt;% \n  clean_names() %&gt;% \n  as_tibble() %&gt;% \n  # Make shorter names:\n  rename_with(~ gsub(\"vol_surf_plus_\", \"\", .x), everything()) %&gt;% \n  rename_with(~ gsub(\"ppfp_fcfp_\", \"fp_\", .x), everything())\n\nset.seed(106)\ndrug_split &lt;- initial_split(drug_data, prop = 0.8, strata = class)\ndrug_train &lt;- training(drug_split)\ndrug_test &lt;- testing(drug_split)\ndrug_rs &lt;- vfold_cv(drug_train, repeats = 10, strata = class)\nThere is a class imbalance for these data:\ndim(drug_train)\n#&gt; [1]  259 1346\ndrug_train %&gt;% count(class)\n#&gt; # A tibble: 2 × 2\n#&gt;   class          n\n#&gt;   &lt;fct&gt;      &lt;int&gt;\n#&gt; 1 inducer       99\n#&gt; 2 noninducer   160\nThe level “inducer” indicates that the molecule has been proven to cause phospholipidosis.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-unsupervised-selection",
    "href": "chapters/feature-selection.html#sec-unsupervised-selection",
    "title": "13  Feature Selection",
    "section": "\n13.2 Unsupervised Selection",
    "text": "13.2 Unsupervised Selection\nIn tidymodels, most preprocessing methods for feature selection are recipe steps.\nThere are several unsupervised feature filters in recipes:\n\n\nstep_zv(): Removes predictors with a single value.\n\nstep_nzv(freq_cut = double(1), unique_cut = double(1)): Removes predictors that have a few unique values that are out of the mainstream.\n\nstep_corr(threshold = double(1)): Reduces the pairwise correlations between predictors.\n\nstep_lincomb(): Eliminates strict linear dependencies between predictors.\n\nstep_filter_missing(threshold = double(1)): Remove predictors that have too many missing values.\n\nThe textrecipes package also has a few unsupervised methods for screening tokens (i.e., words):\n\n\nstep_tokenfilter(): filter tokens based on term frequency.\n\nstep_stopwords(): Removes top words (e.g., “and”, “or”, etc.)\n\nstep_pos_filter(): Part of speech filtering of tokens.\n\nIt is suggested that these steps occur early in a recipe, perhaps after any imputation methods.\nWe suggest adding these to a recipe early, after any imputation methods.\nFor the drug toxicity data, we can visualize the amount of predictor correlations by first computing the correlation matrix:\n\ncor_mat &lt;- \n  drug_train %&gt;% \n  select(-class) %&gt;% \n  cor()\n#&gt; Warning in cor(.): the standard deviation is zero\n\nNote the warning: some columns have a single unique value. Let’s look at this across the entire set of predictors via a function in the vctrs package:\n\nnum_unique &lt;- map_int(drug_train %&gt;% select(-class), vctrs::vec_unique_count)\nnames(num_unique)[num_unique == 1]\n#&gt;  [1] \"fp_0167\" \"fp_0172\" \"fp_0174\" \"fp_0178\" \"fp_0181\" \"fp_0182\" \"fp_0183\" \"fp_0191\"\n#&gt;  [9] \"fp_0193\" \"fp_0195\" \"fp_0198\" \"fp_0200\" \"fp_0204\" \"fp_0205\" \"fp_0206\" \"fp_0212\"\n\nWe’ll start a recipe with the step that will eliminate these:\n\ndrug_rec &lt;- \n  recipe(class ~ ., data = drug_train) %&gt;% \n  step_zv(all_predictors())\n\nReturning to correlations, let’s plot the distribution of pairwise correlation between predictors:\n\npairwise_cor &lt;- cor_mat[upper.tri(cor_mat)]\n\nsum(abs(pairwise_cor) &gt;= 3 / 4, na.rm = TRUE)\n#&gt; [1] 3922\n\ntibble(correlation = pairwise_cor) %&gt;% \n  filter(!is.na(correlation)) %&gt;% \n  ggplot(aes(x = correlation)) + \n  geom_histogram(binwidth = 0.1, col = \"white\")\n\n\n\n\n\n\n\nThis isn’t too bad, but if we wanted to reduce the extreme pairwise correlations, we could use:\n\ndrug_rec %&gt;% \n  step_corr(all_predictors(), threshold = 0.75)\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:      1\n#&gt; predictor: 1345\n#&gt; \n#&gt; ── Operations\n#&gt; • Zero variance filter on: all_predictors()\n#&gt; • Correlation filter on: all_predictors()\n\nor search for an optimal cutoff using threshold = tune() (as we will below).",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-automatic-selection",
    "href": "chapters/feature-selection.html#sec-automatic-selection",
    "title": "13  Feature Selection",
    "section": "\n13.3 Automatic Selection",
    "text": "13.3 Automatic Selection\nThe text mentioned that there are types of models that automatically select predictors. Tree-based models typically fall into this category.\nTo demonstrate, let’s fit a Classification and Regression Tree (CART) to the training set and see how many predictors are removed.\nBefore doing so, let’s turn off a feature of this model. CART computes special alternate splits during training (“surrogate” and “competing” splits) to aid with things like missing value imputation. We’ll use the built-in feature importance measure to see how many predictors were used. Unfortunately, those measures will include splits not actually used by the model, so we prohibit these from being listed using rpart.control().\nWe can pass that to the model fit when we set the engine:\n\ncart_ctrl &lt;- rpart::rpart.control(maxcompete = 0, maxsurrogate = 0)\n\ncart_spec &lt;- \n  decision_tree(mode = \"classification\") %&gt;% \n  set_engine(\"rpart\", control = !!cart_ctrl)\n\nNote the use of !! (“bang-bang”) when adding cart_ctrl as an engine option. If we had just used control = cart_ctrl, it tells R to look for a reference to object “cart_ctrl”, which resides in the global environment. Ordinarily, that works fine. However, if we use parallel processing, that reference is not available to the worker processes, and an error will occur.\nUsing the bang-bang operator, we replace the reference to “cart_ctrl” with the actual value of that object. It splices the actual data into the model specification so parallel workers can find it. Here’s the model fit:\n\ncart_drug_fit &lt;- cart_spec %&gt;% fit(class ~ ., data = drug_train)\ncart_drug_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; n= 259 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 259 99 noninducer (0.38224 0.61776)  \n#&gt;    2) l1lg_s&lt; -2.105 103 30 inducer (0.70874 0.29126)  \n#&gt;      4) logp_c_hex&gt;=1.023 70 10 inducer (0.85714 0.14286)  \n#&gt;        8) cw7&lt; 0.01674 41  1 inducer (0.97561 0.02439) *\n#&gt;        9) cw7&gt;=0.01674 29  9 inducer (0.68966 0.31034)  \n#&gt;         18) w6&gt;=33.5 13  0 inducer (1.00000 0.00000) *\n#&gt;         19) w6&lt; 33.5 16  7 noninducer (0.43750 0.56250) *\n#&gt;      5) logp_c_hex&lt; 1.023 33 13 noninducer (0.39394 0.60606)  \n#&gt;       10) fp_0048&gt;=0.5 9  0 inducer (1.00000 0.00000) *\n#&gt;       11) fp_0048&lt; 0.5 24  4 noninducer (0.16667 0.83333)  \n#&gt;         22) d8&gt;=4.5 7  3 inducer (0.57143 0.42857) *\n#&gt;         23) d8&lt; 4.5 17  0 noninducer (0.00000 1.00000) *\n#&gt;    3) l1lg_s&gt;=-2.105 156 26 noninducer (0.16667 0.83333)  \n#&gt;      6) cw7&lt; 0.002635 9  3 inducer (0.66667 0.33333) *\n#&gt;      7) cw7&gt;=0.002635 147 20 noninducer (0.13605 0.86395)  \n#&gt;       14) vd&gt;=0.4691 8  3 inducer (0.62500 0.37500) *\n#&gt;       15) vd&lt; 0.4691 139 15 noninducer (0.10791 0.89209) *\n\nOf the 1345 predictors, only 7 were actually part of the prediction equations. The partykit package has a nice plot method to visualize the tree:\n\nlibrary(partykit)\n\ncart_drug_party &lt;- \n  cart_drug_fit %&gt;% \n  extract_fit_engine() %&gt;% \n  as.party()\nplot(cart_drug_party)\n\n\n\n\n\n\n\nAs previously mentioned, trees produced by the rpart package have an internal importance score. To return this, let’s write a small function to pull the rpart object out, extract the importance scores, and then return a data frame with that data:\n\nget_active_features &lt;- function(x) {\n  require(tidymodels)\n  x %&gt;% \n    extract_fit_engine() %&gt;% \n    pluck(\"variable.importance\") %&gt;% \n    enframe() %&gt;% \n    setNames(c(\"predictor\", \"importance\"))\n}\n\nget_active_features(cart_drug_fit) \n#&gt; # A tibble: 7 × 2\n#&gt;   predictor  importance\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;\n#&gt; 1 l1lg_s         36.46 \n#&gt; 2 logp_c_hex      9.624\n#&gt; 3 fp_0048         9.091\n#&gt; 4 cw7             7.553\n#&gt; 5 w6              4.539\n#&gt; 6 vd              4.045\n#&gt; # ℹ 1 more row\n\nThis shows us the 7 predictors used, along with their relative effect on the model.\nThese results show what happens with the training set, but would a predictor like l1lg_s be consistently selected?\nTo determine this, we can resample the model and save the importance scores for each of the 100 analysis sets. Let’s take the get_active_features() function and add it to a different control function that will be executed during resampling:\n\nctrl &lt;- control_resamples(extract = get_active_features)\ncart_drug_res &lt;- \n  cart_spec %&gt;% \n  fit_resamples(\n    class ~ ., \n    resamples = drug_rs, \n    control = ctrl\n  )\n\nOur results will have an extra column called .extract that contains the results for the resample. Since we didn’t tune this model, .extract contains a simple tibble with the results:\n\ncart_drug_res$.extracts[[1]]\n#&gt; # A tibble: 1 × 2\n#&gt;   .extracts        .config             \n#&gt;   &lt;list&gt;           &lt;chr&gt;               \n#&gt; 1 &lt;tibble [7 × 2]&gt; Preprocessor1_Model1\n\ncart_drug_res$.extracts[[1]]$.extracts\n#&gt; [[1]]\n#&gt; # A tibble: 7 × 2\n#&gt;   predictor  importance\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;\n#&gt; 1 vd             35.40 \n#&gt; 2 logp_c_hex      9.001\n#&gt; 3 l1lg_s          7.934\n#&gt; 4 aus74           3.75 \n#&gt; 5 cw1             3.438\n#&gt; 6 fp_0027         3.048\n#&gt; # ℹ 1 more row\n\nWe can extract the results from all the resamples, unnest, and count the number of times each predictor was selected:\n\nresampled_selection &lt;- \n  cart_drug_res %&gt;% \n  collect_extracts() %&gt;% \n  unnest(.extracts) %&gt;% \n  count(predictor) %&gt;%\n  arrange(desc(n))\n\nresampled_selection %&gt;% slice_head(n = 5)\n#&gt; # A tibble: 5 × 2\n#&gt;   predictor      n\n#&gt;   &lt;chr&gt;      &lt;int&gt;\n#&gt; 1 l1lg_s        86\n#&gt; 2 vd            58\n#&gt; 3 logp_c_hex    57\n#&gt; 4 cw7           44\n#&gt; 5 iw4           43\n\nA visualization illustrates that a small number of predictors were reliably selected:\n\nresampled_selection %&gt;% \n  ggplot(aes(n)) +\n  geom_histogram(binwidth = 2, col = \"white\") +\n  labs(x = \"# Times Selected (of 100)\")\n\n\n\n\n\n\n\nWe can also see the model’s performance characteristics:\n\ncollect_metrics(cart_drug_res)\n#&gt; # A tibble: 3 × 6\n#&gt;   .metric     .estimator   mean     n  std_err .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary     0.7406   100 0.007049 Preprocessor1_Model1\n#&gt; 2 brier_class binary     0.2001   100 0.005307 Preprocessor1_Model1\n#&gt; 3 roc_auc     binary     0.7559   100 0.008105 Preprocessor1_Model1\n\nOne additional note about using tree-based models to automatically select predictors. Many tree ensembles create a collection of individual tree models. For ensembles to work well, this collection should have a diverse set of trees (rather than those with the same splits). To encourage diversity, many tree models have an mtry parameter. This parameter is an integer for the number of predictors in the data set that should be randomly selected when making a split. For example, if mtry = 3, a different random selection of three predictors would be the only ones considered for each split in the tree. This facilitates diversity but also forces irrelevant predictors to be included in the model.\nHowever, this also means that many tree ensembles will have prediction functions that include predictors that have no effect. If we take the same strategy as above, we will vastly overestimate the number of predictors that affect the model.\nFor this reason, we might consider setting mtry to use the complete predictor set during splitting if we are trying to select predictors. While this might slightly decrease the model’s performance, the false positive rate of finding “important predictors” will be significantly reduced.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-wrappers",
    "href": "chapters/feature-selection.html#sec-wrappers",
    "title": "13  Feature Selection",
    "section": "\n13.4 Wrapper Methods",
    "text": "13.4 Wrapper Methods\ntidymodels does not contain any wrapper methods, primarily due to their computational costs.\nSeveral other packages do, most notably caret. For more information on what that package can do, see the feature selection chapters of the documentation:\n\nFeature Selection Overview\nFeature Selection using Univariate Filters\nRecursive Feature Elimination\nFeature Selection using Genetic Algorithms\nFeature Selection using Simulated Annealing\n\nR code from the Feature Engineering and Selection book can also be found at https://github.com/topepo/FES.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-filters",
    "href": "chapters/feature-selection.html#sec-filters",
    "title": "13  Feature Selection",
    "section": "\n13.5 Filter Methods",
    "text": "13.5 Filter Methods\nCurrently, the majority of supervised filters live in the colino package (although this will change in the Autumn of 2025). Those steps include:\n\n\nstep_select_aov(): filter categorical predictors using the ANOVA F-test.\n\nstep_select_boruta(): feature selection step using the Boruta algorithm (pdf).\n\nstep_select_carscore(): feature selection step using CAR scores.\n\nstep_select_fcbf(): fast correlation-based filter.\n\nstep_select_forests(): feature selection step using random forest feature importance scores.\n\nstep_select_infgain(): information gain feature selection step.\n\nstep_select_linear(): feature selection step using the magnitude of a linear models’ coefficients.\n\nstep_select_mrmr(): apply minimum redundancy maximum relevance feature selection (MRMR).\n\nstep_select_relief(): feature selection step using the Relief algorithm.\n\nstep_select_roc(): filter numeric predictors using ROC curve.\n\nstep_select_tree(): feature selection step using a decision tree importance scores.\n\nstep_select_vip(): feature selection step using a model’s feature importance scores or coefficients.\n\nstep_select_xtab(): filter categorical predictors using contingency tables.\n\nThese steps contain tuning parameters that control how many predictors to retain:\n\n\ntop_n specifies the number to retain while\n\nthreshold describes the cut-point for the metric being used to filter\n\nLet’s add a supervised filter based on the popular random forest importance scores to demonstrate. The step requires a model declaration via a parsnip specification. We’ll choose random forest model and optimize the number of top predictors that should be retained and then given to the model.\n\nbase_model &lt;- \n  rand_forest(trees = 1000, mode = \"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\")\n\ndrug_rec &lt;- \n  recipe(class ~ ., data = drug_train) %&gt;% \n  step_zv(all_predictors(), id = \"zv\") %&gt;% \n  step_corr(all_numeric_predictors(), threshold = tune(), id = \"cor\")  %&gt;%\n  step_select_vip(\n    all_numeric_predictors(),\n    outcome = \"class\",\n    model = base_model,\n    top_p = tune(),\n    id = \"vip\"\n  ) %&gt;%\n  step_orderNorm(all_numeric_predictors())\n\nNote that we also add a correlation filter and optimize the exclusion threshold. This helps the random forest model since the inclusion of highly correlated predictors can dilute the importance of the set of related predictors.\nWe’ll fit a support vector machine model to these data, so the recipe concludes with a step that will normalize the features to have the same distribution (even the binary values).\nNow we can specify the supervised model, tag two parameters for optimization, and then add the model and recipe to a workflow:\n\nsvm_spec &lt;- \n  svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;% \n  set_mode(\"classification\")\n\nvip_svm_wflow &lt;- workflow(drug_rec, svm_spec)\n\nLet’s add specific ranges for the supervised filter parameter since its upper range depends on the data dimensions. We’re not sure how many predictors will pass the unsupervised filter steps, but we’ll guess that we should include, at most, 100 predictors. If we overestimate this number, step_select_vip() will adjust the range to the upper limit.\nWe’ll also adjust the range of the correlation filter to make it more aggressively remove highly correlated predictors:\n\nvip_svm_param &lt;- \n  vip_svm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  update(\n  top_p = top_p(c(1L, 100L)), \n  threshold = threshold(c(0.50, 0.99))\n  )\n\nFinally, let’s tune the model via Bayesian optimization and use the Brier score to guide the process to the best values of the tuning parameters:\n\nctrl &lt;- control_bayes(no_improve = Inf, parallel_over = \"everything\")\n\nvip_svm_res &lt;- \n  vip_svm_wflow %&gt;% \n  tune_bayes(\n    resamples = drug_rs,\n    metrics = metric_set(brier_class, roc_auc),\n    initial = 10L,\n    iter = 25L,\n    control = ctrl,\n    param_info = vip_svm_param\n  )\n\nA visualization of the process shows that the search does reduce the Brier score during the search:\n\nautoplot(vip_svm_res, metric = \"brier_class\", type = \"performance\")\n\n\n\n\n\n\n\nWhen we plot the parameter choices over iterations, we see that each tuning parameter converges to specific ranges. The number of predictors retained fluctuates, and a few choices could be used (say, between 5 and 15 predictors).\n\nautoplot(vip_svm_res, type = \"parameters\")\n\n\n\n\n\n\n\nA plot of the parameter values versus the Brier score tells a similar story:\n\nautoplot(vip_svm_res, metric = \"brier_class\")\n\n\n\n\n\n\n\nThe numerically best results are:\n\nshow_best(vip_svm_res, metric = \"brier_class\")\n#&gt; # A tibble: 5 × 11\n#&gt;    cost rbf_sigma threshold top_p .metric   .estimator   mean     n  std_err .config\n#&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n#&gt; 1 30.67 0.0005070    0.9665    35 brier_cl… binary     0.1331   100 0.003508 Iter10 \n#&gt; 2 29.89 0.001532     0.9811    75 brier_cl… binary     0.1349   100 0.003652 Iter11 \n#&gt; 3 27.73 0.0002266    0.8979    50 brier_cl… binary     0.1357   100 0.003505 Iter24 \n#&gt; 4 23.13 0.0003069    0.8237    38 brier_cl… binary     0.1359   100 0.003820 Iter15 \n#&gt; 5 23.82 0.0008604    0.9667    80 brier_cl… binary     0.1382   100 0.003489 Iter12 \n#&gt; # ℹ 1 more variable: .iter &lt;int&gt;\n\nbest_param &lt;- select_best(vip_svm_res, metric = \"brier_class\")\n\nLet’s update our workflow with the best parameters, then fit the final model on the entire training set:\n\nset.seed(124)\nfinal_model &lt;- \n  vip_svm_wflow %&gt;% \n  finalize_workflow(best_param) %&gt;% \n  fit(drug_train)\nfinal_model\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: svm_rbf()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────────────\n#&gt; 4 Recipe Steps\n#&gt; \n#&gt; • step_zv()\n#&gt; • step_corr()\n#&gt; • step_select_vip()\n#&gt; • step_orderNorm()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────────────\n#&gt; Support Vector Machine object of class \"ksvm\" \n#&gt; \n#&gt; SV type: C-svc  (classification) \n#&gt;  parameter : cost C = 30.6666439770923 \n#&gt; \n#&gt; Gaussian Radial Basis kernel function. \n#&gt;  Hyperparameter : sigma =  0.000507017083942551 \n#&gt; \n#&gt; Number of Support Vectors : 126 \n#&gt; \n#&gt; Objective Function Value : -3333 \n#&gt; Training error : 0.142857 \n#&gt; Probability model included.\n\nHow many predictors were removed, and how many made it to the final model? We can write a function to use the tidy() method on the recipe steps to assess what was eliminated. The “mold” for the workflow can also tell us how many predictors were passed to the SVM model:\n\nget_filter_info &lt;- function(x) {\n  fit_rec &lt;- extract_recipe(x)\n  # The tidy methods show the predictors that were eliminated:\n  corr_rm &lt;- nrow(tidy(fit_rec, id = \"cor\"))\n  zv_rm &lt;- nrow(tidy(fit_rec, id = \"zv\"))\n  vip_rm &lt;- nrow(tidy(fit_rec, id = \"vip\"))\n  \n  # The mold has a 'predictors' element that describes the\n  # columns which are given to the model: \n  kept &lt;- \n    x %&gt;% \n    extract_mold() %&gt;% \n    pluck(\"predictors\") %&gt;% \n    ncol()\n  # We'll save them as a tibble:\n  tibble(corr_rm, zv_rm, vip_rm, kept)\n}\n\nget_filter_info(final_model)\n#&gt; # A tibble: 1 × 4\n#&gt;   corr_rm zv_rm vip_rm  kept\n#&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;\n#&gt; 1     682    16    612    35\n\nThe correlation filter removes a large number of predictors, which is not surprising for this type of data set.\nHow does the model work on the test set of 65 molecules?\n\ntest_pred &lt;- augment(final_model, drug_test)\ntest_pred %&gt;% brier_class(class, .pred_inducer)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary        0.1178\ntest_pred %&gt;% roc_auc(class, .pred_inducer)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.899",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html",
    "href": "chapters/comparing-models.html",
    "title": "14  Comparing Models",
    "section": "",
    "text": "14.1 Requirements\nThis book’s chapter involved taking models that have been fit or resampled and using their results to formally compare them.\nYou’ll need 14 packages (bestNormalize, broom.mixed, C50, discrim, embed, glmnet, klaR, lme4, multcomp, rstanarm, rules, splines2, tidymodels, and tidyposterior) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"bestNormalize\", \"broom.mixed\", \"C50\", \"discrim\", \"embed\", \n             \"glmnet\", \"klaR\", \"lme4\", \"multcomp\", \"rstanarm\", \"rules\", \"splines2\", \n             \"tidymodels\", \"tidyposterior\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the meta package and manage some between-package function conflicts. We’ll also load some packages that are used for the data analysis.\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())\n\n# Other packages that we will load below\n\n# For feature engineering:\nlibrary(embed)\nlibrary(bestNormalize)\n\n# For models:\nlibrary(discrim)\nlibrary(rules)\n\n# For Frequentist analysis:\nlibrary(lme4)\nlibrary(broom.mixed)\nlibrary(multcomp)\n\n# For Bayesian analysis: \nlibrary(tidyposterior)",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#sec-forested-compare-setup",
    "href": "chapters/comparing-models.html#sec-forested-compare-setup",
    "title": "14  Comparing Models",
    "section": "\n14.2 Example Data",
    "text": "14.2 Example Data\nWe’ll use the forestation data just as in the book. These data have already been split, and some interactions have been assessed. Those are captured in two remote RData files:\n\n# Loads the training and test set data\nload(url(\"https://github.com/aml4td/website/raw/refs/heads/main/RData/forested_data.RData\"))\n\n# Load information needed for interactions:\nload(url(\"https://github.com/aml4td/website/raw/refs/heads/main/RData/forested_interactions.RData\"))\n\nThe first file contains these relevant objects:\n\nforested_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;4832/1371/6203&gt;\n\nforested_train\n#&gt; # A tibble: 4,832 × 17\n#&gt;   class  year elevation eastness northness roughness dew_temp precip_annual\n#&gt; * &lt;fct&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 Yes    2005       113      -25        96        30     6.4           1710\n#&gt; 2 No     2005       164      -84        53        13     6.06          1297\n#&gt; 3 Yes    2005       299       93        34         6     4.43          2545\n#&gt; 4 Yes    2005       806       47       -88        35     1.06           609\n#&gt; 5 Yes    2005       736      -27       -96        53     1.35           539\n#&gt; 6 Yes    2005       636      -48        87         3     1.42           702\n#&gt; # ℹ 4,826 more rows\n#&gt; # ℹ 9 more variables: temp_annual_mean &lt;dbl&gt;, temp_annual_min &lt;dbl&gt;,\n#&gt; #   temp_annual_max &lt;dbl&gt;, temp_january_min &lt;dbl&gt;, vapor_min &lt;dbl&gt;,\n#&gt; #   vapor_max &lt;dbl&gt;, county &lt;fct&gt;, longitude &lt;dbl&gt;, latitude &lt;dbl&gt;\n\nforested_test\n#&gt; # A tibble: 1,371 × 17\n#&gt;   class  year elevation eastness northness roughness dew_temp precip_annual\n#&gt; * &lt;fct&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 No     2014       308      -70       -70         4     3.07           233\n#&gt; 2 No     2014       119       94        31         2     3.74           200\n#&gt; 3 No     2014       795      -74        66        39     0.17           255\n#&gt; 4 Yes    2014      1825       99        -9        78    -3.37          1241\n#&gt; 5 No     2014       126       83       -55         6     4.08           234\n#&gt; 6 Yes    2014      1056       99         7        57    -1.6            557\n#&gt; # ℹ 1,365 more rows\n#&gt; # ℹ 9 more variables: temp_annual_mean &lt;dbl&gt;, temp_annual_min &lt;dbl&gt;,\n#&gt; #   temp_annual_max &lt;dbl&gt;, temp_january_min &lt;dbl&gt;, vapor_min &lt;dbl&gt;,\n#&gt; #   vapor_max &lt;dbl&gt;, county &lt;fct&gt;, longitude &lt;dbl&gt;, latitude &lt;dbl&gt;\n\nforested_rs\n#&gt; # A tibble: 10 × 2\n#&gt;   splits             id    \n#&gt;   &lt;list&gt;             &lt;chr&gt; \n#&gt; 1 &lt;split [4127/489]&gt; Fold01\n#&gt; 2 &lt;split [4080/491]&gt; Fold02\n#&gt; 3 &lt;split [4074/505]&gt; Fold03\n#&gt; 4 &lt;split [4185/434]&gt; Fold04\n#&gt; 5 &lt;split [4116/475]&gt; Fold05\n#&gt; 6 &lt;split [4049/523]&gt; Fold06\n#&gt; # ℹ 4 more rows\n\nThe second file has this important formula object that contains what we think are important interactions:\n\nforested_int_form\n#&gt; ~dew_temp:elevation + dew_temp:temp_annual_max + dew_temp:temp_january_min + \n#&gt;     eastness:vapor_max + temp_annual_max:vapor_max",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#sec-forested-model-fits",
    "href": "chapters/comparing-models.html#sec-forested-model-fits",
    "title": "14  Comparing Models",
    "section": "\n14.3 Model Fitting",
    "text": "14.3 Model Fitting\nWe’ll need to resample a few models. We’ll cut to the chase and resample the tuning parameters found to be optimal in the regular text.\nFirst, we load some additional packages and create some preliminaries:\n\n# For resampling the models\nctrl_rs &lt;-\n  control_resamples(\n    save_pred = TRUE,\n    parallel_over = \"everything\",\n    save_workflow = TRUE # Keep this for as_workflow_set()\n  )\n\ncls_mtr &lt;- metric_set(accuracy)\n\nWe start by resampling the boosted tree with the tuning parameter values that were found during the grid search:\n\nboost_spec &lt;- \n  C5_rules(trees = 60, min_n = 20) |&gt; \n  set_engine(\"C5.0\", control = C50::C5.0Control(seed = 864))\n\nboost_wflow &lt;-  workflow(class ~ ., boost_spec) \n\nset.seed(526)\nboost_res &lt;-\n  fit_resamples(\n    boost_wflow,\n    resamples = forested_rs,\n    control = ctrl_rs,\n    metrics = cls_mtr\n  )\n\n# Save the individual resamples and change the name of the accuracy column. \nboost_metrics &lt;- \n  collect_metrics(boost_res, summarize = FALSE) |&gt; \n  select(id, Boosting = .estimate)\n\n\n\n\n\n\n\nWarning\n\n\n\nAlthough we strive for reproducibility, it can be difficult. The boosted tree results from the main text (where it was tuned) used slightly different random numbers than when we trained for a single model. Nine of the ten resamples had different accuracy results; the mean difference between the two was 0.07%, and the largest difference was 0.84%. This is obviously very small, but it does lead to different results here than in the main text. The conclusions will not change.\n\n\nNow let’s create the logistic regression. There is a fair amount of preprocessing and feature engineering. The rationale for these will be discussed in a future chapter in Part 4.\nYou’ll see below that, although the model uses a single penalty value, we pass a sequence of penalties to the lambda parameter of glmnet::glmnet(). See parsnip issue #431 for some background.\nThe model is a full ridge regression model since mixture = 0.\n\nlogistic_rec &lt;-\n  recipe(class ~ ., data = forested_train) |&gt;\n  # standardize numeric predictors\n  step_orderNorm(all_numeric_predictors()) |&gt;\n  # Convert to an effect encoding\n  step_lencode_mixed(county, outcome = \"class\") |&gt;\n  # Create pre-defined interactions\n  step_interact(!!forested_int_form) |&gt;\n  # 10 spline terms for certain predictors\n  step_spline_natural(\n    all_numeric_predictors(),\n    -county,\n    -eastness,\n    -northness,\n    -year,\n    -contains(\"_x_\"),\n    deg_free = 10\n  ) |&gt;\n  # Remove any linear dependencies\n  step_lincomb(all_predictors())\n\n# ------------------------------------------------------------------------------\n\nlogistic_pen &lt;- 10^seq(-6, -1, length.out = 50)\n\nlogistic_spec &lt;- \n  # Values here were determined via grid search:\n  logistic_reg(penalty = 2.12095088792019e-05, mixture = 0.0) |&gt; \n  set_engine(\"glmnet\", path_values = !!logistic_pen)\n\n# ------------------------------------------------------------------------------\n\nlogistic_wflow &lt;- workflow(logistic_rec, logistic_spec)\n\nlogistic_res &lt;- \n  logistic_wflow |&gt; \n  fit_resamples(\n    resamples = forested_rs,\n    control = ctrl_rs,\n    metrics = cls_mtr\n  )\n\nlogistic_metrics &lt;- \n  collect_metrics(logistic_res, summarize = FALSE) |&gt; \n  select(id, Logistic = .estimate)\n\nNow, the naive Bayes model. There is no need for feature engineering and not much in the way of tuning parameters.\n\nnb_rec &lt;-\n  recipe(class ~ ., data = forested_train) |&gt;\n  step_orderNorm(all_numeric_predictors())\n\nnb_wflow &lt;- workflow(nb_rec, naive_Bayes())\n\nnb_res &lt;- \n  nb_wflow |&gt; \n  fit_resamples(\n    resamples = forested_rs,,\n    control = ctrl_rs,\n    metrics = cls_mtr\n  )\n\nnb_metrics &lt;- \n  collect_metrics(nb_res, summarize = FALSE) |&gt; \n  select(id, `Naive Bayes` = .estimate)\n\nDuring resampling, this model will probably provide some warnings when fit that resemble:\n\nNumerical 0 probability for all classes with observation X\n\nThis occurs because the model multiplies about a dozen probabilities together. The consequence is that some of these products become very close to zero, and R complains a bit about this.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#sec-forested-workflow-set",
    "href": "chapters/comparing-models.html#sec-forested-workflow-set",
    "title": "14  Comparing Models",
    "section": "\n14.4 A Workflow Set",
    "text": "14.4 A Workflow Set\nAnother method for resampling or tuning a series of preprocessors and/or models is to create a workflow set. We’ll see these more in later chapters. Let’s take our previous set of three model results and coerce them into a workflow set.\n\nforested_wflow_set &lt;- as_workflow_set(\n    Boosting = boost_res,\n    Logistic = logistic_res,\n    `Naive Bayes` = nb_res\n)\nforested_wflow_set\n#&gt; # A workflow set/tibble: 3 × 4\n#&gt;   wflow_id    info             option    result   \n#&gt;   &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 Boosting    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;rsmp[+]&gt;\n#&gt; 2 Logistic    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;rsmp[+]&gt;\n#&gt; 3 Naive Bayes &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;rsmp[+]&gt;\n\nforested_wflow_set |&gt; rank_results()\n#&gt; # A tibble: 3 × 9\n#&gt;   wflow_id    .config         .metric   mean  std_err     n preprocessor model  rank\n#&gt;   &lt;chr&gt;       &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n#&gt; 1 Boosting    Preprocessor1_… accura… 0.8952 0.009321    10 formula      C5_r…     1\n#&gt; 2 Logistic    Preprocessor1_… accura… 0.8906 0.01038     10 recipe       logi…     2\n#&gt; 3 Naive Bayes Preprocessor1_… accura… 0.8581 0.01542     10 recipe       naiv…     3\n\nWe’ll see the advantage of this in a later section.",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#sec-compare-resamples",
    "href": "chapters/comparing-models.html#sec-compare-resamples",
    "title": "14  Comparing Models",
    "section": "\n14.5 Resampled Data Sets",
    "text": "14.5 Resampled Data Sets\nLet’s collect the accuracy values for each resample and model into a data frame. We’ll make a “wide” version with three columns for each of the three models and a “long” version where there is a column for each model and another for the accuracies. We call the column with the model pipeline.\nWe’ll also plot the data to show that it is similar to the one in the main text.\n\naccuracy_df &lt;- \n  boost_metrics |&gt; \n  full_join(logistic_metrics, by = \"id\") |&gt; \n  full_join(nb_metrics, by = \"id\") \naccuracy_df\n#&gt; # A tibble: 10 × 4\n#&gt;   id     Boosting Logistic `Naive Bayes`\n#&gt;   &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 Fold01   0.8671   0.8773        0.8057\n#&gt; 2 Fold02   0.8900   0.8839        0.8310\n#&gt; 3 Fold03   0.9485   0.9505        0.9386\n#&gt; 4 Fold04   0.8687   0.8848        0.8618\n#&gt; 5 Fold05   0.8589   0.8653        0.8547\n#&gt; 6 Fold06   0.8776   0.8413        0.7820\n#&gt; # ℹ 4 more rows\n  \naccuracy_long_df &lt;- \n  accuracy_df |&gt; \n  pivot_longer(cols = c(-id), names_to = \"pipeline\", values_to = \"accuracy\")\naccuracy_long_df\n#&gt; # A tibble: 30 × 3\n#&gt;   id     pipeline    accuracy\n#&gt;   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 Fold01 Boosting      0.8671\n#&gt; 2 Fold01 Logistic      0.8773\n#&gt; 3 Fold01 Naive Bayes   0.8057\n#&gt; 4 Fold02 Boosting      0.8900\n#&gt; 5 Fold02 Logistic      0.8839\n#&gt; 6 Fold02 Naive Bayes   0.8310\n#&gt; # ℹ 24 more rows\n\naccuracy_long_df |&gt; \n  ggplot(aes(pipeline, accuracy)) + \n  geom_point(aes(col = id), show.legend = FALSE) + \n  geom_line(aes(group = id, col = id), show.legend = FALSE) +\n  scale_y_continuous(label = label_percent()) +\n  labs(x = NULL)\n\n\n\n\n\n\n\nNow we can start analyzing the data in different ways.\n\n14.5.1 Statistical Foundations\nWe’ll start with the Frequentist perspective.\n\n14.5.2 Frequentist Hypothesis Testing Methods\nWe take the wide version of the accuracy data and fit a linear regression to it in a way that accounts for the resample-to-resample effect. Let’s take a minute and think about how our model will set up the parameter estimates.\nSince the data in the pipeline column is currently character, the model function will convert this column to a factor and assign the factor level order alphabetically. This means that the levels are: Boosting, Logistic, and Naive Bayes. Recall that the default parameterization for indicator variables is a “reference cell” parameterization, so the intercept corresponds to the first factor level. This means that, in terms of the fixed effects, we interpret parameter estimates as:\n\\[\n\\hat{y}_{ij} = \\underbrace{\\quad \\hat{\\beta}_0\\quad }_{\\tt{Boosting}} + \\underbrace{\\quad \\hat{\\beta}_1 \\quad }_{\\tt{Boosting - Logistic}} +\\underbrace{\\quad \\hat{\\beta}_2\\quad }_{\\tt{Boosting - Naive\\: Bayes}}\n\\]\nUsing lme4::lmer() to fit the model, we specify pipeline as the fixed effect and designate a random effect due to resampling as (1|id), where 1 symbolizes the intercept.\nThe results are:\n\nfreq_mixed_mod &lt;- lmer(accuracy ~ pipeline + (1|id), data = accuracy_long_df)\n\nfreq_mixed_mod |&gt; tidy()\n#&gt; # A tibble: 5 × 6\n#&gt;   effect   group    term                 estimate std.error statistic\n#&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 fixed    &lt;NA&gt;     (Intercept)          0.8952    0.01200    74.57  \n#&gt; 2 fixed    &lt;NA&gt;     pipelineLogistic    -0.004504  0.007520   -0.5989\n#&gt; 3 fixed    &lt;NA&gt;     pipelineNaive Bayes -0.03702   0.007520   -4.922 \n#&gt; 4 ran_pars id       sd__(Intercept)      0.03403  NA          NA     \n#&gt; 5 ran_pars Residual sd__Observation      0.01682  NA          NA\n\nThe model term corresponding to term = \"sd__(Intercept)\" is the standard deviation of the random effects, and the one with a term value of sd__Observation is the residual standard deviation. Note that the former is almost twice the size of the latter; the resample-to-resample effect is very large for these data.\nNotice that p-values that are not automatically computed. This is addressed by reading help(\"pvalues\") or the package vignette called “Fitting Linear Mixed-Effects Models using lme4”. The CRAN PDF for the latter is here. We can create approximate p-values using the lmerTest or multicomp packages.\nWe’ll show the multicomp package but first we will define contrasts of the parameter estimates that provide all three pairwise comparisons.\n\n# Coefficients for the estimates to create differences in accuracies.\ncomparisons &lt;- \n  rbind(\"Boosting - Logistic\"    = c(0,  1,  0),     \n        \"Boosting - Naive Bayes\" = c(0,  0,  1),     \n        \"Naive Bayes - Logistic\" = c(0, -1,  1))\n\nWe’ll use the multcomp::glht() function1 to generate them. The glht() function relies on another function from that package called mcp()2, and that can be used to sort out the details. For the raw (but approximate) p-values, we simply give it a single contrast at a time:\n\nraw_res &lt;- NULL\nfor (i in 1:nrow(comparisons)) {\n  raw_res &lt;- \n    bind_rows(\n      raw_res, \n      freq_mixed_mod |&gt; \n        glht(linfct = mcp(pipeline = comparisons[i,,drop = FALSE])) |&gt; \n        summary() |&gt; \n        tidy() |&gt; \n        rename(raw_pvalue = adj.p.value)\n    )\n}\n\nraw_res |&gt; \n  select(-term, -null.value)\n#&gt; # A tibble: 3 × 5\n#&gt;   contrast                estimate std.error statistic   raw_pvalue\n#&gt;   &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 Boosting - Logistic    -0.004504  0.007520   -0.5989 0.5493      \n#&gt; 2 Boosting - Naive Bayes -0.03702   0.007520   -4.922  0.0000008554\n#&gt; 3 Naive Bayes - Logistic -0.03251   0.007520   -4.323  0.00001536\n\nAgain, due to differences in random numbers, the accuracy numbers are slightly different. Consequently, the p-values are also different in value but not in interpretation.\n\n\n14.5.2.1 Post Hoc Pairwise Comparisons and Protecting Against False Positive Findings\nTo compute the Bonferroni or FDR adjustments, we can use the stats::p.adjust() package. However, for the FDR, there are many alternatives. In CRAN alone:\n\ntools::CRAN_package_db() |&gt; \n  as_tibble() |&gt; \n  filter(grepl(\"(fdr)|(false discovery rate)\", tolower(Description))) |&gt; \n  mutate(Title = gsub(\"\\n\", \"\", Title)) |&gt; \n  select(Package, Title)\n#&gt; # A tibble: 84 × 2\n#&gt;   Package             Title                                                         \n#&gt;   &lt;chr&gt;               &lt;chr&gt;                                                         \n#&gt; 1 adaptMT             Adaptive P-Value Thresholding for Multiple Hypothesis Testing…\n#&gt; 2 alpha.correction.bh Benjamini-Hochberg Alpha Correction                           \n#&gt; 3 APFr                Multiple Testing Approach using Average Power Function (APF) …\n#&gt; 4 ashr                Methods for Adaptive Shrinkage, using Empirical Bayes         \n#&gt; 5 bayefdr             Bayesian Estimation and Optimisation of Expected False Discov…\n#&gt; 6 BonEV               An Improved Multiple Testing Procedure for Controlling FalseD…\n#&gt; # ℹ 78 more rows\n\nand even more on Bioconductor:\n\nBiocManager::available(\"(fdr)|(false discovery rate)\")\n#&gt;  [1] \"bayefdr\"              \"benchmarkfdrData2019\" \"cffdrs\"              \n#&gt;  [4] \"CorrectedFDR\"         \"dfdr\"                 \"DiscreteFDR\"         \n#&gt;  [7] \"EFDR\"                 \"fcfdr\"                \"fdrame\"              \n#&gt; [10] \"fdrci\"                \"fdrDiscreteNull\"      \"FDRestimation\"       \n#&gt; [13] \"FDRsamplesize2\"       \"FDRsampsize\"          \"fdrtool\"             \n#&gt; [16] \"GFDrmst\"              \"GFDrmtl\"              \"LFDR.MLE\"            \n#&gt; [19] \"LFDR.MME\"             \"LFDREmpiricalBayes\"   \"locfdr\"              \n#&gt; [22] \"nzffdr\"               \"onlineFDR\"            \"pafdR\"               \n#&gt; [25] \"pwrFDR\"               \"repfdr\"               \"RFlocalfdr\"          \n#&gt; [28] \"RFlocalfdr.data\"      \"sffdr\"                \"simpleFDR\"           \n#&gt; [31] \"SNVLFDR\"              \"ssize.fdr\"            \"swfdr\"\n\nstats::p.adjust() is very simple to use:\n\nraw_res |&gt; \n  mutate(\n    Bonnferoni = p.adjust(raw_pvalue, method = \"bonferroni\"),\n    FDR = p.adjust(raw_pvalue, method = \"BH\")\n  ) |&gt; \n  select(-term, -null.value, -estimate)\n#&gt; # A tibble: 3 × 6\n#&gt;   contrast               std.error statistic   raw_pvalue  Bonnferoni         FDR\n#&gt;   &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 Boosting - Logistic     0.007520   -0.5989 0.5493       1           0.5493     \n#&gt; 2 Boosting - Naive Bayes  0.007520   -4.922  0.0000008554 0.000002566 0.000002566\n#&gt; 3 Naive Bayes - Logistic  0.007520   -4.323  0.00001536   0.00004609  0.00002304\n\nFor Tukey’s HSD, we can use glht() again:\n\nfreq_mixed_mod |&gt; \n  glht(linfct = mcp(pipeline = \"Tukey\")) |&gt; \n  summary() |&gt; \n  tidy() |&gt; \n  select(-term, -null.value, -estimate)\n#&gt; # A tibble: 3 × 4\n#&gt;   contrast               std.error statistic adj.p.value\n#&gt;   &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 Logistic - Boosting     0.007520   -0.5989 0.8207     \n#&gt; 2 Naive Bayes - Boosting  0.007520   -4.922  0.000002667\n#&gt; 3 Naive Bayes - Logistic  0.007520   -4.323  0.00004223\n\n\n\n14.5.2.2 Comparing Performance using Equivalence Tests\nWe’ll conduct the Two One-Sided Test (TOST) procedure by computing the confidence intervals on the differences, which we can compute via glht() with the Tukey correction:\n\n# The defaults that mcp() uses has the opposite order from the books' main\n# test uses (e.g. Logistic - Boosting instead of Boosting - Logistic)\ndifference_int &lt;- \n  freq_mixed_mod |&gt; \n  glht(linfct = mcp(pipeline = \"Tukey\")) |&gt; \n  confint(level = 0.90) |&gt; \n  tidy()\n\nFrom these data, we can create the same plot as the main text:\n\ndifference_int |&gt; \n  ggplot(aes(x = contrast, y = -estimate)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = -conf.low, ymax = -conf.high), width = 1 / 7) +\n  geom_hline(yintercept = c(-0.03, 0.03), linetype = 2) +\n  ylim(-0.07, 0.07) +\n  labs(x = \"Comparison\", y = \"Accuracy Difference\") +\n  scale_y_continuous(label = label_percent()) \n\n\n\n\n\n\n\n\n\n14.5.3 Comparisons Using Bayesian Models\nFor Bayesian evaluation, tidymodels replies heavily on the rstanarm package, which is based on Stan. The tidyposterior package is the main interface for tidymodels objects. The main function, tidyposterior::perf_mod(), has methods for several types of objects. First, we’ll show how to use it with a basic data frame of results, then with specific types of tidymodels objects.\nWhen the resampled performance estimates are in a data frame, it should be in the “wide” format, such as accuracy_df. The simplest use of the function is:\n\nset.seed(101)\nbasic_bayes_mod &lt;- perf_mod(accuracy_df, refresh = 0)\nbasic_bayes_mod\n#&gt; Bayesian Analysis of Resampling Results\n\n# The underlying Stan model: \nbasic_bayes_mod |&gt; \n  pluck(\"stan\") |&gt; \n  print(digits = 3)\n#&gt; stan_glmer\n#&gt;  family:       gaussian [identity]\n#&gt;  formula:      statistic ~ model + (1 | id)\n#&gt;  observations: 30\n#&gt; ------\n#&gt;                  Median MAD_SD\n#&gt; (Intercept)       0.895  0.011\n#&gt; modelLogistic    -0.004  0.008\n#&gt; modelNaive Bayes -0.037  0.008\n#&gt; \n#&gt; Auxiliary parameter(s):\n#&gt;       Median MAD_SD\n#&gt; sigma 0.018  0.003 \n#&gt; \n#&gt; Error terms:\n#&gt;  Groups   Name        Std.Dev.\n#&gt;  id       (Intercept) 0.03491 \n#&gt;  Residual             0.01857 \n#&gt; Num. levels: id 10 \n#&gt; \n#&gt; ------\n#&gt; * For help interpreting the printed output see ?print.stanreg\n#&gt; * For info on the priors used see ?prior_summary.stanreg\n\nThe broom.mixed package’s tidy() method can also be used:\n\nbasic_bayes_mod |&gt; \n  pluck(\"stan\") |&gt; \n  tidy()\n#&gt; # A tibble: 5 × 4\n#&gt;   term                     estimate std.error group   \n#&gt;   &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   \n#&gt; 1 (Intercept)              0.8950    0.01146  &lt;NA&gt;    \n#&gt; 2 modelLogistic           -0.004410  0.008247 &lt;NA&gt;    \n#&gt; 3 modelNaive Bayes        -0.03717   0.008039 &lt;NA&gt;    \n#&gt; 4 sd_(Intercept).id        0.03491  NA        id      \n#&gt; 5 sd_Observation.Residual  0.01857  NA        Residual\n\ntidyposterior also has a tidy() method for obtaining a sample from the posterior that has been configured not to be in the format of the parameters but in terms of the model means.\n\nset.seed(185)\nbasic_bayes_post &lt;- \n  basic_bayes_mod |&gt; \n  tidy()\n\nbasic_bayes_post\n#&gt; # Posterior samples of performance\n#&gt; # A tibble: 12,000 × 2\n#&gt;   model       posterior\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;\n#&gt; 1 Boosting       0.8913\n#&gt; 2 Logistic       0.9014\n#&gt; 3 Naive Bayes    0.8676\n#&gt; 4 Boosting       0.9014\n#&gt; 5 Logistic       0.9074\n#&gt; 6 Naive Bayes    0.8636\n#&gt; # ℹ 11,994 more rows\n\nThis object also has a summary object to get credible intervals:\n\nsummary(basic_bayes_post,  prob = 0.9)\n#&gt; # A tibble: 3 × 4\n#&gt;   model         mean  lower  upper\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Boosting    0.8949 0.8749 0.9156\n#&gt; 2 Logistic    0.8905 0.8700 0.9113\n#&gt; 3 Naive Bayes 0.8579 0.8370 0.8782\n\nand an autoplot() method:\n\nautoplot(basic_bayes_post)\n\n\n\n\n\n\n\nTo take a look at pairwise differences, there is tidyposterior::contrast_models(). This takes the objects produced by perf_mod() and two vector arguments of groups to compare. If these two group arguments are left to their defaults, all comparisons are used:\n\nset.seed(185)\nbasic_bayes_diff_post &lt;- \n  basic_bayes_mod |&gt; \n  contrast_models()\n\nbasic_bayes_diff_post\n#&gt; # Posterior samples of performance differences\n#&gt; # A tibble: 12,000 × 4\n#&gt;   difference model_1  model_2  contrast             \n#&gt;        &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                \n#&gt; 1 -0.01012   Boosting Logistic Boosting vs. Logistic\n#&gt; 2 -0.005995  Boosting Logistic Boosting vs. Logistic\n#&gt; 3 -0.005884  Boosting Logistic Boosting vs. Logistic\n#&gt; 4  0.0002408 Boosting Logistic Boosting vs. Logistic\n#&gt; 5  0.002266  Boosting Logistic Boosting vs. Logistic\n#&gt; 6  0.01333   Boosting Logistic Boosting vs. Logistic\n#&gt; # ℹ 11,994 more rows\n\nThere is an autoplot() method, but the summary() method is more interesting since that is where the ROPE analysis can be conducted. Using default arguments gives the 90% credible intervals on the differences:\n\nsummary(basic_bayes_diff_post)\n#&gt; # A tibble: 3 × 9\n#&gt;   contrast        probability     mean     lower   upper  size pract_neg pract_equiv\n#&gt;   &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 Boosting vs Lo…      0.6992 0.004403 -0.009418 0.01798     0        NA          NA\n#&gt; 2 Boosting vs Na…      1      0.03701   0.02354  0.05048     0        NA          NA\n#&gt; 3 Logistic vs Na…      0.9998 0.03260   0.01831  0.04643     0        NA          NA\n#&gt; # ℹ 1 more variable: pract_pos &lt;dbl&gt;\n\nUsing the size argument enables a ROPE analysis:\n\nsummary(basic_bayes_diff_post, size = 0.03) |&gt; \n  select(contrast, starts_with(\"pract\"))\n#&gt; # A tibble: 3 × 4\n#&gt;   contrast                pract_neg pract_equiv pract_pos\n#&gt;   &lt;chr&gt;                       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Boosting vs Logistic       0.0005      0.9962   0.00325\n#&gt; 2 Boosting vs Naive Bayes    0           0.1892   0.8108 \n#&gt; 3 Logistic vs Naive Bayes    0           0.372    0.628\n\nThere are an abundance of options that can be used with perf_mod(), including:\n\nThe transform argument can convert the metric to a different scale before the analysis and back transforms when creating posteriors. For example, if you believe that \\(log(RMSE)\\) should be used in the analysis, use this argument.\nWhen we think that the probability distributions of the metrics have different variances, the hetero_var is logical and can enable that. Be aware that this option makes the model fit more complex, which may result in convergence issues.\nSet Stan’s random number seed using the seed argument.\n\nThere are also options that can be passed to the Stan fit, including:\n\nSampling-related options such as chains (number of MCMC chains) and cores (how many things to do in parallel).\nPrior distributions for parameters. Of interest might be:\n\n\nprior_aux: the prior for the distribution’s scale parameter (e.g. \\(\\sigma\\) for the Gaussian).\n\nprior_intercept: the distribution for the random intercepts.\n\nprior: distributions for the regression parameters.\n\n\n\nFor priors, the Stan page on priors is important to read since it details how the default priors are automatically scaled. The page on sampling is also very helpful.\nFinally, you can use perf_mod() with other types of tidymodels objects. For example, if you want to conduct a within-model analysis where you compare candidates (say from a grid search), you can pass that as the first argument to perf_mod().\nIf you have a workflow set, you can perform within- and/or between-model comparisons. This can potentially compare a large number of model/candidate combinations.\nWe previously created forested_wflow_set to house our resampling results. Here is how we could use that object for the analysis:\n\nset.seed(24)\nforested_wflow_set |&gt; \n  perf_mod(\n    seed = 310,\n    iter = 10000,\n    chains = 10,\n    # Don't print a log:\n    refresh = 0,\n    prior_aux = rstanarm::exponential(floor(1/sd(accuracy_long_df$accuracy))), \n    prior_intercept = rstanarm::cauchy(0, 1),\n    prior = rstanarm::normal(0, 5)\n  ) |&gt; \n  contrast_models() |&gt; \n  summary(size = 0.03) |&gt; \n  select(contrast, starts_with(\"pract\"))\n#&gt; # A tibble: 3 × 4\n#&gt;   contrast                pract_neg pract_equiv pract_pos\n#&gt;   &lt;chr&gt;                       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Boosting vs Logistic       0.0003      0.9971   0.00264\n#&gt; 2 Boosting vs Naive Bayes    0           0.1941   0.8059 \n#&gt; 3 Logistic vs Naive Bayes    0           0.3749   0.6251",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#sec-compare-holdout",
    "href": "chapters/comparing-models.html#sec-compare-holdout",
    "title": "14  Comparing Models",
    "section": "\n14.6 Single Holdout Data Sets",
    "text": "14.6 Single Holdout Data Sets\nFor bootstrap confidence intervals, there is a helper function rsample::int_pctl() that has methods that can work for resampled objects (e.g., tune::fit_resamples()) and the outputs from the various tune_*() functions.\nAs an example, if we were using a validation set, we could produce the 90% bootstrap interval for the model via:\n\nint_pctl(boost_res, times = 2000, alpha = 0.1)\n\nBy default, this will compute intervals for every tuning parameter candidate and metric in the data (but there are arguments that can be used to restrict the computations to a smaller set).\nFor a test set, we encourage users to use last_fit(). Let’s suppose that we still can’t decide if the boosted tree or logistic regression is the final model. We can fit them and evaluate their test set via:\n\nboost_final_res &lt;- \n  last_fit(boost_wflow, split = forested_split, metrics = cls_mtr)\n\nlogistic_final_res &lt;- \n  last_fit(logistic_wflow, split = forested_split, metrics = cls_mtr)\n\nthen use:\n\nset.seed(885)\nint_pctl(boost_final_res, times = 2000, alpha = 0.1)\n#&gt; # A tibble: 1 × 6\n#&gt;   .metric  .estimator .lower .estimate .upper .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy bootstrap  0.8651    0.8795 0.8942 Preprocessor1_Model1\n\nset.seed(885)\nint_pctl(logistic_final_res, times = 2000, alpha = 0.1)\n#&gt; # A tibble: 1 × 6\n#&gt;   .metric  .estimator .lower .estimate .upper .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy bootstrap  0.8578    0.8730 0.8877 Preprocessor1_Model1\n\nTo compute intervals on differences, there is no current interface. However, you can use the core functions to get the results. For example, we can join the predictions for different models by their row number:\n\npaired_class_pred &lt;- \n  logistic_final_res |&gt; \n  collect_predictions() |&gt; \n  select(.row, class, logistic = .pred_class) |&gt; \n  full_join(\n    boost_final_res |&gt; \n      collect_predictions() |&gt; \n      select(.row, boosting = .pred_class),\n    by = \".row\"\n  )\npaired_class_pred\n#&gt; # A tibble: 1,371 × 4\n#&gt;    .row class logistic boosting\n#&gt;   &lt;int&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt;   \n#&gt; 1  4833 No    No       No      \n#&gt; 2  4834 No    No       No      \n#&gt; 3  4835 No    No       No      \n#&gt; 4  4836 Yes   Yes      Yes     \n#&gt; 5  4837 No    No       No      \n#&gt; 6  4838 Yes   Yes      Yes     \n#&gt; # ℹ 1,365 more rows\n\nthen create bootstrap samples:\n\nset.seed(649)\npaired_class_bt &lt;- \n  paired_class_pred |&gt; \n  bootstraps(times = 2000)\n\nWe can write a function to get the difference:\n\nmetric_diff &lt;- function(split) {\n  # Get the bootstrap sample:\n  est &lt;- \n    split |&gt; \n    rsample::analysis() |&gt; \n    # Stack the rows\n    tidyr::pivot_longer(\n      cols = c(-.row, -class),\n      names_to = \"model\",\n      values_to = \"estimate\"\n    ) |&gt; \n    dplyr::group_by(model) |&gt; \n    yardstick::accuracy(class, estimate)\n  tibble::tibble(term = \"Boosting - Logistic\", estimate = -diff(est$.estimate))\n}\n\n# Run on one sample to demonstrate: \npaired_class_bt$splits[[1]] |&gt; metric_diff()\n#&gt; # A tibble: 1 × 2\n#&gt;   term                estimate\n#&gt;   &lt;chr&gt;                  &lt;dbl&gt;\n#&gt; 1 Boosting - Logistic -0.01240\n\n# Run on all:\npaired_class_bt &lt;- \n  paired_class_bt |&gt; \n  mutate(stats = map(splits, metric_diff))\n\nint_pctl(paired_class_bt, stats, alpha = .1)\n#&gt; # A tibble: 1 × 6\n#&gt;   term                   .lower .estimate  .upper .alpha .method   \n#&gt;   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n#&gt; 1 Boosting - Logistic -0.005106  0.006411 0.01823    0.1 percentile",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#footnotes",
    "href": "chapters/comparing-models.html#footnotes",
    "title": "14  Comparing Models",
    "section": "",
    "text": "“GLHT” stands for “General Linear Hypothesis Tests.”↩︎\n“MCP” presumably stands for “Multiple Comparison Procedure.”↩︎",
    "crumbs": [
      "Optmization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  }
]