[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidymodels Computing Supplement",
    "section": "",
    "text": "Preface\nThis is a computing supplement to the main website that uses the tidymodels framework for modeling. The structure is similar to the website, but the content here shows how to use this software (and sometimes others) for each topic.\nWe also want these materials to be reusable and open. The sources are in the source GitHub repository with a Creative Commons license attached (see below).\nTo cite this work, we suggest:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Tidymodels Computing Supplement",
    "section": "License",
    "text": "License\n\nThis work is licensed under CC BY-SA 4.0",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Tidymodels Computing Supplement",
    "section": "Intended Audience",
    "text": "Intended Audience\nReaders should have used R before but do not have to be experts. If you are new to R, we suggest taking a look at R for Data Science.\nYou do not have to be a modeling expert either. We hope that you have used a linear or logistic regression before and understand basic statistical concepts such as correlation, variability, probabilities, etc.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-can-i-ask-questions",
    "href": "index.html#how-can-i-ask-questions",
    "title": "Tidymodels Computing Supplement",
    "section": "How can I ask questions?",
    "text": "How can I ask questions?\nIf you have questions about the content, it is probably best to ask on a public forum, like cross-validated or Posit Community. You’ll most likely get a faster answer there if you take the time to ask the questions in the best way possible.\nIf you want a direct answer from us, you should follow what I call Yihui’s Rule: add an issue to GitHub (labeled as “Discussion”) first. It may take some time for us to get back to you.\nIf you think there is a bug, please file an issue.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#can-i-contribute",
    "href": "index.html#can-i-contribute",
    "title": "Tidymodels Computing Supplement",
    "section": "Can I contribute?",
    "text": "Can I contribute?\nThere is a contributing page with details on how to get up and running to compile the materials (there are a lot of software dependencies) and suggestions on how to help.\nIf you just want to fix a typo, you can make a pull request to alter the appropriate .qmd file.\nPlease feel free to improve the quality of this content by submitting pull requests. A merged PR will make you appear in the contributor list.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#computing-notes",
    "href": "index.html#computing-notes",
    "title": "Tidymodels Computing Supplement",
    "section": "Computing Notes",
    "text": "Computing Notes\nQuarto was used to compile and render the materials\n\n\nQuarto 1.4.510\n[✓] Checking versions of quarto binary dependencies...\n      Pandoc version 3.1.9: OK\n      Dart Sass version 1.69.5: OK\n      Deno version 1.37.2: OK\n[✓] Checking versions of quarto dependencies......OK\n[✓] Checking Quarto installation......OK\n      Version: 1.4.510\n[✓] Checking tools....................OK\n      TinyTeX: (external install)\n      Chromium: (not installed)\n[✓] Checking LaTeX....................OK\n      Using: TinyTex\n      Version: 2023\n[✓] Checking basic markdown render....OK\n[✓] Checking Python 3 installation....OK\n      Version: 3.11.6\n      Jupyter: (None)\n      Jupyter is not available in this Python installation.\n[✓] Checking R installation...........OK\n      Version: 4.3.2\n      LibPaths:\n      knitr: 1.45\n      rmarkdown: 2.25\n[✓] Checking Knitr engine render......OK\n\n\nR version 4.3.2 (2023-10-31) was used for the majority of the computations. torch 1.13.1 was also used. The versions of the primary R modeling and visualization packages used here are:\n\n\n\n\n\n\n\n\n\n\nbrulee (0.2.0)\n\nCubist (0.4.2.1)\n\ndials (1.2.0)\n\n\n\ndplyr (1.1.4)\n\ne1071 (1.7-13)\n\nggplot2 (3.4.4)\n\n\n\ngt (0.10.0)\n\nhardhat (1.3.0)\n\nmodeldata (1.2.0)\n\n\n\nmodeldatatoo (0.2.1)\n\nparsnip (1.1.1)\n\npatchwork (1.1.3)\n\n\n\nprobably (1.0.2)\n\npurrr (1.0.2)\n\nragg (1.2.6)\n\n\n\nrecipes (1.0.8)\n\nrsample (1.2.0)\n\nrstudioapi (0.15.0)\n\n\n\nrules (1.0.2)\n\nsplines2 (0.5.1)\n\nstopwords (2.3)\n\n\n\ntextrecipes (1.0.6.9000)\n\ntidymodels (1.1.1)\n\ntidyr (1.3.0)\n\n\n\ntorch (0.11.0)\n\ntune (1.1.2)\n\nusethis (2.2.2)\n\n\n\nworkflows (1.1.3)\n\nworkflowsets (1.0.1)\n\nyardstick (1.2.0)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/news.html#errata",
    "href": "chapters/news.html#errata",
    "title": "News",
    "section": "Errata",
    "text": "Errata\nNo reported edits (yet).",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#changelog",
    "href": "chapters/news.html#changelog",
    "title": "News",
    "section": "Changelog",
    "text": "Changelog\n\n2023-12-11\nAdded a chapter on initial data splitting.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/contributing.html#software",
    "href": "chapters/contributing.html#software",
    "title": "Contributing",
    "section": "Software",
    "text": "Software\nRegarding R packages, the repository has a DESCRIPTION file as if it were an R package. This lets us specify precisely what packages and their versions should be installed. The packages listed in the imports field contain packages for modeling/analysis and packages used to make the website/book. Some basic system requirements are likely needed to install packages: Fortran, gdal, and others.\nThe main requirements are as follows.\nQuarto\nQuarto is an open-source scientific and technical publishing system. Quarto version 1.4.510 is used to compile the website.\nR and renv\nR version 4.3.2 (2023-10-31) is what we are currently using. We suggest using rig to manage R versions. There are several IDEs that you can use. We’ve used RStudio (&gt;= 2023.6.1.524).\nThe current strategy is to use the renv (&gt;= version 1.0.3) package to make this project more isolated, portable and reproducible.\nTo get package dependencies installed…\nWhen you open the computing-tidymodels.Rproj file, the renv package should be automatically installed/updated (if needed). For example:\n# Bootstrapping renv 1.0.3 ---------------------------------------------------\n- Downloading renv ... OK\n- Installing renv  ... OK\n\nThe following package(s) will be installed:\n- BiocManager [1.30.22]\nThese packages will be installed into \"~/content/aml4td/renv/library/R-4.3/x86_64-apple-darwin20\".\nif you try to compile the book, you probably get and error:\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nYou can get more information using renv::status() but you can get them installed by first running renv::activate(). As an example:\n&gt; renv::activate()\n\nRestarting R session...\n\n- Project '~/content/aml4td' loaded. [renv 1.0.3]\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nSince we have package versions recorded in the lockfile, we can installed them using renv::restore(). Here is an example of that output:\n&gt; renv::restore() \nThe following package(s) will be updated:\n\n# Bioconductor ---------------------------------------------------------------\n- mixOmics         [* -&gt; mixOmicsTeam/mixOmics]\n\n# CRAN -----------------------------------------------------------------------\n- BiocManager      [1.30.22 -&gt; 1.30.21.1]\n- lattice          [0.21-9 -&gt; 0.21-8]\n- Matrix           [1.6-1.1 -&gt; 1.6-0]\n- nlme             [3.1-163 -&gt; 3.1-162]\n- rpart            [4.1.21 -&gt; 4.1.19]\n- survival         [3.5-7 -&gt; 3.5-5]\n- abind            [* -&gt; 1.4-5]\n\n&lt;snip&gt;\n\n- zip              [* -&gt; 2.2.0]\n- zoo              [* -&gt; 1.8-12]\n\n# GitHub ---------------------------------------------------------------------\n- BiocParallel     [* -&gt; Bioconductor/BiocParallel@devel]\n- BiocVersion      [* -&gt; Bioconductor/BiocVersion@devel]\n- modeldatatoo     [* -&gt; tidymodels/modeldatatoo@HEAD]\n- parsnip          [* -&gt; tidymodels/parsnip@HEAD]\n- usethis          [* -&gt; r-lib/usethis@HEAD]\n\n# RSPM -----------------------------------------------------------------------\n- bslib            [* -&gt; 0.5.1]\n- fansi            [* -&gt; 1.0.5]\n- fontawesome      [* -&gt; 0.5.2]\n- ggplot2          [* -&gt; 3.4.4]\n- htmltools        [* -&gt; 0.5.6.1]\n- withr            [* -&gt; 2.5.1]\n\nDo you want to proceed? [Y/n]: y\n\n# Downloading packages -------------------------------------------------------\n- Downloading BiocManager from CRAN ...         OK [569 Kb in 0.19s]\n- Downloading nlme from CRAN ...                OK [828.7 Kb in 0.19s]\n- Downloading BH from CRAN ...                  OK [12.7 Mb in 0.4s]\n- Downloading BiocVersion from GitHub ...       OK [826 bytes in 0.37s]\n\n&lt;snip&gt;\n\nDepending on whether you have to install packages from source, you may need to install some system dependencies and try again (I had to install libgit2 the last time I did this).\nOnce you have everything installed, we recommend installing the underlying torch computational libraries. You can do this by loading the torch package A download will automatically begin if you need one.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing.html#contributor-list",
    "href": "chapters/contributing.html#contributor-list",
    "title": "Contributing",
    "section": "Contributor List",
    "text": "Contributor List\nThe would like to thank users who have made a contribution to the project:",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/introduction.html#installation",
    "href": "chapters/introduction.html#installation",
    "title": "1  Introduction",
    "section": "\n1.1 Installation",
    "text": "1.1 Installation\ntidymodels is built in R so you’ll need to install that. We used R version 4.3.2 (2023-10-31) for these notes. To install R, you can go to CRAN1 to download it for your operating system. If you are comfortable at the command line, the rig application is an excellent way to install and manage R versions.\nYou probably want to use an integrated development environment (IDE); it will make your life much better. We use the RStudio IDE, which can be downloaded here. Other applications are Visual Studio and emacs.\nTo use tidymodels, you need to install multiple packages. The core packages are bundled into a “verse” package called tidymodels. When you install that, you get the primary packages as well as some tidyverse packages such as dplyr and ggplot2.\nTo install it, you can use\ninstall.packages(\"tidymodels\")\nWe suggest using the pak package for installation. To do this, first install that and then use it for further installations:\n\ninstall.packages(\"pak\")\n\n# check that it is installed then use it to install tidymodels\nif (require(pak)) {\n  pak::pak(\"tidymodels\")\n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#loading-tidymodels",
    "href": "chapters/introduction.html#loading-tidymodels",
    "title": "1  Introduction",
    "section": "\n1.2 Loading tidymodels",
    "text": "1.2 Loading tidymodels\nOnce you do that, load tidymodels:\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────────────── tidymodels 1.1.1 ──\n#&gt; ✔ broom        1.0.5     ✔ recipes      1.0.8\n#&gt; ✔ dials        1.2.0     ✔ rsample      1.2.0\n#&gt; ✔ dplyr        1.1.4     ✔ tibble       3.2.1\n#&gt; ✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n#&gt; ✔ infer        1.0.5     ✔ tune         1.1.2\n#&gt; ✔ modeldata    1.2.0     ✔ workflows    1.1.3\n#&gt; ✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n#&gt; ✔ purrr        1.0.2     ✔ yardstick    1.2.0\n#&gt; ── Conflicts ────────────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\n#&gt; • Use suppressPackageStartupMessages() to eliminate package startup messages\n\nThe default output shows the packages that are automatically attached. There are a lot of functions in tidy models, but by loading this meta-package, you don’t have to remember which functions come from which packages.\nNote the lines at the bottom that messages like :\n\ndplyr::filter() masks stats::filter()\n\nThis means that two packages, dplyr and stats, have functions with the same name (filter())2. If you were to type filter at an R prompt, the function that you get corresponds to the one in the most recently loaded package. That’s not ideal.\nTo handle this, we have a function called tidymodels_prefer(). When you use this, it prioritizes functions from the tidy models and tidyverse groups so that you get those 3 If you want to see the specific conflicts and how we resolve them, see this output:\n\ntidymodels_prefer(quiet = FALSE)\n#&gt; [conflicted] Will prefer agua::refit over any other package.\n#&gt; [conflicted] Will prefer dials::Laplace over any other package.\n#&gt; [conflicted] Will prefer dials::max_rules over any other package.\n#&gt; [conflicted] Will prefer dials::neighbors over any other package.\n#&gt; [conflicted] Will prefer dials::prune over any other package.\n#&gt; [conflicted] Will prefer dials::smoothness over any other package.\n#&gt; [conflicted] Will prefer dplyr::collapse over any other package.\n#&gt; [conflicted] Will prefer dplyr::combine over any other package.\n#&gt; [conflicted] Will prefer dplyr::filter over any other package.\n#&gt; [conflicted] Will prefer dplyr::rename over any other package.\n#&gt; [conflicted] Will prefer dplyr::select over any other package.\n#&gt; [conflicted] Will prefer dplyr::slice over any other package.\n#&gt; [conflicted] Will prefer ggplot2::`%+%` over any other package.\n#&gt; [conflicted] Will prefer ggplot2::margin over any other package.\n#&gt; [conflicted] Will prefer parsnip::bart over any other package.\n#&gt; [conflicted] Will prefer parsnip::fit over any other package.\n#&gt; [conflicted] Will prefer parsnip::mars over any other package.\n#&gt; [conflicted] Will prefer parsnip::pls over any other package.\n#&gt; [conflicted] Will prefer purrr::cross over any other package.\n#&gt; [conflicted] Will prefer purrr::invoke over any other package.\n#&gt; [conflicted] Will prefer purrr::map over any other package.\n#&gt; [conflicted] Will prefer recipes::discretize over any other package.\n#&gt; [conflicted] Will prefer recipes::step over any other package.\n#&gt; [conflicted] Will prefer rsample::populate over any other package.\n#&gt; [conflicted] Will prefer scales::rescale over any other package.\n#&gt; [conflicted] Will prefer themis::step_downsample over any other package.\n#&gt; [conflicted] Will prefer themis::step_upsample over any other package.\n#&gt; [conflicted] Will prefer tidyr::expand over any other package.\n#&gt; [conflicted] Will prefer tidyr::extract over any other package.\n#&gt; [conflicted] Will prefer tidyr::pack over any other package.\n#&gt; [conflicted] Will prefer tidyr::unpack over any other package.\n#&gt; [conflicted] Will prefer tune::parameters over any other package.\n#&gt; [conflicted] Will prefer tune::tune over any other package.\n#&gt; [conflicted] Will prefer yardstick::get_weights over any other package.\n#&gt; [conflicted] Will prefer yardstick::precision over any other package.\n#&gt; [conflicted] Will prefer yardstick::recall over any other package.\n#&gt; [conflicted] Will prefer yardstick::spec over any other package.\n#&gt; [conflicted] Will prefer recipes::update over Matrix::update.\n#&gt; ── Conflicts ───────────────────────────────────────────────── tidymodels_prefer() ──\n\nIf you want to know more about why tidymodels exists, we’ve written a bit about this in the tidymodels book. The second chapter describes how tidyverse principles can be used for modeling.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#package-versions-and-reproducability",
    "href": "chapters/introduction.html#package-versions-and-reproducability",
    "title": "1  Introduction",
    "section": "\n1.3 Package Versions and Reproducability",
    "text": "1.3 Package Versions and Reproducability\nWe will do our best to use versions of our packages corresponding to the CRAN versions. We can’t always do that, and, for many packages, a version number ending with a value in the 9000 range (e.g., version “1.1.4.9001”) means that it was a development version of the package and was most likely installed from a GitHub repository.\nAt the end of each session, we’ll show which packages were loaded and used:\n\nsessioninfo::session_info()\n#&gt; ─ Session info ────────────────────────────────────────────────────────────────────\n#&gt;  setting  value\n#&gt;  version  R version 4.3.2 (2023-10-31)\n#&gt;  os       macOS Monterey 12.7.1\n#&gt;  system   x86_64, darwin20\n#&gt;  ui       X11\n#&gt;  language (EN)\n#&gt;  collate  en_US.UTF-8\n#&gt;  ctype    en_US.UTF-8\n#&gt;  tz       America/New_York\n#&gt;  date     2023-12-11\n#&gt;  pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#&gt; \n#&gt; ─ Packages ────────────────────────────────────────────────────────────────────────\n#&gt;  ! package      * version    date (UTC) lib source\n#&gt;  P backports      1.4.1      2021-12-13 [?] CRAN (R 4.3.0)\n#&gt;  P broom        * 1.0.5      2023-06-09 [?] CRAN (R 4.3.0)\n#&gt;  P cachem         1.0.8      2023-05-01 [?] CRAN (R 4.3.0)\n#&gt;  P class          7.3-22     2023-05-03 [?] CRAN (R 4.3.2)\n#&gt;  P cli            3.6.2      2023-12-11 [?] CRAN (R 4.3.0)\n#&gt;  P codetools      0.2-19     2023-02-01 [?] CRAN (R 4.3.2)\n#&gt;  P colorspace     2.1-0      2023-01-23 [?] CRAN (R 4.3.0)\n#&gt;  P conflicted     1.2.0      2023-02-01 [?] CRAN (R 4.3.0)\n#&gt;  P data.table     1.14.8     2023-02-17 [?] CRAN (R 4.3.0)\n#&gt;  P dials        * 1.2.0      2023-04-03 [?] CRAN (R 4.3.0)\n#&gt;  P DiceDesign     1.9        2021-02-13 [?] CRAN (R 4.3.0)\n#&gt;  P digest         0.6.33     2023-07-07 [?] CRAN (R 4.3.0)\n#&gt;  P dplyr        * 1.1.4      2023-11-17 [?] CRAN (R 4.3.0)\n#&gt;  P evaluate       0.23       2023-11-01 [?] CRAN (R 4.3.0)\n#&gt;  P fansi          1.0.5      2023-10-08 [?] RSPM (R 4.3.0)\n#&gt;  P fastmap        1.1.1      2023-02-24 [?] CRAN (R 4.3.0)\n#&gt;  P foreach        1.5.2      2022-02-02 [?] CRAN (R 4.3.0)\n#&gt;  P furrr          0.3.1      2022-08-15 [?] CRAN (R 4.3.0)\n#&gt;  P future         1.33.0     2023-07-01 [?] CRAN (R 4.3.0)\n#&gt;  P future.apply   1.11.0     2023-05-21 [?] CRAN (R 4.3.0)\n#&gt;  P generics       0.1.3      2022-07-05 [?] CRAN (R 4.3.0)\n#&gt;  P ggplot2      * 3.4.4      2023-10-12 [?] CRAN (R 4.3.0)\n#&gt;  P globals        0.16.2     2022-11-21 [?] CRAN (R 4.3.0)\n#&gt;  P glue           1.6.2      2022-02-24 [?] CRAN (R 4.3.0)\n#&gt;  P gower          1.0.1      2022-12-22 [?] CRAN (R 4.3.0)\n#&gt;  P GPfit          1.0-8      2019-02-08 [?] CRAN (R 4.3.0)\n#&gt;  P gtable         0.3.4      2023-08-21 [?] CRAN (R 4.3.0)\n#&gt;  P hardhat        1.3.0      2023-03-30 [?] CRAN (R 4.3.0)\n#&gt;  P htmltools      0.5.7      2023-11-03 [?] CRAN (R 4.3.0)\n#&gt;  P htmlwidgets    1.6.2      2023-03-17 [?] CRAN (R 4.3.0)\n#&gt;  P infer        * 1.0.5      2023-09-06 [?] RSPM (R 4.3.0)\n#&gt;  P ipred          0.9-14     2023-03-09 [?] CRAN (R 4.3.0)\n#&gt;  P iterators      1.0.14     2022-02-05 [?] CRAN (R 4.3.0)\n#&gt;  P jsonlite       1.8.7      2023-06-29 [?] CRAN (R 4.3.0)\n#&gt;  P knitr          1.45       2023-10-30 [?] CRAN (R 4.3.0)\n#&gt;  P lattice        0.21-9     2023-10-01 [?] CRAN (R 4.3.1)\n#&gt;  P lava           1.7.3      2023-11-04 [?] CRAN (R 4.3.0)\n#&gt;  P lhs            1.1.6      2022-12-17 [?] CRAN (R 4.3.0)\n#&gt;  P lifecycle      1.0.4      2023-11-07 [?] CRAN (R 4.3.0)\n#&gt;  P listenv        0.9.0      2022-12-16 [?] CRAN (R 4.3.0)\n#&gt;  P lubridate      1.9.3      2023-09-27 [?] RSPM (R 4.3.0)\n#&gt;  P magrittr       2.0.3      2022-03-30 [?] CRAN (R 4.3.0)\n#&gt;  P MASS           7.3-60     2023-05-04 [?] CRAN (R 4.3.2)\n#&gt;  P Matrix         1.6-1.1    2023-09-18 [?] CRAN (R 4.3.1)\n#&gt;  P memoise        2.0.1      2021-11-26 [?] CRAN (R 4.3.0)\n#&gt;  P modeldata    * 1.2.0      2023-08-09 [?] CRAN (R 4.3.0)\n#&gt;  P munsell        0.5.0      2018-06-12 [?] CRAN (R 4.3.0)\n#&gt;  P nnet           7.3-19     2023-05-03 [?] CRAN (R 4.3.2)\n#&gt;  P parallelly     1.36.0     2023-05-26 [?] CRAN (R 4.3.0)\n#&gt;  P parsnip      * 1.1.1      2023-08-17 [?] CRAN (R 4.3.0)\n#&gt;  P pillar         1.9.0      2023-03-22 [?] CRAN (R 4.3.0)\n#&gt;  P pkgconfig      2.0.3      2019-09-22 [?] CRAN (R 4.3.0)\n#&gt;  P prodlim        2023.08.28 2023-08-28 [?] CRAN (R 4.3.0)\n#&gt;  P purrr        * 1.0.2      2023-08-10 [?] CRAN (R 4.3.0)\n#&gt;  P R6             2.5.1      2021-08-19 [?] CRAN (R 4.3.0)\n#&gt;  P Rcpp           1.0.11     2023-07-06 [?] CRAN (R 4.3.0)\n#&gt;  P recipes      * 1.0.8      2023-08-25 [?] CRAN (R 4.3.0)\n#&gt;    renv           1.0.3      2023-09-19 [1] CRAN (R 4.3.0)\n#&gt;  P rlang          1.1.2      2023-11-04 [?] CRAN (R 4.3.0)\n#&gt;  P rmarkdown      2.25       2023-09-18 [?] RSPM (R 4.3.0)\n#&gt;  P rpart          4.1.21     2023-10-09 [?] CRAN (R 4.3.0)\n#&gt;  P rsample      * 1.2.0      2023-08-23 [?] CRAN (R 4.3.0)\n#&gt;  P rstudioapi     0.15.0     2023-07-07 [?] CRAN (R 4.3.0)\n#&gt;  P scales       * 1.2.1      2022-08-20 [?] CRAN (R 4.3.0)\n#&gt;  P sessioninfo    1.2.2      2021-12-06 [?] CRAN (R 4.3.0)\n#&gt;  P survival       3.5-7      2023-08-14 [?] CRAN (R 4.3.0)\n#&gt;  P tibble       * 3.2.1      2023-03-20 [?] CRAN (R 4.3.0)\n#&gt;  P tidymodels   * 1.1.1      2023-08-24 [?] CRAN (R 4.3.0)\n#&gt;  P tidyr        * 1.3.0      2023-01-24 [?] CRAN (R 4.3.0)\n#&gt;  P tidyselect     1.2.0      2022-10-10 [?] CRAN (R 4.3.0)\n#&gt;  P timechange     0.2.0      2023-01-11 [?] CRAN (R 4.3.0)\n#&gt;  P timeDate       4022.108   2023-01-07 [?] CRAN (R 4.3.0)\n#&gt;  P tune         * 1.1.2      2023-08-23 [?] CRAN (R 4.3.0)\n#&gt;  P utf8           1.2.4      2023-10-22 [?] CRAN (R 4.3.0)\n#&gt;  P vctrs          0.6.4      2023-10-12 [?] CRAN (R 4.3.0)\n#&gt;  P withr          2.5.2      2023-10-30 [?] CRAN (R 4.3.0)\n#&gt;  P workflows    * 1.1.3      2023-02-22 [?] CRAN (R 4.3.0)\n#&gt;  P workflowsets * 1.0.1      2023-04-06 [?] CRAN (R 4.3.0)\n#&gt;  P xfun           0.41       2023-11-01 [?] CRAN (R 4.3.0)\n#&gt;  P yaml           2.3.7      2023-01-23 [?] CRAN (R 4.3.0)\n#&gt;  P yardstick    * 1.2.0      2023-04-21 [?] CRAN (R 4.3.0)\n#&gt; \n#&gt;  [1] /Users/max/content/computing-tidymodels/renv/library/R-4.3/x86_64-apple-darwin20\n#&gt;  [2] /Users/max/Library/Caches/org.R-project.R/R/renv/sandbox/R-4.3/x86_64-apple-darwin20/b06620f4\n#&gt; \n#&gt;  P ── Loaded and on-disk path mismatch.\n#&gt; \n#&gt; ───────────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#requirements",
    "href": "chapters/whole-game.html#requirements",
    "title": "2  The Whole Game",
    "section": "\n2.1 Requirements",
    "text": "2.1 Requirements\nYou’ll need 6 packages (brulee, Cubist, patchwork, scales, splines2, and tidymodels) for this chapter. You can install them via:\n\npak::pak(c(\"brulee\", \"Cubist\", \"patchwork\", \"scales\", \"splines2\", \"tidymodels\"))\n\nOnce you’ve installed brulee, you should load it using library(brulee) to install the underlying torch executables. You only have to do this once.\nTwo other packages are described but not directly used: parallel and doParallel.\nLet’s run some code to get started:\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────────────── tidymodels 1.1.1 ──\n#&gt; ✔ broom        1.0.5     ✔ recipes      1.0.8\n#&gt; ✔ dials        1.2.0     ✔ rsample      1.2.0\n#&gt; ✔ dplyr        1.1.4     ✔ tibble       3.2.1\n#&gt; ✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n#&gt; ✔ infer        1.0.5     ✔ tune         1.1.2\n#&gt; ✔ modeldata    1.2.0     ✔ workflows    1.1.3\n#&gt; ✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n#&gt; ✔ purrr        1.0.2     ✔ yardstick    1.2.0\n#&gt; ── Conflicts ────────────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\n#&gt; • Use suppressPackageStartupMessages() to eliminate package startup messages\nlibrary(probably)\n#&gt; \n#&gt; Attaching package: 'probably'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.factor, as.ordered\nlibrary(patchwork)\n\ntidymodels_prefer()\n\nFinally, this note:\n\n\n\n\n\n\nNote\n\n\n\nAll of these notes will assume that you have an R session that is running from the root of the directory containing the GitHub repository files. In other words, if you were to execute list.dirs(recursive = FALSE), the output would show entries such as \"./chapters\", \"./RData\", etc.\nIf you are not in the right place, use setwd() to change the working directory to the correct location.\nIf you start by opening the Rproj file, you will always start in the right place.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#the-data",
    "href": "chapters/whole-game.html#the-data",
    "title": "2  The Whole Game",
    "section": "\n2.2 The Data",
    "text": "2.2 The Data\nThe data set is pre-compiled into a binary format R uses (called “RData format” here). It is in the RData directory. A csv version is also in the delimited directory. Let’s load it:\n\nload(\"RData/deliveries.RData\")\n\nThere are a lot of ways that you can examine the contents of an object. View() is good for data frames; in the RStudio IDE, it opens a spreadsheet-like viewer. tibble::glimpse() shows more details about the object, such as classes, but can be a bad choice if you have &gt;50 columns in the data (or if it is a long list or similar). We’ll use that:\n\nglimpse(deliveries)\n#&gt; Rows: 10,012\n#&gt; Columns: 31\n#&gt; $ time_to_delivery &lt;dbl&gt; 16.11, 22.95, 30.29, 33.43, 27.23, 19.65, 22.10, 26.63, 3…\n#&gt; $ hour             &lt;dbl&gt; 11.90, 19.23, 18.37, 15.84, 19.62, 12.95, 15.48, 17.05, 1…\n#&gt; $ day              &lt;fct&gt; Thu, Tue, Fri, Thu, Fri, Sat, Sun, Thu, Fri, Sun, Tue, Fr…\n#&gt; $ distance         &lt;dbl&gt; 3.15, 3.69, 2.06, 5.97, 2.52, 3.35, 2.46, 2.21, 2.62, 2.7…\n#&gt; $ item_01          &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n#&gt; $ item_02          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, …\n#&gt; $ item_03          &lt;int&gt; 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n#&gt; $ item_04          &lt;int&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_05          &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_06          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, …\n#&gt; $ item_07          &lt;int&gt; 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, …\n#&gt; $ item_08          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, …\n#&gt; $ item_09          &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, …\n#&gt; $ item_10          &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_11          &lt;int&gt; 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_12          &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_13          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_14          &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_15          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_16          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_17          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_18          &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, …\n#&gt; $ item_19          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_20          &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_21          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_22          &lt;int&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, …\n#&gt; $ item_23          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_24          &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_25          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, …\n#&gt; $ item_26          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_27          &lt;int&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, …\n\nWe can see that this is a data frame and, more specifically a specialized version called a tibble. There are 10,012 data points and 31 columns and their types.\nNote that the day column is a factor. This is the preferred way to represent most categorical data (for modeling, at least). A factor catalogs the possible values of the data and stores those levels. That is important when we convert categorical predictors to “dummy variables” or “indicators” and similar operations.\nIn some cases, storing categorical data as integers might seem like a good idea (especially 0/1 for binary data). Do your best to avoid that. R (and tidymodels) would instead you use a data type that is designed explicitly for categories (a factor); it knows what to do with factors. If an integer is used, R can’t distinguish this from a column of counts (such as the number of times that item_01 was included in the order).\nTo create the histograms of the delivery times, we used this code to create each:\n\n# Setup some fancy code for the axes: \nlog_2_breaks &lt;- scales::trans_breaks(\"log2\", function(x) 2^x)\nlog_2_labs   &lt;- scales::trans_format(\"log2\", scales::math_format(2^.x))\n\ndelivery_hist &lt;- \n  deliveries %&gt;% \n  ggplot(aes(x = time_to_delivery)) +\n  geom_histogram(bins = 30, col = \"white\") +\n  geom_rug(alpha = 1 / 4) +\n  labs(x = \"Time Until Delivery (min)\", title = \"(a)\")\n\ndelivery_log_hist &lt;- \n  deliveries %&gt;% \n  ggplot(aes(x = time_to_delivery)) +\n  geom_histogram(bins = 30, col = \"white\") +\n  geom_rug(alpha = 1 / 4) +\n  labs(x = \"Time Until Delivery (min)\", title = \"(b)\") +\n  scale_x_log10(breaks = log_2_breaks, labels = log_2_labs)\n\nYou don’t need to assign the plots to objects; you can just print each. We did this so that we can concatenate the two plots with the patchwork package1:\n\ndelivery_hist + delivery_log_hist\n\n\n\n\n\n\n\nIn the code above, we use an option called \"alpha\". This is jargon for transparency; a value of 1/4 means that the points in the rug are 25% opaque.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#data-spending",
    "href": "chapters/whole-game.html#data-spending",
    "title": "2  The Whole Game",
    "section": "\n2.3 Data Spending",
    "text": "2.3 Data Spending\ntidymodels has a variety of ways to split the data at the outset of your modeling project. We will create a three-way split of the data using a function called initial_validation_split().\nIt uses random numbers so we will set the random number seed before using it.\n\n\n\n\n\n\nWhat’s a random number seed?\n\n\n\nWe are using random numbers (actually pseudo-random numbers). We want to get the same “random” values every time we run the same code for reproducibility. To do that, we use the set.seed() function and give it an integer value. The value itself doesn’t matter.\nThe random number stream is like a river. If you want to see the same things in your journey down the river, you must get in at the same exact spot. The seed is like the location where you start a journey (that is always the same).\n\n\nThe code is below.\n\nThe prop argument shows the fraction of the original data that should go into the training set (60%) and the validation set (20%). The remaining 20% are put in the test set.\nThe strata argument specifies that the splitting should consider the outcome column (time_to_delivery). This will be discussed in a future section. In short, the three-way splitting is done in different regions of the outcome data in a way that makes the distribution of the outcome as similar as possible across the three partitions.\n\nWe used a value of 991 to set the seed2:\n\nset.seed(991)\ndelivery_split &lt;-\n  initial_validation_split(deliveries, prop = c(0.6, 0.2), strata = time_to_delivery)\n\n# What is in it? \ndelivery_split\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;6004/2004/2004/10012&gt;\n\nThis object records which rows of the original data go into the training, validation, or test sets. The printed output shows the totals for each as &lt;train/val/test/total&gt;.\nTo get the data frames with the correct rows, use these three eponymous functions:\n\ndelivery_train &lt;- training(delivery_split)\ndelivery_test  &lt;- testing(delivery_split)\ndelivery_val   &lt;- validation(delivery_split)\n\nWe will mostly work with the training set of 6,004 deliveries. We’ll use that to explore the data, fit models, and so on.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#exploratory-data-analysis",
    "href": "chapters/whole-game.html#exploratory-data-analysis",
    "title": "2  The Whole Game",
    "section": "\n2.4 Exploratory Data Analysis",
    "text": "2.4 Exploratory Data Analysis\nWe mostly used ggplot2 and patchwork to create these graphics:\n\n# Make specific colors for each day\nday_cols &lt;-  c(\"#000000FF\", \"#24FF24FF\", \"#009292FF\",  \"#B66DFFFF\", \n               \"#6DB6FFFF\", \"#920000FF\", \"#FFB6DBFF\")\n\ndelivery_dist &lt;- \n  delivery_train %&gt;% \n  ggplot(aes(x = distance, time_to_delivery)) +\n  geom_point(alpha = 1 / 10, cex = 1) +\n  labs(y = \"Time Until Delivery (min)\", x = \"Distance (miles)\", title = \"(a)\") +\n  # This function creates the smooth trend line. The `se` option shuts off the\n  # confidence band around the line; too much information to put into one plot. \n  geom_smooth(se = FALSE, col = \"red\")\n\ndelivery_day &lt;- \n  delivery_train %&gt;% \n  ggplot(aes(x = day, time_to_delivery, col = day)) +\n  geom_boxplot(show.legend = FALSE)  +\n  labs(y = \"Time Until Delivery (min)\", x = NULL, title = \"(c)\") +\n  scale_color_manual(values = day_cols)\n\ndelivery_time &lt;- \n  delivery_train %&gt;% \n  ggplot(aes(x = hour, time_to_delivery)) +\n  labs(y = \"Time Until Delivery (min)\", x = \"Order Time (decimal hours)\", title = \"(b)\") +\n  geom_point(alpha = 1 / 10, cex = 1) + \n  geom_smooth(se = FALSE, col = \"red\")\n\ndelivery_time_day &lt;- \n  delivery_train %&gt;% \n  ggplot(aes(x = hour, time_to_delivery, col = day)) +\n  labs(y = \"Time Until Delivery (min)\", x = \"Order Time (decimal hours)\", title = \"(d)\") +\n  # With `col = day`, the trends will be estimated separately for each value of 'day'.\n  geom_smooth(se = FALSE) + \n  scale_color_manual(values = day_cols)\n\npatchwork puts it together.\n\n# Row 1\n( delivery_dist + delivery_time ) / \n  # Row 2\n  ( delivery_day + delivery_time_day ) +\n  # Consolidate the legends\n  plot_layout(guides = 'collect')  & \n  # Place the legend at the bottom\n  theme(legend.title = element_blank(), legend.position = \"bottom\")\n#&gt; `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n#&gt; `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n#&gt; `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\nggplot2 is a bit noisy. The messages tell you details about how it made the smooth trend line. The code s(x, bs = \"cs\") defines a spline smoother that we will see more of shortly (using a different function).\nThe methods that we used to compute the effects of the item_* columns are more complicated. We must make probabilistic assumptions about the data if we want to get something like a confidence interval. Alternatively, we could specify the empirical distribution function via the bootstrap resampling method. This helps us estimate the standard error of some statistic and use that to compute an interval.\nFirst, we make a function that takes some data and computes our statistics of interest. It assumes x is the entire data set with the delivery time column and each item column.\n\ntime_ratios &lt;- function(x) {\n  x %&gt;%\n    # The items are in columns; we'll stack these columns on one another.\n    pivot_longer(\n      cols = c(starts_with(\"item\")),\n      names_to = \"predictor\",\n      values_to = \"count\"\n    ) %&gt;%\n    # Collapse the counts into a \"in order\"/\"not in order\" variable. \n    mutate(ordered = ifelse(count &gt; 0, \"yes\", \"no\")) %&gt;%\n    # Compute, for each value of the 'predictor' and 'ordered' columns, \n    # the mean delivery time. \n    summarize(mean = mean(time_to_delivery),\n              .by = c(predictor, ordered)) %&gt;%\n    # Move the means to columns for when they were in the order \n    # and when they were not. The new column names are `yes` and `no`.\n    pivot_wider(id_cols = predictor,\n                names_from = ordered,\n                values_from = mean) %&gt;%\n    # Compute the ratio. This is a fold-difference in delivery times.\n    mutate(ratio = yes / no) %&gt;%\n    select(term = predictor, estimate = ratio)\n}\n\nWhen run in the training set:\n\ntime_ratios(delivery_train)\n#&gt; # A tibble: 27 × 2\n#&gt;   term    estimate\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 item_01    1.074\n#&gt; 2 item_02    1.010\n#&gt; 3 item_03    1.010\n#&gt; 4 item_04    1.002\n#&gt; 5 item_05    1.005\n#&gt; 6 item_06    1.018\n#&gt; # ℹ 21 more rows\n\nA value of 1.07 means that there is a 7% increase in the delivery time when that item is in the order at least once.\nA tidymodels function called int_pctl() can take a collection of bootstrap samples of a data set, compute their statistics, and use the results to produce confidence intervals (we’ll use 90% intervals). To use it, we’ll resample the training set using the bootstraps() function and then use a mutate() to compute the fold differences.\nWe are using random numbers again, so let’s reset the seed3.\n\nset.seed(624)\nresampled_data &lt;- \n  delivery_train %&gt;% \n  select(time_to_delivery, starts_with(\"item\")) %&gt;% \n  # This takes a while to compute. The materials use 5000 bootstraps\n  # but a smaller number is used here for demonstration.\n  bootstraps(times = 1001) \n\nresampled_data\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 1,001 × 2\n#&gt;   splits              id           \n#&gt;   &lt;list&gt;              &lt;chr&gt;        \n#&gt; 1 &lt;split [6004/2227]&gt; Bootstrap0001\n#&gt; 2 &lt;split [6004/2197]&gt; Bootstrap0002\n#&gt; 3 &lt;split [6004/2156]&gt; Bootstrap0003\n#&gt; 4 &lt;split [6004/2210]&gt; Bootstrap0004\n#&gt; 5 &lt;split [6004/2208]&gt; Bootstrap0005\n#&gt; 6 &lt;split [6004/2227]&gt; Bootstrap0006\n#&gt; # ℹ 995 more rows\n\nThe splits column contains the information on each bootstrap sample. To get a specific bootstrap sample, we can use the analysis(split_object) function on each element of the splits column. purrr::map() takes each split, extracts the bootstrap sample, then computes all of the ratios4.\n\nresampled_ratios &lt;- \n  resampled_data %&gt;% \n  mutate(stats = map(splits, ~ time_ratios(analysis(.x))))\n\nresampled_ratios\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 1,001 × 3\n#&gt;   splits              id            stats            \n#&gt;   &lt;list&gt;              &lt;chr&gt;         &lt;list&gt;           \n#&gt; 1 &lt;split [6004/2227]&gt; Bootstrap0001 &lt;tibble [27 × 2]&gt;\n#&gt; 2 &lt;split [6004/2197]&gt; Bootstrap0002 &lt;tibble [27 × 2]&gt;\n#&gt; 3 &lt;split [6004/2156]&gt; Bootstrap0003 &lt;tibble [27 × 2]&gt;\n#&gt; 4 &lt;split [6004/2210]&gt; Bootstrap0004 &lt;tibble [27 × 2]&gt;\n#&gt; 5 &lt;split [6004/2208]&gt; Bootstrap0005 &lt;tibble [27 × 2]&gt;\n#&gt; 6 &lt;split [6004/2227]&gt; Bootstrap0006 &lt;tibble [27 × 2]&gt;\n#&gt; # ℹ 995 more rows\n\n# An example: \nresampled_ratios$stats[[1]]\n#&gt; # A tibble: 27 × 2\n#&gt;   term    estimate\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 item_01    1.070\n#&gt; 2 item_02    1.018\n#&gt; 3 item_03    1.008\n#&gt; 4 item_04    1.008\n#&gt; 5 item_05    1.017\n#&gt; 6 item_06    1.007\n#&gt; # ℹ 21 more rows\n\nrsample::int_pctl() can consume these results and produce an interval for each item column5.\n\nresampled_intervals &lt;- \n  resampled_ratios %&gt;% \n  int_pctl(stats, alpha = 0.1) \n\nresampled_intervals\n#&gt; # A tibble: 27 × 6\n#&gt;   term    .lower .estimate .upper .alpha .method   \n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n#&gt; 1 item_01 1.052      1.075  1.097    0.1 percentile\n#&gt; 2 item_02 0.9950     1.009  1.024    0.1 percentile\n#&gt; 3 item_03 0.9940     1.009  1.024    0.1 percentile\n#&gt; 4 item_04 0.9879     1.002  1.016    0.1 percentile\n#&gt; 5 item_05 0.9884     1.005  1.021    0.1 percentile\n#&gt; 6 item_06 1.002      1.018  1.033    0.1 percentile\n#&gt; # ℹ 21 more rows\n\nHere’s our plot:\n\nresampled_intervals %&gt;% \n  # Convert the folds to percentages and make the item values\n  # a little cleaner:\n  mutate(\n    term = gsub(\"_0\", \" \", term),\n    term = factor(gsub(\"_\", \" \", term)),\n    term = reorder(term, .estimate),\n    increase = .estimate - 1,\n  ) %&gt;% \n  ggplot(aes(increase, term)) + \n  geom_vline(xintercept = 0, col = \"red\", alpha = 1 / 3) +\n  geom_point() + \n  geom_errorbar(aes(xmin = .lower - 1, xmax = .upper - 1), width = 1 / 2) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(y = NULL, x = \"Increase in Delivery Time When Ordered\") +\n  theme(axis.text.y = element_text(hjust = 0))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#model-development",
    "href": "chapters/whole-game.html#model-development",
    "title": "2  The Whole Game",
    "section": "\n2.5 Model Development",
    "text": "2.5 Model Development\nThe analyses in this section define a model pipeline, fit it to the training set, and then measure performance using the validation set. We’ll review the three evaluated models and describe how those computations were done.\nBefore we get started, we need to specify how to measure model effectiveness. The materials use the mean absolute error (MAE). To specify this performance metric, you can use the yardstick::metric_set() function and give it the function names for specific metrics (like the yardstick::mae() function):\n\nreg_metrics &lt;- metric_set(mae)\n\nWe’ll show you how to use reg_metrics in a bit.\nLinear Regression\nThe linear regression model is fairly simple to define and fit. Before we get to that, we must introduce a major tidymodels component: the recipe.\nA recipe is a set of instructions defining a potential series of computations on the predictor variables to put them into a format the model (or data set) requires. For example, the day-of-the-week factor must be converted into a numeric format. We’ll use the standard “Dummy variable” approach to do that. Additionally, our exploratory data analysis discovered that:\n\nThere is a nonlinear relationship between the outcome and the time of the order.\nThis nonlinear relationship is different for different days. This is an interaction effect between a qualitative predictor (day) and a nonlinear function of another (hour).\nThere also appeared to be an additional nonlinear effect for the order distance.\n\nWe can initialize a recipe using a simple formula method:\n\nspline_rec &lt;- recipe(time_to_delivery ~ ., data = delivery_train)\n\nThere are a few things that this function call does:\n\nThe formula declares that the column time_to_delivery is the outcome (since it is on the left-hand side of the tilde). The dot on the right-hand side indicates that all of the columns in delivery_train, besides the outcome, should be treated as predictors.\nThe recipe collects information on each column’s type. For example, it understands that day is a factor and that the item_* columns are numeric.\n\nLet’s add to the recipe by converting day to indicator columns. We do this by adding a step to the recipe via:\n\nspline_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) \n\nThe first argument to step functions is the variables that should be affected by the function. We can use any dplyr selector such as everything() and/or the bare column names. Here, we want to change every factor column that was the role of “predictor”. For this purpose, recipes have an extended set of selector functions.\nOnce the recipe is processed, this step will record which columns were captured by all_factor_predictors(), retain their factor levels, then convert them to a set of 0/1 indicators for each predictor/level.\nUnlike base R’s formula method, the resulting columns are named rationally. By default, it uses the pattern {column name}_{level} for the new features. So, the column day will not exist after this step. It is replaced with columns such as day_Thursday and so on.\nThe next recipe step is probably unnecessary for this data set but automatically using it is not problematic. What happens if there is a factor level that occurs very infrequently? It is possible that this will only be observed in the validation or test set. step_dummy() will make a column for that factor level since it knows it exists but the training set will have all zeros for this column; it has zero variance. We can screen these out using step_zv() (‘zv’ = zero-variance):\n\nspline_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  step_zv(all_predictors()) \n\nNow, we can address the nonlinear effects. We’ll use a spline basis expansion (described later on the main page) that creates additional columns from some numeric predictor. We’ll use a natural spline function and create ten new columns for both hour and distance:\n\nspline_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(hour, distance, deg_free = 10)\n\nThe naming convention for these new features are hour_01 … hour_10 and so on. The original hour column is removed (same for the distance column).\nThis step allows the linear regression to have nonlinear relationships between predictors and the outcome.\nFinally, we can create interactions. In base R, an interaction between variables a and b is specified in the formula using a:b. We’ll use the same method here with step_interact(). The main difference is that the columns day and hour no longer exist at this point. To capture all of the interactions, we can use the : convention with selector functions. Using starts_wth(\"day_\") will capture the existing indicator columns and, similarly, starts_wth(\"hour_\") finds the appropriate spline terms. Our final recipe is then:\n\nspline_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(hour, distance, deg_free = 10) %&gt;% \n  step_interact(~ starts_with(\"hour_\"):starts_with(\"day_\"))\n\n\n\n\n\n\n\nLearn More About Recipes\n\n\n\nYou can learn more about recipes later and there is material in the tidymodels book as well as tidymodels.org.\n\nFeature Engineering with recipes\nDimensionality Reduction\nEncoding Categorical Data\nGet Started: Preprocess your data with recipes\nArticles with recipes\nA list of recipe steps on CRAN\n\n\n\nTo specify the linear regression model, we use one of the functions from the parsnip package called… linear_reg(). Since we are using ordinary least squares, this function defaults to stats::lm().\n\n# This creates a model specification: \nlm_spec &lt;- linear_reg()\nlm_spec\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\nThe engine mentioned here is the computational method to fit the model. R has many ways to do this and \"lm\" is the default engine.\nHow do we combine the recipe and the model specifications? The best approach is to make a pipeline-like object called a workflow:\n\nlin_reg_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_spec) %&gt;% \n  add_recipe(spline_rec)\n\nWe can use the fit() function to fit the workflow to the training set. This executes the recipe on the data set then passes the appropriate data to stats::lm():\n\nlin_reg_fit &lt;- fit(lin_reg_wflow, data = delivery_train)\n\nWe can print the results out but the results are kind of long:\n\nlin_reg_fit\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────────────\n#&gt; 4 Recipe Steps\n#&gt; \n#&gt; • step_dummy()\n#&gt; • step_zv()\n#&gt; • step_spline_natural()\n#&gt; • step_interact()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)            item_01            item_02            item_03  \n#&gt;           13.3434             1.2444             0.6461             0.7313  \n#&gt;           item_04            item_05            item_06            item_07  \n#&gt;            0.2820             0.5839             0.5250             0.5063  \n#&gt;           item_08            item_09            item_10            item_11  \n#&gt;            0.6376             0.7368             1.5341             0.5172  \n#&gt;           item_12            item_13            item_14            item_15  \n#&gt;            0.5945             0.5799             0.5080             0.6432  \n#&gt;           item_16            item_17            item_18            item_19  \n#&gt;            0.4309             0.6089             0.4028            -0.3568  \n#&gt;           item_20            item_21            item_22            item_23  \n#&gt;            0.5280             0.7743             0.5105             0.6876  \n#&gt;           item_24            item_25            item_26            item_27  \n#&gt;            0.8126             0.4768             0.5274             0.5773  \n#&gt;           day_Tue            day_Wed            day_Thu            day_Fri  \n#&gt;           -1.1200            -2.8421            -3.7059            -4.3130  \n#&gt;           day_Sat            day_Sun            hour_01            hour_02  \n#&gt;           -4.6659            -3.0919             1.8776             2.5588  \n#&gt;           hour_03            hour_04            hour_05            hour_06  \n#&gt;            2.5826             3.8464             2.5826             4.0220  \n#&gt;           hour_07            hour_08            hour_09            hour_10  \n#&gt;            4.0112             5.9056            -0.8782            11.1905  \n#&gt;       distance_01        distance_02        distance_03        distance_04  \n#&gt;           -0.0523             0.5777             0.6157             0.7524  \n#&gt;       distance_05        distance_06        distance_07        distance_08  \n#&gt;            1.6695             1.7837             2.9039             3.3200  \n#&gt;       distance_09        distance_10  hour_01_x_day_Tue  hour_01_x_day_Wed  \n#&gt;          -19.3835            75.2868             1.3304             1.2875  \n#&gt; hour_01_x_day_Thu  hour_01_x_day_Fri  hour_01_x_day_Sat  hour_01_x_day_Sun  \n#&gt;            2.4448             1.3560             2.4253             1.5058  \n#&gt; hour_02_x_day_Tue  hour_02_x_day_Wed  hour_02_x_day_Thu  hour_02_x_day_Fri  \n#&gt;            0.2268             3.1441             3.9958             4.8608  \n#&gt; hour_02_x_day_Sat  hour_02_x_day_Sun  hour_03_x_day_Tue  hour_03_x_day_Wed  \n#&gt;            4.8380             3.7002             2.9359             6.4783  \n#&gt; hour_03_x_day_Thu  hour_03_x_day_Fri  hour_03_x_day_Sat  hour_03_x_day_Sun  \n#&gt;            7.4590             9.9423            10.9673             5.7549  \n#&gt; hour_04_x_day_Tue  hour_04_x_day_Wed  hour_04_x_day_Thu  hour_04_x_day_Fri  \n#&gt;            1.6701             6.2609             9.7183            12.1586  \n#&gt; hour_04_x_day_Sat  hour_04_x_day_Sun  hour_05_x_day_Tue  hour_05_x_day_Wed  \n#&gt;           13.9715             7.9502             3.9981             9.4129  \n#&gt; hour_05_x_day_Thu  hour_05_x_day_Fri  hour_05_x_day_Sat  hour_05_x_day_Sun  \n#&gt;           12.1748            16.5402            17.5038             9.3518  \n#&gt; hour_06_x_day_Tue  hour_06_x_day_Wed  hour_06_x_day_Thu  hour_06_x_day_Fri  \n#&gt;            2.5202             8.2079            11.9678            15.7150  \n#&gt; hour_06_x_day_Sat  hour_06_x_day_Sun  hour_07_x_day_Tue  hour_07_x_day_Wed  \n#&gt; \n#&gt; ...\n#&gt; and 14 more lines.\n\nOne helpful function is tidy(). It is designed to return the object results rationally, helpfully. In our case, the tidy() method for an lm object gives us a nice data frame back with information on the fitted coefficients:\n\ntidy(lin_reg_fit)\n#&gt; # A tibble: 114 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)  13.34     1.585       8.420 4.664e-17\n#&gt; 2 item_01       1.244    0.1031     12.08  3.498e-33\n#&gt; 3 item_02       0.6461   0.06865     9.412 6.852e-21\n#&gt; 4 item_03       0.7313   0.06914    10.58  6.463e-26\n#&gt; 5 item_04       0.2820   0.06260     4.505 6.776e- 6\n#&gt; 6 item_05       0.5839   0.07871     7.418 1.360e-13\n#&gt; # ℹ 108 more rows\n\nUnlike the summary() method for lm objects, this object can immediately be used in plots or tables.\nAnother valuable supporting function is augment(). It can take a model object and data set and attach the prediction columns to the data frame. Essentially, this is an upgraded version of predict(). Let’s predict the validation set:\n\nlm_reg_val_pred &lt;- augment(lin_reg_fit, new_data = delivery_val)\nnames(lm_reg_val_pred)\n#&gt;  [1] \"time_to_delivery\" \"hour\"             \"day\"              \"distance\"        \n#&gt;  [5] \"item_01\"          \"item_02\"          \"item_03\"          \"item_04\"         \n#&gt;  [9] \"item_05\"          \"item_06\"          \"item_07\"          \"item_08\"         \n#&gt; [13] \"item_09\"          \"item_10\"          \"item_11\"          \"item_12\"         \n#&gt; [17] \"item_13\"          \"item_14\"          \"item_15\"          \"item_16\"         \n#&gt; [21] \"item_17\"          \"item_18\"          \"item_19\"          \"item_20\"         \n#&gt; [25] \"item_21\"          \"item_22\"          \"item_23\"          \"item_24\"         \n#&gt; [29] \"item_25\"          \"item_26\"          \"item_27\"          \".pred\"\n\nWhat is our MAE? This is where we use our metric set reg_metrics. Note that there is a column in the results called .pred. For regression models, this is the predicted delivery time for each order in the validation set. We can use that and the original observed outcome column to estimate the MAE6:\n\nlm_reg_val_pred %&gt;% \n  reg_metrics(truth = time_to_delivery, estimate = .pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       1.609\n\nThe units are fractional minutes.\nAt this point, we can make diagnostic plots of our data and so on.\nLet’s take a minor distraction that will pay off a bit later. The main page mentions that we can treat the validation set as a single resample of the data. If we were to do that, our code wouldn’t have to change much when we get into more complex scenarios such as cross-validation or model tuning. To do this, we can convert the initial split object into a resampling set (a.k.a. an rset):\n\ndelivery_rs &lt;- validation_set(delivery_split)\n\nclass(delivery_rs)\n#&gt; [1] \"validation_set\" \"rset\"           \"tbl_df\"         \"tbl\"           \n#&gt; [5] \"data.frame\"\n\ndelivery_rs\n#&gt; # A tibble: 1 × 2\n#&gt;   splits              id        \n#&gt;   &lt;list&gt;              &lt;chr&gt;     \n#&gt; 1 &lt;split [6004/2004]&gt; validation\n\nThis packages the training and validation sets together in a way that it knows when to use each data set appropriately.\nSince we are treating this as if it were resampling, we can use the fit_resamples() function to do much of the manual work we just showed. We’ll add a control object to the mix to specify that we want to retain the validation set predictions (and our original workflow).\n\nctrl_rs &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\nlin_reg_res &lt;-\n  fit_resamples(lin_reg_wflow,\n                resamples = delivery_rs,\n                control = ctrl_rs,\n                metrics = reg_metrics)\n\nThe benefit here is that there are a lot of helper functions to simplify your code. For example, to get the validation set MAE and predictions:\n\ncollect_metrics(lin_reg_res)\n#&gt; # A tibble: 1 × 6\n#&gt;   .metric .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   1.609     1      NA Preprocessor1_Model1\n\ncollect_predictions(lin_reg_res)\n#&gt; # A tibble: 2,004 × 5\n#&gt;   id         .pred  .row time_to_delivery .config             \n#&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 validation 30.12  6005            27.23 Preprocessor1_Model1\n#&gt; 2 validation 23.02  6006            22.10 Preprocessor1_Model1\n#&gt; 3 validation 28.38  6007            26.63 Preprocessor1_Model1\n#&gt; 4 validation 31.04  6008            30.84 Preprocessor1_Model1\n#&gt; 5 validation 38.57  6009            41.17 Preprocessor1_Model1\n#&gt; 6 validation 27.04  6010            27.04 Preprocessor1_Model1\n#&gt; # ℹ 1,998 more rows\n\nThe probably package also has a nice helper to check the calibration of the model via a plot:\n\ncal_plot_regression(lin_reg_res, alpha = 1 / 3)\n\n\n\n\n\n\n\nRule-Based Ensemble\nTo fit the Cubist model, we need to load one of the tidymodels extension packages called rules. It has the tools to fit this model and will automatically (and silently) use the Cubist package when the model fit occurs.\nWe’ll create a model specification that has an enselmble size of 100:\n\nlibrary(rules)\n\n# A model specification: \ncb_spec &lt;- cubist_rules(committees = 100)\n\nOne advantage of rule-based models is that very little preprocessing is required (i.e., no dummy variables or spline terms). For that reason, we’ll use a simple R formula instead of a recipe:\n\ncb_wflow &lt;- \n  workflow() %&gt;% \n  add_model(cb_spec) %&gt;% \n  add_formula(time_to_delivery ~ .)\n\nLet’s go straight to fit_resamples():\n\ncb_res &lt;-\n  fit_resamples(\n    cb_wflow, \n    resamples = delivery_rs, \n    control = ctrl_rs, \n    metrics = reg_metrics\n  )\n\ncollect_metrics(cb_res)\n#&gt; # A tibble: 1 × 6\n#&gt;   .metric .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   1.416     1      NA Preprocessor1_Model1\n\nThe calibration plot:\n\ncal_plot_regression(cb_res, alpha = 1 / 3)\n\n\n\n\n\n\n\nThis is pretty simple and demonstrates that, after an initial investment in learning tidymodels syntax, the process of fitting different models does not require huge changes to your scripts.\nTo get the model fit, we previously used fit(). With resampling objects (and the tuning objects that we are about to see), there is another helper function called fit_best() that will create the model from the entire training set using the resampling results7:\n\ncb_fit &lt;- fit_best(cb_res)\n\ncb_fit\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: cubist_rules()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────────────\n#&gt; time_to_delivery ~ .\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; cubist.default(x = x, y = y, committees = 100)\n#&gt; \n#&gt; Number of samples: 6004 \n#&gt; Number of predictors: 30 \n#&gt; \n#&gt; Number of committees: 100 \n#&gt; Number of rules per committee: 31, 22, 23, 26, 21, 24, 23, 23, 22, 23, 21, 22, 18, 24, 20, 22, 16, 26, 25, 26 ...\n\nThe tidy() method is also helpful here. It contains all of the rules and corresponding regression models. Let’s get these values for the second rule in the fourth ensemble:\n\nrule_details &lt;- tidy(cb_fit)\n\nrule_details %&gt;% \n  filter(committee == 4 & rule_num == 2) %&gt;% \n  pluck(\"rule\")\n#&gt; [1] \"( hour &lt;= 14.946 ) & ( day  %in% c( 'Mon','Tue','Wed','Thu','Sun' ) ) & ( distance &lt;= 4.52 )\"\n\nrule_details %&gt;% \n  filter(committee == 4 & rule_num == 2) %&gt;% \n  select(estimate) %&gt;% \n  pluck(1) %&gt;% \n  pluck(1)\n#&gt; # A tibble: 15 × 2\n#&gt;   term        estimate\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 (Intercept)   -11.24\n#&gt; 2 hour            2.05\n#&gt; 3 distance        0.78\n#&gt; 4 item_01        -0.5 \n#&gt; 5 item_02         0.3 \n#&gt; 6 item_05         0.8 \n#&gt; # ℹ 9 more rows\n\nNeural Network\nThe model function for this type of model is parsnip::mlp() (MLP is short for “multi-layer perceptron”). There are quite a few packages for neural networks in R. tidymodels has interfaces to several engines:\n\nshow_engines(\"mlp\")\n#&gt; # A tibble: 6 × 2\n#&gt;   engine mode          \n#&gt;   &lt;chr&gt;  &lt;chr&gt;         \n#&gt; 1 keras  classification\n#&gt; 2 keras  regression    \n#&gt; 3 nnet   classification\n#&gt; 4 nnet   regression    \n#&gt; 5 brulee classification\n#&gt; 6 brulee regression\n\nWe’ll use the brulee package. This uses the torch software to fit the model. We’ll only tune the number of hidden units (for now, see later chapters). To mark any parameter for tuning, we pass the tune() function to an argument:\n\nnnet_spec &lt;- \n  mlp(\n    hidden_units = tune(),\n    # Some specific argument values that we chose:\n    penalty = 0.01,\n    learn_rate = 0.1,\n    epochs = 5000\n  ) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"brulee\", stop_iter = 10, rate_schedule = \"cyclic\")\n\nA few notes on this:\n\nThe arguments to mlp() are called the main arguments since they are used by several engines.\nThe default engine uses the nnet package; set_engine() specifies that we want to use the brulee package instead.\nTwo arguments (stop_iter and rate_schedule) are specific to our engine. We set them here (and can also pass a tune() value to them).\nNeural networks can fit both classification and regression models. We must state what model type (called a “mode”) to create.\n\nThis model requires the conversion to dummy variables but does not require features to handle nonlinear trends and interactions. One additional preprocessing step that is required is to put the predictors in the same units (e.g., not miles or hours). There are a few ways to do this. We will center and scale the predictors using recipes::step_normalize():\n\nnorm_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\nnnet_wflow &lt;- \n  workflow() %&gt;% \n  add_model(nnet_spec) %&gt;% \n  add_recipe(norm_rec)\n\nUnlike the previous models, we are tuning one of the hyper-parameters. Instead of fit_resamples(), we’ll use tune_grid() to search over a predefined set of values for the number of hidden units. We can set how many values we should try or directly declare the candidate values. We’ll do the latter and, for simplicity, use a smaller range of values.\nFinally, we’ll use another control function:\n\n# The main materials used 2:100\nnnet_grid &lt;- tibble(hidden_units = 2:10)\n\nctrl_grid &lt;- control_grid(save_pred = TRUE, save_workflow = TRUE)\n\n# The model initializes the parameters with random numbers so set the seed:\nset.seed(388)\nnnet_res &lt;-\n  tune_grid(nnet_wflow,\n            resamples = delivery_rs,\n            control = ctrl_grid,\n            grid = nnet_grid,\n            metrics = reg_metrics)\n\nThere are some additional helper functions for model tuning. For example, we can rank the models based on a metric:\n\nshow_best(nnet_res, metric = \"mae\")\n#&gt; # A tibble: 5 × 7\n#&gt;   hidden_units .metric .estimator  mean     n std_err .config             \n#&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1            7 mae     standard   1.540     1      NA Preprocessor1_Model6\n#&gt; 2           10 mae     standard   1.555     1      NA Preprocessor1_Model9\n#&gt; 3            9 mae     standard   1.577     1      NA Preprocessor1_Model8\n#&gt; 4            8 mae     standard   1.649     1      NA Preprocessor1_Model7\n#&gt; 5            5 mae     standard   1.746     1      NA Preprocessor1_Model4\n\nWe can also return the parameter with the numerically best results8:\n\nbest_hidden_units &lt;- select_best(nnet_res, metric = \"mae\")\nbest_hidden_units\n#&gt; # A tibble: 1 × 2\n#&gt;   hidden_units .config             \n#&gt;          &lt;int&gt; &lt;chr&gt;               \n#&gt; 1            7 Preprocessor1_Model6\n\nThe autoplot() method can visualize the relationship between the tuning parameter(s) and the performance metric(s).\n\nautoplot(nnet_res, metric = \"mae\")\n\n\n\n\n\n\n\ntune::collect_predictions() will automatically return the out-of-sample predictions for every candidate model (e.g., every tuning parameter value.). We might not want them all; it has an argument called parameters that can be used to filter the results.\nprobably::cal_plot_regression() automatically shows the results for each tuning parameter combination. For example:\n\ncal_plot_regression(nnet_res, alpha = 1 / 3)\n\n\n\n\n\n\n\nThere are two options if we need a model fit on the training set. If the numerically best parameter is best (i.e., smallest MAE), then tune::fit_best() is the easiest approach. Alternatively, you can choose the exact tuning parameter values you desire and splice them into the model to replace the current values of tune(). To do this, there is a finalize_workflow() function. It takes a data frame with one row and columns for each tuning parameter. Here’s an example where we decide that 10 hidden units are best:\n\nset.seed(814)\nnnet_fit &lt;- \n  nnet_wflow %&gt;% \n  finalize_workflow(tibble(hidden_units = 10)) %&gt;% \n  fit(data = delivery_train)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#aside-parallel-processing",
    "href": "chapters/whole-game.html#aside-parallel-processing",
    "title": "2  The Whole Game",
    "section": "\n2.6 Aside: Parallel Processing",
    "text": "2.6 Aside: Parallel Processing\nFor model tuning, we are fitting many models. With grid search, these models are not dependent on one another. For this reason, it is possible to compute these model fits simultaneously (i.e., in parallel).\nTo do so, tidymodels requires you to specify a parallel backend. There are several types, and we will use PSOCK clusters since they work on all operating systems. For this technology, we can run the following commands before running fit_resamples() or any of the tune_*() functions:\n\ncores &lt;- parallel::detectCores(logical = FALSE)\nlibrary(doParallel)\ncl &lt;- makePSOCKcluster(cores)\nregisterDoParallel(cl)\n\nAfter we are finished, we also close down the cluster:\n\nparallel::stopCluster(cl)\n\nThere can be significant speed-ups when running in parallel. See the section in the tidymodels book for more details.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#calibration",
    "href": "chapters/whole-game.html#calibration",
    "title": "2  The Whole Game",
    "section": "\n2.7 Calibration",
    "text": "2.7 Calibration\nThe functions to calibrate predictions are in the probably package and have names that start with cal_*. There are methods that work on the results from fit_resamples() or the tune_*() functions, but you can also just use a data frame of predicted values.\nWe must estimate the trend with the validation set. If we use our object lin_reg_res, it knows what data to use:\n\nlin_reg_cal &lt;- cal_estimate_linear(lin_reg_res)\nlin_reg_cal\n#&gt; \n#&gt; ── Regression Calibration\n#&gt; Method: Generalized additive model\n#&gt; Source class: Tune Results\n#&gt; Data points: 2,004\n#&gt; Truth variable: `time_to_delivery`\n#&gt; Estimate variable: `.pred`\n\nAs you’ll see in a minute, the function probably::cal_apply() calibrates new predictions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#test-set-results",
    "href": "chapters/whole-game.html#test-set-results",
    "title": "2  The Whole Game",
    "section": "\n2.8 Test Set Results",
    "text": "2.8 Test Set Results\nAs with best_fit(), there are two ways to predict the test set.\nThe more manual approach is to fit the model on the training set, use predict() or augment() to compute the test set predictions, calibrate them with our object, then use our metric to compute performance. If we had not already fit the model, the pre-calibration code is:\n\nlin_reg_fit &lt;- fit(lin_reg_wflow, delivery_train)\nlin_reg_test_pred &lt;- augment(lin_reg_fit, delivery_test)\n\nlin_reg_test_pred %&gt;% \n  reg_metrics(time_to_delivery, .pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       1.607\n\n# plot the uncalibrated results: \nlin_reg_test_pred %&gt;% \n  cal_plot_regression(truth = time_to_delivery, estimate = .pred, alpha = 1 / 3)\n\n\n\n\n\n\n\nThere is a shortcut for the first three commands. tune::last_fit() takes our initial split object and automatically does the rest (but not calibration yet):\n\nlin_reg_test_res &lt;- \n  lin_reg_wflow %&gt;% \n  last_fit(delivery_split, metrics = reg_metrics)\n\nWe can pull out the elements we need from this object using some extract_*() and collect_*() functions. Here are a few:\n\n# Test set metrics:\ncollect_metrics(lin_reg_test_res)\n#&gt; # A tibble: 1 × 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard       1.607 Preprocessor1_Model1\n\n# Test set predictions: \ncollect_predictions(lin_reg_test_res)\n#&gt; # A tibble: 2,004 × 5\n#&gt;   id               .pred  .row time_to_delivery .config             \n#&gt;   &lt;chr&gt;            &lt;dbl&gt; &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 train/test split 15.98     7            18.02 Preprocessor1_Model1\n#&gt; 2 train/test split 16.03    14            17.57 Preprocessor1_Model1\n#&gt; 3 train/test split 27.60    16            26.71 Preprocessor1_Model1\n#&gt; 4 train/test split 17.15    29            17.64 Preprocessor1_Model1\n#&gt; 5 train/test split 32.24    33            32.19 Preprocessor1_Model1\n#&gt; 6 train/test split 20.18    34            20.31 Preprocessor1_Model1\n#&gt; # ℹ 1,998 more rows\n\n# Final model fit: \nlin_reg_fit &lt;- extract_fit_parsnip(lin_reg_test_res)\n\n# cal_plot_regression(lin_reg_test_res)\n\nNow let’s calibrate and compute performance:\n\n# apply calibration\nlin_reg_test_pred_cal &lt;- \n  lin_reg_test_pred %&gt;% \n  cal_apply(lin_reg_cal)\n\nlin_reg_test_pred_cal %&gt;% \n  reg_metrics(time_to_delivery, .pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       1.545\n\n# plot the calibrated results: \nlin_reg_test_pred_cal %&gt;% \n  cal_plot_regression(truth = time_to_delivery, estimate = .pred, alpha = 1 / 3)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#conclusion",
    "href": "chapters/whole-game.html#conclusion",
    "title": "2  The Whole Game",
    "section": "\n2.9 Conclusion",
    "text": "2.9 Conclusion\nThis has been an abbreviated, high-level introduction to using tidymodels. Future chapters will go into much more detail on these subjects and illustrate additional features and functions as needed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#requirements",
    "href": "chapters/initial-data-splitting.html#requirements",
    "title": "3  Initial Data Splitting",
    "section": "\n3.1 Requirements",
    "text": "3.1 Requirements\nYou’ll need 2 packages (caret and tidymodels) for this chapter. You can install them via:\n\nreq_pkg &lt;- c(\"caret\", \"tidymodels\")\n\n# Check to see if they are installed: \nif (!rlang::is_installed(req_pkg)) {\n  pak::pak(req_pkg)\n}\n\nLet’s load the meta package and manage some between-package function conflicts.\n\nlibrary(tidymodels)\ntidymodels_prefer()\n\nThe data used here are both in R packages that are already installed. Let’s work with the primary data set: the Ames Iowa housing data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-ames-intro",
    "href": "chapters/initial-data-splitting.html#sec-ames-intro",
    "title": "3  Initial Data Splitting",
    "section": "\n3.2 The Ames Housing Data",
    "text": "3.2 The Ames Housing Data\nThese data are in the modeldata package, which is part of tidymodels. Let’s load the data, subset a few columns, and modify the sale price units. We’ll also combine the two bathroom-related columns into a single column.\n\ndata(ames, package = \"modeldata\")\n\names &lt;-\n  ames %&gt;%\n  select(Sale_Price, Bldg_Type, Neighborhood, Year_Built, Gr_Liv_Area, Full_Bath,\n         Half_Bath, Year_Sold, Lot_Area, Central_Air, Longitude, Latitude) %&gt;%\n  mutate(\n    Sale_Price = log10(Sale_Price),\n    Baths = Full_Bath  + Half_Bath/2\n  ) %&gt;%\n  select(-Half_Bath, -Full_Bath)\n\nglimpse(ames)\n#&gt; Rows: 2,930\n#&gt; Columns: 11\n#&gt; $ Sale_Price   &lt;dbl&gt; 5.332, 5.021, 5.236, 5.387, 5.279, 5.291, 5.329, 5.282, 5.374…\n#&gt; $ Bldg_Type    &lt;fct&gt; OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, TwnhsE, Twnhs…\n#&gt; $ Neighborhood &lt;fct&gt; North_Ames, North_Ames, North_Ames, North_Ames, Gilbert, Gilb…\n#&gt; $ Year_Built   &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 1995, 1999, 1…\n#&gt; $ Gr_Liv_Area  &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616, 1804, 16…\n#&gt; $ Year_Sold    &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2…\n#&gt; $ Lot_Area     &lt;int&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005, 5389, 75…\n#&gt; $ Central_Air  &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y…\n#&gt; $ Longitude    &lt;dbl&gt; -93.62, -93.62, -93.62, -93.62, -93.64, -93.64, -93.63, -93.6…\n#&gt; $ Latitude     &lt;dbl&gt; 42.05, 42.05, 42.05, 42.05, 42.06, 42.06, 42.06, 42.06, 42.06…\n#&gt; $ Baths        &lt;dbl&gt; 1.0, 1.0, 1.5, 2.5, 2.5, 2.5, 2.0, 2.0, 2.0, 2.5, 2.5, 2.0, 2…\n\ntidymodels requires that, for outcome data, any basic transformations should occur before data splitting.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-basic-splitting",
    "href": "chapters/initial-data-splitting.html#sec-basic-splitting",
    "title": "3  Initial Data Splitting",
    "section": "\n3.3 Simple Data Splitting",
    "text": "3.3 Simple Data Splitting\nThere are a few main functions for an initial split:\n\n\nrsample::initial_split(): completely random splits and stratified splits.\n\nrsample::initial_time_split(): non-random splits for times series; the most recent data are used for testing.\n\nrsample::initial_validation_split() and rsample::initial_validation_time_split(): an initial split into three partitions.\n\nrsample::group_initial_split(): for situations with repeated measures or other important grouping factors.\n\nMost of our applications will use the first function, where the default is to use 75% for training and 25% for testing. This is determined at random; there is no need to randomly sort the rows before splitting. By default, a simple random split is used.\nFor the Ames data, we know that the distribution of sale prices has some outlying points. To deal with this, we’ll use a stratified split (on the outcome) using 5 quantiles of the data in ames:\n\nset.seed(3024)\names_split &lt;- initial_split(ames, strata = Sale_Price, breaks = 5)\n\names_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;2196/734/2930&gt;\n\nThe output shows the size of the resulting data sets. To get the two data sets, there are simple accessor functions:\n\names_train &lt;- training(ames_split)\names_test  &lt;- testing(ames_split)\n\nConsistent with the printed output, there are 2,196 data points in the training set and 734 reserved for testing.\nWe won’t touch on initial_time_split() here but only mention that it takes the fraction of the data specified for testing from the bottom/tail of the data frame. Unlike the previous function, the order of the rows matters.\ngroup_initial_split() and initial_validation_split() are discussed in more detail below.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-split-with-predictors",
    "href": "chapters/initial-data-splitting.html#sec-split-with-predictors",
    "title": "3  Initial Data Splitting",
    "section": "\n3.4 Using the Predictors",
    "text": "3.4 Using the Predictors\nInstead of using the outcome to partition the data, other columns can be used when applicable. The text mentions using the twinning package (CRAN page). The same authors have a second approach that can be found in the SPlit package (CRAN). Both are straightforward to use.\nMaximum dissimilarity sampling can be conducted using caret::maxDissim(). It starts with an initial set of one or more or fewer data points to use as a starter. Unless there is a specific set of points of interest, picking one close to the center of the multivariate predictor distribution might make sense. Here is some code that uses the geographic coordinates as the splitting variables:\n\n# Since we will be using distances in the calculations, create centered \n# and scaled versions of the coordinates then add a row index column. \names_scaled &lt;-\n  ames %&gt;%\n  select(Longitude, Latitude) %&gt;%\n  mutate(\n    scaled_lon = scale(Longitude)[,1], \n    scaled_lat = scale(Latitude)[,1]\n  ) %&gt;%\n  select(starts_with(\"scaled\")) %&gt;% \n  add_rowindex()\n\n# Select an initial data point closest to the middle\nseed_row &lt;-\n  ames_scaled %&gt;%\n  mutate(\n    dist = (scaled_lon)^2 + (scaled_lat)^2\n  ) %&gt;%\n  slice_min(dist, n = 1) %&gt;%\n  pluck(\".row\")\n\n# Partition the data\names_scaled_seed &lt;- ames_scaled %&gt;% slice( seed_row)\names_scaled_pool &lt;- ames_scaled %&gt;% slice(-seed_row)\n\n# Conduct the selection process\nselection_path &lt;- \n  caret::maxDissim(\n    # Only give the function the predictor columns for each data set\n    ames_scaled_seed %&gt;% select(-.row), \n    ames_scaled_pool %&gt;% select(-.row), \n    n = 24\n  )\n\n# Get the selected row numbers that correspond to the 'ames' data frame.\nselected_rows &lt;- c(seed_row, ames_scaled_pool$.row[selection_path])\n\nselected_data &lt;- ames %&gt;% slice(selected_rows)\n\n# A non-map plot of the values: \nselected_data %&gt;%\n  mutate(xend = lead(Longitude), yend = lead(Latitude)) %&gt;%\n  ggplot(aes(Longitude, Latitude)) +\n  geom_point() +\n  geom_segment(aes(xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.1, \"inches\"), type = \"closed\"),\n               col = \"blue\", alpha = 1 / 5) +\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-multilevel-splitting",
    "href": "chapters/initial-data-splitting.html#sec-multilevel-splitting",
    "title": "3  Initial Data Splitting",
    "section": "\n3.5 Multi-Level Data",
    "text": "3.5 Multi-Level Data\nThis section will focus on data with a rational grouping of data. For example, medical data might follow patient over time so that there are multiple rows per patient. The patient is the independent experimental unit (IEU), meaning that the data between patients are thought to be independent, and those within a patient are (statistically) related. We want to partition the data so that all of the data for each IEU end up in either the training or test sets but not both. We want to sample the data by the group – where the group in this example is the patient.\nThere are other applications of grouped data but the example data that we’ll use fits into the description above: 27 patients were followed and had data collected at four time points. The data are in the nlme package:\n\ndata(Orthodont, package = \"nlme\")\nglimpse(Orthodont)\n#&gt; Rows: 108\n#&gt; Columns: 4\n#&gt; $ distance &lt;dbl&gt; 26.0, 25.0, 29.0, 31.0, 21.5, 22.5, 23.0, 26.5, 23.0, 22.5, 24.0,…\n#&gt; $ age      &lt;dbl&gt; 8, 10, 12, 14, 8, 10, 12, 14, 8, 10, 12, 14, 8, 10, 12, 14, 8, 10…\n#&gt; $ Subject  &lt;ord&gt; M01, M01, M01, M01, M02, M02, M02, M02, M03, M03, M03, M03, M04, …\n#&gt; $ Sex      &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Male, Male,…\n\nTo use rsample::group_initial_split(), we must supply a group argument that corresponds to one of the columns in the data. There is also a prop argument that specifies the proportion of the groups that should go into the training set.\n\nset.seed(93)\north_split &lt;- group_initial_split(Orthodont, group = Subject, prop = 2 / 3)\n\n# The numbers in this output are individual rows (not numbers of groups)\north_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;72/36/108&gt;\n\nFrom here, the code to get the resulting data sets is the same as previously shown. We’ll also verify that no subjects are in both data sets:\n\north_train &lt;- training(orth_split)\north_test  &lt;- testing(orth_split)\n\n# Is there any overlap in the subjects? \nsubjects_train &lt;- unique(orth_train$Subject)\nsubjects_test  &lt;- unique(orth_test$Subject)\n\nintersect(subjects_train, subjects_test)\n#&gt; character(0)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-three-way-split",
    "href": "chapters/initial-data-splitting.html#sec-three-way-split",
    "title": "3  Initial Data Splitting",
    "section": "\n3.6 Validation Sets",
    "text": "3.6 Validation Sets\nTo add a validation set at the outset, initial_validation_split() works the same as initial_split(). The prop argument requires two values now: the first is the training set proportion, and the second is for the validation set. In this example below, we add 80% to training, 10% to validation, and the remaining 10% to the testing set:\n\nset.seed(4)\names_val_split &lt;- initial_validation_split(ames, strata = Sale_Price, prop = c(0.8, 0.1))\n\names_val_split\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;2342/293/295/2930&gt;\n\nAgain, the acquisition of data is the same but has the additional use of the validation() function:\n\names_train &lt;- training(ames_val_split)\names_val   &lt;- validation(ames_val_split)\names_test  &lt;- testing(ames_val_split)\n\nrsample::initial_validation_time_split() does the same thing but based on the ordering of the data (as opposed to random selection).\nSuppose a data frame had 100 rows. Using prop = c(0.8, 0.1) would place the first 80 rows into training, the next 10 into validation, and the last 10 into testing. Keeping the data appropriately ordered is important when using validation sets in tidymodels.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  }
]