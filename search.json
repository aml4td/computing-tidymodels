[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidymodels Computing Supplement",
    "section": "",
    "text": "Preface\nThis is a computing supplement to the main website that uses the tidymodels framework for modeling. The structure is similar to the website, but the content here shows how to use this software (and sometimes others) for each topic.\nWe also want these materials to be reusable and open. The sources are in the source GitHub repository with a Creative Commons license attached (see below).\nTo cite this work, we suggest:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Tidymodels Computing Supplement",
    "section": "License",
    "text": "License\n\nThis work is licensed under CC BY-SA 4.0",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Tidymodels Computing Supplement",
    "section": "Intended Audience",
    "text": "Intended Audience\nReaders should have used R before but do not have to be experts. If you are new to R, we suggest taking a look at R for Data Science.\nYou do not have to be a modeling expert either. We hope that you have used a linear or logistic regression before and understand basic statistical concepts such as correlation, variability, probabilities, etc.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-can-i-ask-questions",
    "href": "index.html#how-can-i-ask-questions",
    "title": "Tidymodels Computing Supplement",
    "section": "How can I ask questions?",
    "text": "How can I ask questions?\nIf you have questions about the content, it is probably best to ask on a public forum, like cross-validated or Posit Community. You’ll most likely get a faster answer there if you take the time to ask the questions in the best way possible.\nIf you want a direct answer from us, you should follow what I call Yihui’s Rule: add an issue to GitHub (labeled as “Discussion”) first. It may take some time for us to get back to you.\nIf you think there is a bug, please file an issue.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#can-i-contribute",
    "href": "index.html#can-i-contribute",
    "title": "Tidymodels Computing Supplement",
    "section": "Can I contribute?",
    "text": "Can I contribute?\nThere is a contributing page with details on how to get up and running to compile the materials (there are a lot of software dependencies) and suggestions on how to help.\nIf you just want to fix a typo, you can make a pull request to alter the appropriate .qmd file.\nPlease feel free to improve the quality of this content by submitting pull requests. A merged PR will make you appear in the contributor list.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#computing-notes",
    "href": "index.html#computing-notes",
    "title": "Tidymodels Computing Supplement",
    "section": "Computing Notes",
    "text": "Computing Notes\nQuarto was used to compile and render the materials\n\n\nQuarto 1.5.22\n[✓] Checking versions of quarto binary dependencies...\n      Pandoc version 3.1.11: OK\n      Dart Sass version 1.69.5: OK\n      Deno version 1.37.2: OK\n[✓] Checking versions of quarto dependencies......OK\n[✓] Checking Quarto installation......OK\n      Version: 1.5.22\n[✓] Checking tools....................OK\n      TinyTeX: (external install)\n      Chromium: (not installed)\n[✓] Checking LaTeX....................OK\n      Using: TinyTex\n      Version: 2022\n[✓] Checking basic markdown render....OK\n[✓] Checking Python 3 installation....OK\n      Version: 3.9.6\n      Jupyter: (None)\n      Jupyter is not available in this Python installation.\n[✓] Checking R installation...........OK\n      Version: 4.3.2\n      LibPaths:\n      knitr: 1.47\n      rmarkdown: 2.27\n[✓] Checking Knitr engine render......OK\n\n\nR version 4.3.2 (2023-10-31) was used for the majority of the computations. torch 2.0.1 was also used. The versions of the primary R modeling and visualization packages used here are:\n  Warning: package 'aorsf' was built under R version 4.3.3\n  Warning: package 'lme4' was built under R version 4.3.3\n  Warning: package 'ragg' was built under R version 4.3.3\n  Warning: package 'rmarkdown' was built under R version 4.3.3\n  Warning: package 'splines2' was built under R version 4.3.3\n  Warning: package 'torch' was built under R version 4.3.3\n\n\n\n\n\n\n\n\n\naorsf (0.1.5)\n\nbestNormalize (1.9.1)\n\nbrulee (0.3.0)\n\n\n\ncaret (6.0-94)\n\nCubist (0.4.2.1)\n\ndials (1.2.1)\n\n\n\ndownlit (0.4.4.9000)\n\ndplyr (1.1.4)\n\ne1071 (1.7-14)\n\n\n\nggplot2 (3.5.1)\n\ngt (0.10.1)\n\nhardhat (1.4.0)\n\n\n\nhstats (1.1.2)\n\nlme4 (1.1-35.4)\n\nMatrix (1.6-5)\n\n\n\nmixOmics (6.25.1)\n\nmodeldata (1.4.0)\n\nmodeldatatoo (0.3.0)\n\n\n\nparsnip (1.2.1)\n\npatchwork (1.2.0)\n\nprobably (1.0.3)\n\n\n\npurrr (1.0.2)\n\nragg (1.3.2)\n\nrecipes (1.0.10)\n\n\n\nrmarkdown (2.27)\n\nrsample (1.2.1)\n\nrstudioapi (0.16.0)\n\n\n\nrules (1.0.2)\n\nsplines2 (0.5.2)\n\nstopwords (2.3)\n\n\n\ntextrecipes (1.0.6)\n\ntidymodels (1.2.0)\n\ntidyr (1.3.1)\n\n\n\ntorch (0.13.0)\n\ntune (1.2.1)\n\nusethis (2.2.3)\n\n\n\nworkflows (1.1.4)\n\nworkflowsets (1.1.0)\n\nxml2 (1.3.6)\n\n\n\nyardstick (1.3.1)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/news.html",
    "href": "chapters/news.html",
    "title": "News",
    "section": "",
    "text": "Errata\nNo reported edits (yet).",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#changelog",
    "href": "chapters/news.html#changelog",
    "title": "News",
    "section": "Changelog",
    "text": "Changelog\n\n2024-07-01\nAdded chapters on embeddings, interactions, splines, and overfitting\n\n\n2023-12-11\nAdded a chapter on initial data splitting.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/contributing.html",
    "href": "chapters/contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "Software\nRegarding R packages, the repository has a DESCRIPTION file as if it were an R package. This lets us specify precisely what packages and their versions should be installed. The packages listed in the imports field contain packages for modeling/analysis and packages used to make the website/book. Some basic system requirements are likely needed to install packages: Fortran, gdal, and others.\nThe main requirements are as follows.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing.html#software",
    "href": "chapters/contributing.html#software",
    "title": "Contributing",
    "section": "",
    "text": "Quarto\nQuarto is an open-source scientific and technical publishing system. Quarto version 1.5.22 is used to compile the website.\nR and renv\nR version 4.3.2 (2023-10-31) is what we are currently using. We suggest using rig to manage R versions. There are several IDEs that you can use. We’ve used RStudio (&gt;= 2023.6.1.524).\nThe current strategy is to use the renv (&gt;= version 1.0.5) package to make this project more isolated, portable and reproducible.\nTo get package dependencies installed…\nWhen you open the computing-tidymodels.Rproj file, the renv package should be automatically installed/updated (if needed). For example:\n# Bootstrapping renv 1.0.3 ---------------------------------------------------\n- Downloading renv ... OK\n- Installing renv  ... OK\n\nThe following package(s) will be installed:\n- BiocManager [1.30.22]\nThese packages will be installed into \"~/content/aml4td/renv/library/R-4.3/x86_64-apple-darwin20\".\nif you try to compile the book, you probably get and error:\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nYou can get more information using renv::status() but you can get them installed by first running renv::activate(). As an example:\n&gt; renv::activate()\n\nRestarting R session...\n\n- Project '~/content/aml4td' loaded. [renv 1.0.3]\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nSince we have package versions recorded in the lockfile, we can installed them using renv::restore(). Here is an example of that output:\n&gt; renv::restore() \nThe following package(s) will be updated:\n\n# Bioconductor ---------------------------------------------------------------\n- mixOmics         [* -&gt; mixOmicsTeam/mixOmics]\n\n# CRAN -----------------------------------------------------------------------\n- BiocManager      [1.30.22 -&gt; 1.30.21.1]\n- lattice          [0.21-9 -&gt; 0.21-8]\n- Matrix           [1.6-1.1 -&gt; 1.6-0]\n- nlme             [3.1-163 -&gt; 3.1-162]\n- rpart            [4.1.21 -&gt; 4.1.19]\n- survival         [3.5-7 -&gt; 3.5-5]\n- abind            [* -&gt; 1.4-5]\n\n&lt;snip&gt;\n\n- zip              [* -&gt; 2.2.0]\n- zoo              [* -&gt; 1.8-12]\n\n# GitHub ---------------------------------------------------------------------\n- BiocParallel     [* -&gt; Bioconductor/BiocParallel@devel]\n- BiocVersion      [* -&gt; Bioconductor/BiocVersion@devel]\n- modeldatatoo     [* -&gt; tidymodels/modeldatatoo@HEAD]\n- parsnip          [* -&gt; tidymodels/parsnip@HEAD]\n- usethis          [* -&gt; r-lib/usethis@HEAD]\n\n# RSPM -----------------------------------------------------------------------\n- bslib            [* -&gt; 0.5.1]\n- fansi            [* -&gt; 1.0.5]\n- fontawesome      [* -&gt; 0.5.2]\n- ggplot2          [* -&gt; 3.4.4]\n- htmltools        [* -&gt; 0.5.6.1]\n- withr            [* -&gt; 2.5.1]\n\nDo you want to proceed? [Y/n]: y\n\n# Downloading packages -------------------------------------------------------\n- Downloading BiocManager from CRAN ...         OK [569 Kb in 0.19s]\n- Downloading nlme from CRAN ...                OK [828.7 Kb in 0.19s]\n- Downloading BH from CRAN ...                  OK [12.7 Mb in 0.4s]\n- Downloading BiocVersion from GitHub ...       OK [826 bytes in 0.37s]\n\n&lt;snip&gt;\n\nDepending on whether you have to install packages from source, you may need to install some system dependencies and try again (I had to install libgit2 the last time I did this).\nOnce you have everything installed, we recommend installing the underlying torch computational libraries. You can do this by loading the torch package A download will automatically begin if you need one.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing.html#contributor-list",
    "href": "chapters/contributing.html#contributor-list",
    "title": "Contributing",
    "section": "Contributor List",
    "text": "Contributor List\nThe would like to thank users who have made a contribution to the project:",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Installation\ntidymodels is built in R so you’ll need to install that. We used R version 4.3.2 (2023-10-31) for these notes. To install R, you can go to CRAN1 to download it for your operating system. If you are comfortable at the command line, the rig application is an excellent way to install and manage R versions.\nYou probably want to use an integrated development environment (IDE); it will make your life much better. We use the RStudio IDE, which can be downloaded here. Other applications are Visual Studio and emacs.\nTo use tidymodels, you need to install multiple packages. The core packages are bundled into a “verse” package called tidymodels. When you install that, you get the primary packages as well as some tidyverse packages such as dplyr and ggplot2.\nTo install it, you can use\nWe suggest using the pak package for installation. To do this, first install that and then use it for further installations:\ninstall.packages(\"pak\")\n\n# check that it is installed then use it to install tidymodels\nif (require(pak)) {\n  pak::pak(\"tidymodels\")\n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#installation",
    "href": "chapters/introduction.html#installation",
    "title": "1  Introduction",
    "section": "",
    "text": "install.packages(\"tidymodels\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#loading-tidymodels",
    "href": "chapters/introduction.html#loading-tidymodels",
    "title": "1  Introduction",
    "section": "\n1.2 Loading tidymodels",
    "text": "1.2 Loading tidymodels\nOnce you do that, load tidymodels:\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────────────── tidymodels 1.2.0 ──\n#&gt; ✔ broom        1.0.5      ✔ recipes      1.0.10\n#&gt; ✔ dials        1.2.1      ✔ rsample      1.2.1 \n#&gt; ✔ dplyr        1.1.4      ✔ tibble       3.2.1 \n#&gt; ✔ ggplot2      3.5.1      ✔ tidyr        1.3.1 \n#&gt; ✔ infer        1.0.7      ✔ tune         1.2.1 \n#&gt; ✔ modeldata    1.4.0      ✔ workflows    1.1.4 \n#&gt; ✔ parsnip      1.2.1      ✔ workflowsets 1.1.0 \n#&gt; ✔ purrr        1.0.2      ✔ yardstick    1.3.1\n#&gt; ── Conflicts ────────────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\n#&gt; • Use suppressPackageStartupMessages() to eliminate package startup messages\n\nThe default output shows the packages that are automatically attached. There are a lot of functions in tidy models, but by loading this meta-package, you don’t have to remember which functions come from which packages.\nNote the lines at the bottom that messages like :\n\ndplyr::filter() masks stats::filter()\n\nThis means that two packages, dplyr and stats, have functions with the same name (filter())2. If you were to type filter at an R prompt, the function that you get corresponds to the one in the most recently loaded package. That’s not ideal.\nTo handle this, we have a function called tidymodels_prefer(). When you use this, it prioritizes functions from the tidy models and tidyverse groups so that you get those 3 If you want to see the specific conflicts and how we resolve them, see this output:\n\ntidymodels_prefer(quiet = FALSE)\n#&gt; [conflicted] Will prefer agua::refit over any other package.\n#&gt; [conflicted] Will prefer dials::Laplace over any other package.\n#&gt; [conflicted] Will prefer dials::max_rules over any other package.\n#&gt; [conflicted] Will prefer dials::neighbors over any other package.\n#&gt; [conflicted] Will prefer dials::prune over any other package.\n#&gt; [conflicted] Will prefer dials::smoothness over any other package.\n#&gt; [conflicted] Will prefer dplyr::collapse over any other package.\n#&gt; [conflicted] Will prefer dplyr::combine over any other package.\n#&gt; [conflicted] Will prefer dplyr::filter over any other package.\n#&gt; [conflicted] Will prefer dplyr::rename over any other package.\n#&gt; [conflicted] Will prefer dplyr::select over any other package.\n#&gt; [conflicted] Will prefer dplyr::slice over any other package.\n#&gt; [conflicted] Will prefer ggplot2::`%+%` over any other package.\n#&gt; [conflicted] Will prefer ggplot2::margin over any other package.\n#&gt; [conflicted] Will prefer parsnip::bart over any other package.\n#&gt; [conflicted] Will prefer parsnip::fit over any other package.\n#&gt; [conflicted] Will prefer parsnip::mars over any other package.\n#&gt; [conflicted] Will prefer parsnip::pls over any other package.\n#&gt; [conflicted] Will prefer purrr::cross over any other package.\n#&gt; [conflicted] Will prefer purrr::invoke over any other package.\n#&gt; [conflicted] Will prefer purrr::map over any other package.\n#&gt; [conflicted] Will prefer recipes::discretize over any other package.\n#&gt; [conflicted] Will prefer recipes::step over any other package.\n#&gt; [conflicted] Will prefer rsample::populate over any other package.\n#&gt; [conflicted] Will prefer scales::rescale over any other package.\n#&gt; [conflicted] Will prefer themis::step_downsample over any other package.\n#&gt; [conflicted] Will prefer themis::step_upsample over any other package.\n#&gt; [conflicted] Will prefer tidyr::expand over any other package.\n#&gt; [conflicted] Will prefer tidyr::extract over any other package.\n#&gt; [conflicted] Will prefer tidyr::pack over any other package.\n#&gt; [conflicted] Will prefer tidyr::unpack over any other package.\n#&gt; [conflicted] Will prefer tune::parameters over any other package.\n#&gt; [conflicted] Will prefer tune::tune over any other package.\n#&gt; [conflicted] Will prefer yardstick::get_weights over any other package.\n#&gt; [conflicted] Will prefer yardstick::precision over any other package.\n#&gt; [conflicted] Will prefer yardstick::recall over any other package.\n#&gt; [conflicted] Will prefer yardstick::spec over any other package.\n#&gt; [conflicted] Will prefer recipes::update over Matrix::update.\n#&gt; ── Conflicts ───────────────────────────────────────────────── tidymodels_prefer() ──\n\nIf you want to know more about why tidymodels exists, we’ve written a bit about this in the tidymodels book. The second chapter describes how tidyverse principles can be used for modeling.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#package-versions-and-reproducability",
    "href": "chapters/introduction.html#package-versions-and-reproducability",
    "title": "1  Introduction",
    "section": "\n1.3 Package Versions and Reproducability",
    "text": "1.3 Package Versions and Reproducability\nWe will do our best to use versions of our packages corresponding to the CRAN versions. We can’t always do that, and, for many packages, a version number ending with a value in the 9000 range (e.g., version “1.1.4.9001”) means that it was a development version of the package and was most likely installed from a GitHub repository.\nAt the end of each session, we’ll show which packages were loaded and used:\n\nsessioninfo::session_info()\n#&gt; ─ Session info ────────────────────────────────────────────────────────────────────\n#&gt;  setting  value\n#&gt;  version  R version 4.3.2 (2023-10-31)\n#&gt;  os       macOS Sonoma 14.5\n#&gt;  system   aarch64, darwin20\n#&gt;  ui       X11\n#&gt;  language (EN)\n#&gt;  collate  en_US.UTF-8\n#&gt;  ctype    en_US.UTF-8\n#&gt;  tz       America/New_York\n#&gt;  date     2024-07-01\n#&gt;  pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#&gt; \n#&gt; ─ Packages ────────────────────────────────────────────────────────────────────────\n#&gt;  ! package      * version    date (UTC) lib source\n#&gt;  P backports      1.4.1      2021-12-13 [?] CRAN (R 4.3.0)\n#&gt;  P BiocManager    1.30.23    2024-05-04 [?] CRAN (R 4.3.1)\n#&gt;  P broom        * 1.0.5      2023-06-09 [?] CRAN (R 4.3.0)\n#&gt;  P cachem         1.0.8      2023-05-01 [?] CRAN (R 4.3.0)\n#&gt;  P class          7.3-22     2023-05-03 [?] CRAN (R 4.3.2)\n#&gt;  P cli            3.6.3      2024-06-21 [?] CRAN (R 4.3.3)\n#&gt;  P codetools      0.2-19     2023-02-01 [?] CRAN (R 4.3.2)\n#&gt;  P colorspace     2.1-0      2023-01-23 [?] CRAN (R 4.3.0)\n#&gt;  P conflicted     1.2.0      2023-02-01 [?] CRAN (R 4.3.0)\n#&gt;  P data.table     1.14.8     2023-02-17 [?] CRAN (R 4.3.0)\n#&gt;  P dials        * 1.2.1      2024-02-22 [?] CRAN (R 4.3.1)\n#&gt;  P DiceDesign     1.9        2021-02-13 [?] CRAN (R 4.3.0)\n#&gt;  P digest         0.6.33     2023-07-07 [?] CRAN (R 4.3.0)\n#&gt;  P dplyr        * 1.1.4      2023-11-17 [?] CRAN (R 4.3.1)\n#&gt;  P evaluate       0.23       2023-11-01 [?] CRAN (R 4.3.1)\n#&gt;  P fansi          1.0.5      2023-10-08 [?] RSPM (R 4.3.0)\n#&gt;  P fastmap        1.1.1      2023-02-24 [?] CRAN (R 4.3.0)\n#&gt;  P foreach        1.5.2      2022-02-02 [?] CRAN (R 4.3.0)\n#&gt;  P furrr          0.3.1      2022-08-15 [?] CRAN (R 4.3.0)\n#&gt;  P future         1.33.0     2023-07-01 [?] CRAN (R 4.3.0)\n#&gt;  P future.apply   1.11.0     2023-05-21 [?] CRAN (R 4.3.0)\n#&gt;  P generics       0.1.3      2022-07-05 [?] CRAN (R 4.3.0)\n#&gt;  P ggplot2      * 3.5.1      2024-04-23 [?] CRAN (R 4.3.1)\n#&gt;  P globals        0.16.2     2022-11-21 [?] CRAN (R 4.3.0)\n#&gt;  P glue           1.6.2      2022-02-24 [?] CRAN (R 4.3.0)\n#&gt;  P gower          1.0.1      2022-12-22 [?] CRAN (R 4.3.0)\n#&gt;  P GPfit          1.0-8      2019-02-08 [?] CRAN (R 4.3.0)\n#&gt;  P gtable         0.3.4      2023-08-21 [?] RSPM (R 4.3.0)\n#&gt;  P hardhat        1.4.0      2024-06-02 [?] CRAN (R 4.3.2)\n#&gt;  P htmltools      0.5.7      2023-11-03 [?] CRAN (R 4.3.1)\n#&gt;  P htmlwidgets    1.6.2      2023-03-17 [?] CRAN (R 4.3.0)\n#&gt;  P infer        * 1.0.7      2024-03-25 [?] CRAN (R 4.3.1)\n#&gt;  P ipred          0.9-14     2023-03-09 [?] CRAN (R 4.3.0)\n#&gt;  P iterators      1.0.14     2022-02-05 [?] CRAN (R 4.3.0)\n#&gt;  P jsonlite       1.8.8      2023-12-04 [?] CRAN (R 4.3.1)\n#&gt;  P knitr          1.47       2024-05-29 [?] CRAN (R 4.3.3)\n#&gt;  P lattice        0.21-9     2023-10-01 [?] CRAN (R 4.3.2)\n#&gt;  P lava           1.7.3      2023-11-04 [?] CRAN (R 4.3.1)\n#&gt;  P lhs            1.1.6      2022-12-17 [?] CRAN (R 4.3.0)\n#&gt;  P lifecycle      1.0.4      2023-11-07 [?] CRAN (R 4.3.1)\n#&gt;  P listenv        0.9.0      2022-12-16 [?] CRAN (R 4.3.0)\n#&gt;  P lubridate      1.9.3      2023-09-27 [?] CRAN (R 4.3.1)\n#&gt;  P magrittr       2.0.3      2022-03-30 [?] CRAN (R 4.3.0)\n#&gt;  P MASS           7.3-60     2023-05-04 [?] CRAN (R 4.3.2)\n#&gt;  P Matrix         1.6-5      2024-01-11 [?] CRAN (R 4.3.1)\n#&gt;  P memoise        2.0.1      2021-11-26 [?] CRAN (R 4.3.0)\n#&gt;  P modeldata    * 1.4.0      2024-06-19 [?] CRAN (R 4.3.2)\n#&gt;  P munsell        0.5.0      2018-06-12 [?] CRAN (R 4.3.0)\n#&gt;  P nnet           7.3-19     2023-05-03 [?] CRAN (R 4.3.2)\n#&gt;  P parallelly     1.36.0     2023-05-26 [?] CRAN (R 4.3.0)\n#&gt;  P parsnip      * 1.2.1      2024-03-22 [?] CRAN (R 4.3.1)\n#&gt;  P pillar         1.9.0      2023-03-22 [?] CRAN (R 4.3.0)\n#&gt;  P pkgconfig      2.0.3      2019-09-22 [?] CRAN (R 4.3.0)\n#&gt;  P prodlim        2023.08.28 2023-08-28 [?] CRAN (R 4.3.0)\n#&gt;  P purrr        * 1.0.2      2023-08-10 [?] CRAN (R 4.3.0)\n#&gt;  P R6             2.5.1      2021-08-19 [?] CRAN (R 4.3.0)\n#&gt;  P Rcpp           1.0.11     2023-07-06 [?] CRAN (R 4.3.0)\n#&gt;  P recipes      * 1.0.10     2024-02-18 [?] CRAN (R 4.3.1)\n#&gt;  P renv           1.0.5      2024-02-29 [?] CRAN (R 4.3.2)\n#&gt;  P rlang          1.1.4      2024-06-04 [?] CRAN (R 4.3.3)\n#&gt;  P rmarkdown      2.27       2024-05-17 [?] CRAN (R 4.3.3)\n#&gt;  P rpart          4.1.21     2023-10-09 [?] CRAN (R 4.3.2)\n#&gt;  P rsample      * 1.2.1      2024-03-25 [?] CRAN (R 4.3.1)\n#&gt;  P rstudioapi     0.16.0     2024-03-24 [?] CRAN (R 4.3.1)\n#&gt;  P scales       * 1.3.0      2023-11-28 [?] CRAN (R 4.3.1)\n#&gt;  P sessioninfo    1.2.2      2021-12-06 [?] CRAN (R 4.3.0)\n#&gt;  P survival       3.5-7      2023-08-14 [?] CRAN (R 4.3.2)\n#&gt;  P tibble       * 3.2.1      2023-03-20 [?] CRAN (R 4.3.0)\n#&gt;  P tidymodels   * 1.2.0      2024-03-25 [?] CRAN (R 4.3.1)\n#&gt;  P tidyr        * 1.3.1      2024-01-24 [?] CRAN (R 4.3.1)\n#&gt;  P tidyselect     1.2.0      2022-10-10 [?] CRAN (R 4.3.0)\n#&gt;  P timechange     0.2.0      2023-01-11 [?] CRAN (R 4.3.0)\n#&gt;  P timeDate       4022.108   2023-01-07 [?] CRAN (R 4.3.0)\n#&gt;  P tune         * 1.2.1      2024-04-18 [?] CRAN (R 4.3.1)\n#&gt;  P utf8           1.2.4      2023-10-22 [?] CRAN (R 4.3.1)\n#&gt;  P vctrs          0.6.4      2023-10-12 [?] RSPM (R 4.3.0)\n#&gt;  P withr          2.5.2      2023-10-30 [?] CRAN (R 4.3.1)\n#&gt;  P workflows    * 1.1.4      2024-02-19 [?] CRAN (R 4.3.1)\n#&gt;  P workflowsets * 1.1.0      2024-03-21 [?] CRAN (R 4.3.1)\n#&gt;  P xfun           0.45       2024-06-16 [?] CRAN (R 4.3.3)\n#&gt;  P yaml           2.3.8      2023-12-11 [?] CRAN (R 4.3.1)\n#&gt;  P yardstick    * 1.3.1      2024-03-21 [?] CRAN (R 4.3.1)\n#&gt; \n#&gt;  [1] /Users/max/github/computing-tidymodels/renv/library/R-4.3/aarch64-apple-darwin20\n#&gt;  [2] /Users/max/Library/Caches/org.R-project.R/R/renv/sandbox/R-4.3/aarch64-apple-darwin20/ac5c2659\n#&gt; \n#&gt;  P ── Loaded and on-disk path mismatch.\n#&gt; \n#&gt; ───────────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#footnotes",
    "href": "chapters/introduction.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "The Comprehensive R Archive Network↩︎\nThe syntax foo::bar() means that the function bar() is inside of the package foo When used together, this is often referred to as “calling the function by its namespace.”. You can do this in your code, and developers often do. However, it’s fairly ugly. ↩︎\nUnfortunately, this is not a guarantee but it does work most of the time.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html",
    "href": "chapters/whole-game.html",
    "title": "2  The Whole Game",
    "section": "",
    "text": "2.1 Requirements\nYou’ll need 6 packages (brulee, Cubist, patchwork, scales, splines2, and tidymodels) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"brulee\", \"Cubist\", \"patchwork\", \"scales\", \"splines2\", \"tidymodels\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nOnce you’ve installed brulee, you should load it using library(brulee) to install the underlying torch executables. You only have to do this once.\nTwo other packages are described but not directly used: parallel and doParallel.\nLet’s run some code to get started:\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────────────── tidymodels 1.2.0 ──\n#&gt; ✔ broom        1.0.5      ✔ recipes      1.0.10\n#&gt; ✔ dials        1.2.1      ✔ rsample      1.2.1 \n#&gt; ✔ dplyr        1.1.4      ✔ tibble       3.2.1 \n#&gt; ✔ ggplot2      3.5.1      ✔ tidyr        1.3.1 \n#&gt; ✔ infer        1.0.7      ✔ tune         1.2.1 \n#&gt; ✔ modeldata    1.4.0      ✔ workflows    1.1.4 \n#&gt; ✔ parsnip      1.2.1      ✔ workflowsets 1.1.0 \n#&gt; ✔ purrr        1.0.2      ✔ yardstick    1.3.1\n#&gt; ── Conflicts ────────────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\n#&gt; • Use suppressPackageStartupMessages() to eliminate package startup messages\nlibrary(probably)\n#&gt; \n#&gt; Attaching package: 'probably'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.factor, as.ordered\nlibrary(patchwork)\n\ntidymodels_prefer()\nFinally, this note:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#requirements",
    "href": "chapters/whole-game.html#requirements",
    "title": "2  The Whole Game",
    "section": "",
    "text": "Note\n\n\n\nAll of these notes will assume that you have an R session that is running from the root of the directory containing the GitHub repository files. In other words, if you were to execute list.dirs(recursive = FALSE), the output would show entries such as \"./chapters\", \"./RData\", etc.\nIf you are not in the right place, use setwd() to change the working directory to the correct location.\nIf you start by opening the Rproj file, you will always start in the right place.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#the-data",
    "href": "chapters/whole-game.html#the-data",
    "title": "2  The Whole Game",
    "section": "\n2.2 The Data",
    "text": "2.2 The Data\nThe data set is pre-compiled into a binary format R uses (called “RData format” here). It is in the RData directory. A csv version is also in the delimited directory. Let’s load it:\n\nload(\"RData/deliveries.RData\")\n\nThere are a lot of ways that you can examine the contents of an object. View() is good for data frames; in the RStudio IDE, it opens a spreadsheet-like viewer. tibble::glimpse() shows more details about the object, such as classes, but can be a bad choice if you have &gt;50 columns in the data (or if it is a long list or similar). We’ll use that:\n\nglimpse(deliveries)\n#&gt; Rows: 10,012\n#&gt; Columns: 31\n#&gt; $ time_to_delivery &lt;dbl&gt; 16.11, 22.95, 30.29, 33.43, 27.23, 19.65, 22.10, 26.63, 3…\n#&gt; $ hour             &lt;dbl&gt; 11.90, 19.23, 18.37, 15.84, 19.62, 12.95, 15.48, 17.05, 1…\n#&gt; $ day              &lt;fct&gt; Thu, Tue, Fri, Thu, Fri, Sat, Sun, Thu, Fri, Sun, Tue, Fr…\n#&gt; $ distance         &lt;dbl&gt; 3.15, 3.69, 2.06, 5.97, 2.52, 3.35, 2.46, 2.21, 2.62, 2.7…\n#&gt; $ item_01          &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n#&gt; $ item_02          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, …\n#&gt; $ item_03          &lt;int&gt; 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n#&gt; $ item_04          &lt;int&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_05          &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_06          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, …\n#&gt; $ item_07          &lt;int&gt; 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, …\n#&gt; $ item_08          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, …\n#&gt; $ item_09          &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, …\n#&gt; $ item_10          &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_11          &lt;int&gt; 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_12          &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_13          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_14          &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_15          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_16          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_17          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_18          &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, …\n#&gt; $ item_19          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_20          &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_21          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_22          &lt;int&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, …\n#&gt; $ item_23          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_24          &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_25          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, …\n#&gt; $ item_26          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ item_27          &lt;int&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, …\n\nWe can see that this is a data frame and, more specifically a specialized version called a tibble. There are 10,012 data points and 31 columns and their types.\nNote that the day column is a factor. This is the preferred way to represent most categorical data (for modeling, at least). A factor catalogs the possible values of the data and stores those levels. That is important when we convert categorical predictors to “dummy variables” or “indicators” and similar operations.\nIn some cases, storing categorical data as integers might seem like a good idea (especially 0/1 for binary data). Do your best to avoid that. R (and tidymodels) would instead you use a data type that is designed explicitly for categories (a factor); it knows what to do with factors. If an integer is used, R can’t distinguish this from a column of counts (such as the number of times that item_01 was included in the order).\nTo create the histograms of the delivery times, we used this code to create each:\n\n# Setup some fancy code for the axes: \nlog_2_breaks &lt;- scales::trans_breaks(\"log2\", function(x) 2^x)\nlog_2_labs   &lt;- scales::trans_format(\"log2\", scales::math_format(2^.x))\n\ndelivery_hist &lt;- \n  deliveries %&gt;% \n  ggplot(aes(x = time_to_delivery)) +\n  geom_histogram(bins = 30, col = \"white\") +\n  geom_rug(alpha = 1 / 4) +\n  labs(x = \"Time Until Delivery (min)\", title = \"(a)\")\n\ndelivery_log_hist &lt;- \n  deliveries %&gt;% \n  ggplot(aes(x = time_to_delivery)) +\n  geom_histogram(bins = 30, col = \"white\") +\n  geom_rug(alpha = 1 / 4) +\n  labs(x = \"Time Until Delivery (min)\", title = \"(b)\") +\n  scale_x_log10(breaks = log_2_breaks, labels = log_2_labs)\n\nYou don’t need to assign the plots to objects; you can just print each. We did this so that we can concatenate the two plots with the patchwork package1:\n\ndelivery_hist + delivery_log_hist\n\n\n\n\n\n\n\nIn the code above, we use an option called \"alpha\". This is jargon for transparency; a value of 1/4 means that the points in the rug are 25% opaque.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#data-spending",
    "href": "chapters/whole-game.html#data-spending",
    "title": "2  The Whole Game",
    "section": "\n2.3 Data Spending",
    "text": "2.3 Data Spending\ntidymodels has a variety of ways to split the data at the outset of your modeling project. We will create a three-way split of the data using a function called initial_validation_split().\nIt uses random numbers so we will set the random number seed before using it.\n\n\n\n\n\n\nWhat’s a random number seed?\n\n\n\nWe are using random numbers (actually pseudo-random numbers). We want to get the same “random” values every time we run the same code for reproducibility. To do that, we use the set.seed() function and give it an integer value. The value itself doesn’t matter.\nThe random number stream is like a river. If you want to see the same things in your journey down the river, you must get in at the same exact spot. The seed is like the location where you start a journey (that is always the same).\n\n\nThe code is below.\n\nThe prop argument shows the fraction of the original data that should go into the training set (60%) and the validation set (20%). The remaining 20% are put in the test set.\nThe strata argument specifies that the splitting should consider the outcome column (time_to_delivery). This will be discussed in a future section. In short, the three-way splitting is done in different regions of the outcome data in a way that makes the distribution of the outcome as similar as possible across the three partitions.\n\nWe used a value of 991 to set the seed2:\n\nset.seed(991)\ndelivery_split &lt;-\n  initial_validation_split(deliveries, prop = c(0.6, 0.2), strata = time_to_delivery)\n\n# What is in it? \ndelivery_split\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;6004/2004/2004/10012&gt;\n\nThis object records which rows of the original data go into the training, validation, or test sets. The printed output shows the totals for each as &lt;train/val/test/total&gt;.\nTo get the data frames with the correct rows, use these three eponymous functions:\n\ndelivery_train &lt;- training(delivery_split)\ndelivery_test  &lt;- testing(delivery_split)\ndelivery_val   &lt;- validation(delivery_split)\n\nWe will mostly work with the training set of 6,004 deliveries. We’ll use that to explore the data, fit models, and so on.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#exploratory-data-analysis",
    "href": "chapters/whole-game.html#exploratory-data-analysis",
    "title": "2  The Whole Game",
    "section": "\n2.4 Exploratory Data Analysis",
    "text": "2.4 Exploratory Data Analysis\nWe mostly used ggplot2 and patchwork to create these graphics:\n\n# Make specific colors for each day\nday_cols &lt;-  c(\"#000000FF\", \"#24FF24FF\", \"#009292FF\",  \"#B66DFFFF\", \n               \"#6DB6FFFF\", \"#920000FF\", \"#FFB6DBFF\")\n\ndelivery_dist &lt;- \n  delivery_train %&gt;% \n  ggplot(aes(x = distance, time_to_delivery)) +\n  geom_point(alpha = 1 / 10, cex = 1) +\n  labs(y = \"Time Until Delivery (min)\", x = \"Distance (miles)\", title = \"(a)\") +\n  # This function creates the smooth trend line. The `se` option shuts off the\n  # confidence band around the line; too much information to put into one plot. \n  geom_smooth(se = FALSE, col = \"red\")\n\ndelivery_day &lt;- \n  delivery_train %&gt;% \n  ggplot(aes(x = day, time_to_delivery, col = day)) +\n  geom_boxplot(show.legend = FALSE)  +\n  labs(y = \"Time Until Delivery (min)\", x = NULL, title = \"(c)\") +\n  scale_color_manual(values = day_cols)\n\ndelivery_time &lt;- \n  delivery_train %&gt;% \n  ggplot(aes(x = hour, time_to_delivery)) +\n  labs(y = \"Time Until Delivery (min)\", x = \"Order Time (decimal hours)\", title = \"(b)\") +\n  geom_point(alpha = 1 / 10, cex = 1) + \n  geom_smooth(se = FALSE, col = \"red\")\n\ndelivery_time_day &lt;- \n  delivery_train %&gt;% \n  ggplot(aes(x = hour, time_to_delivery, col = day)) +\n  labs(y = \"Time Until Delivery (min)\", x = \"Order Time (decimal hours)\", title = \"(d)\") +\n  # With `col = day`, the trends will be estimated separately for each value of 'day'.\n  geom_smooth(se = FALSE) + \n  scale_color_manual(values = day_cols)\n\npatchwork puts it together.\n\n# Row 1\n( delivery_dist + delivery_time ) / \n  # Row 2\n  ( delivery_day + delivery_time_day ) +\n  # Consolidate the legends\n  plot_layout(guides = 'collect')  & \n  # Place the legend at the bottom\n  theme(legend.title = element_blank(), legend.position = \"bottom\")\n#&gt; `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n#&gt; `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n#&gt; `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\nggplot2 is a bit noisy. The messages tell you details about how it made the smooth trend line. The code s(x, bs = \"cs\") defines a spline smoother that we will see more of shortly (using a different function).\nThe methods that we used to compute the effects of the item_* columns are more complicated. We must make probabilistic assumptions about the data if we want to get something like a confidence interval. Alternatively, we could specify the empirical distribution function via the bootstrap resampling method. This helps us estimate the standard error of some statistic and use that to compute an interval.\nFirst, we make a function that takes some data and computes our statistics of interest. It assumes x is the entire data set with the delivery time column and each item column.\n\ntime_ratios &lt;- function(x) {\n  x %&gt;%\n    # The items are in columns; we'll stack these columns on one another.\n    pivot_longer(\n      cols = c(starts_with(\"item\")),\n      names_to = \"predictor\",\n      values_to = \"count\"\n    ) %&gt;%\n    # Collapse the counts into a \"in order\"/\"not in order\" variable. \n    mutate(ordered = ifelse(count &gt; 0, \"yes\", \"no\")) %&gt;%\n    # Compute, for each value of the 'predictor' and 'ordered' columns, \n    # the mean delivery time. \n    summarize(mean = mean(time_to_delivery),\n              .by = c(predictor, ordered)) %&gt;%\n    # Move the means to columns for when they were in the order \n    # and when they were not. The new column names are `yes` and `no`.\n    pivot_wider(id_cols = predictor,\n                names_from = ordered,\n                values_from = mean) %&gt;%\n    # Compute the ratio. This is a fold-difference in delivery times.\n    mutate(ratio = yes / no) %&gt;%\n    select(term = predictor, estimate = ratio)\n}\n\nWhen run in the training set:\n\ntime_ratios(delivery_train)\n#&gt; # A tibble: 27 × 2\n#&gt;   term    estimate\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 item_01    1.074\n#&gt; 2 item_02    1.010\n#&gt; 3 item_03    1.010\n#&gt; 4 item_04    1.002\n#&gt; 5 item_05    1.005\n#&gt; 6 item_06    1.018\n#&gt; # ℹ 21 more rows\n\nA value of 1.07 means that there is a 7% increase in the delivery time when that item is in the order at least once.\nA tidymodels function called int_pctl() can take a collection of bootstrap samples of a data set, compute their statistics, and use the results to produce confidence intervals (we’ll use 90% intervals). To use it, we’ll resample the training set using the bootstraps() function and then use a mutate() to compute the fold differences.\nWe are using random numbers again, so let’s reset the seed3.\n\nset.seed(624)\nresampled_data &lt;- \n  delivery_train %&gt;% \n  select(time_to_delivery, starts_with(\"item\")) %&gt;% \n  # This takes a while to compute. The materials use 5000 bootstraps\n  # but a smaller number is used here for demonstration.\n  bootstraps(times = 1001) \n\nresampled_data\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 1,001 × 2\n#&gt;   splits              id           \n#&gt;   &lt;list&gt;              &lt;chr&gt;        \n#&gt; 1 &lt;split [6004/2227]&gt; Bootstrap0001\n#&gt; 2 &lt;split [6004/2197]&gt; Bootstrap0002\n#&gt; 3 &lt;split [6004/2156]&gt; Bootstrap0003\n#&gt; 4 &lt;split [6004/2210]&gt; Bootstrap0004\n#&gt; 5 &lt;split [6004/2208]&gt; Bootstrap0005\n#&gt; 6 &lt;split [6004/2227]&gt; Bootstrap0006\n#&gt; # ℹ 995 more rows\n\nThe splits column contains the information on each bootstrap sample. To get a specific bootstrap sample, we can use the analysis(split_object) function on each element of the splits column. purrr::map() takes each split, extracts the bootstrap sample, then computes all of the ratios4.\n\nresampled_ratios &lt;- \n  resampled_data %&gt;% \n  mutate(stats = map(splits, ~ time_ratios(analysis(.x))))\n\nresampled_ratios\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 1,001 × 3\n#&gt;   splits              id            stats            \n#&gt;   &lt;list&gt;              &lt;chr&gt;         &lt;list&gt;           \n#&gt; 1 &lt;split [6004/2227]&gt; Bootstrap0001 &lt;tibble [27 × 2]&gt;\n#&gt; 2 &lt;split [6004/2197]&gt; Bootstrap0002 &lt;tibble [27 × 2]&gt;\n#&gt; 3 &lt;split [6004/2156]&gt; Bootstrap0003 &lt;tibble [27 × 2]&gt;\n#&gt; 4 &lt;split [6004/2210]&gt; Bootstrap0004 &lt;tibble [27 × 2]&gt;\n#&gt; 5 &lt;split [6004/2208]&gt; Bootstrap0005 &lt;tibble [27 × 2]&gt;\n#&gt; 6 &lt;split [6004/2227]&gt; Bootstrap0006 &lt;tibble [27 × 2]&gt;\n#&gt; # ℹ 995 more rows\n\n# An example: \nresampled_ratios$stats[[1]]\n#&gt; # A tibble: 27 × 2\n#&gt;   term    estimate\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 item_01    1.070\n#&gt; 2 item_02    1.018\n#&gt; 3 item_03    1.008\n#&gt; 4 item_04    1.008\n#&gt; 5 item_05    1.017\n#&gt; 6 item_06    1.007\n#&gt; # ℹ 21 more rows\n\nrsample::int_pctl() can consume these results and produce an interval for each item column5.\n\nresampled_intervals &lt;- \n  resampled_ratios %&gt;% \n  int_pctl(stats, alpha = 0.1) \n\nresampled_intervals\n#&gt; # A tibble: 27 × 6\n#&gt;   term    .lower .estimate .upper .alpha .method   \n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n#&gt; 1 item_01 1.052      1.075  1.097    0.1 percentile\n#&gt; 2 item_02 0.9950     1.009  1.024    0.1 percentile\n#&gt; 3 item_03 0.9940     1.009  1.024    0.1 percentile\n#&gt; 4 item_04 0.9879     1.002  1.016    0.1 percentile\n#&gt; 5 item_05 0.9884     1.005  1.021    0.1 percentile\n#&gt; 6 item_06 1.002      1.018  1.033    0.1 percentile\n#&gt; # ℹ 21 more rows\n\nHere’s our plot:\n\nresampled_intervals %&gt;% \n  # Convert the folds to percentages and make the item values\n  # a little cleaner:\n  mutate(\n    term = gsub(\"_0\", \" \", term),\n    term = factor(gsub(\"_\", \" \", term)),\n    term = reorder(term, .estimate),\n    increase = .estimate - 1,\n  ) %&gt;% \n  ggplot(aes(increase, term)) + \n  geom_vline(xintercept = 0, col = \"red\", alpha = 1 / 3) +\n  geom_point() + \n  geom_errorbar(aes(xmin = .lower - 1, xmax = .upper - 1), width = 1 / 2) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(y = NULL, x = \"Increase in Delivery Time When Ordered\") +\n  theme(axis.text.y = element_text(hjust = 0))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#model-development",
    "href": "chapters/whole-game.html#model-development",
    "title": "2  The Whole Game",
    "section": "\n2.5 Model Development",
    "text": "2.5 Model Development\nThe analyses in this section define a model pipeline, fit it to the training set, and then measure performance using the validation set. We’ll review the three evaluated models and describe how those computations were done.\nBefore we get started, we need to specify how to measure model effectiveness. The materials use the mean absolute error (MAE). To specify this performance metric, you can use the yardstick::metric_set() function and give it the function names for specific metrics (like the yardstick::mae() function):\n\nreg_metrics &lt;- metric_set(mae)\n\nWe’ll show you how to use reg_metrics in a bit.\nLinear Regression\nThe linear regression model is fairly simple to define and fit. Before we get to that, we must introduce a major tidymodels component: the recipe.\nA recipe is a set of instructions defining a potential series of computations on the predictor variables to put them into a format the model (or data set) requires. For example, the day-of-the-week factor must be converted into a numeric format. We’ll use the standard “Dummy variable” approach to do that. Additionally, our exploratory data analysis discovered that:\n\nThere is a nonlinear relationship between the outcome and the time of the order.\nThis nonlinear relationship is different for different days. This is an interaction effect between a qualitative predictor (day) and a nonlinear function of another (hour).\nThere also appeared to be an additional nonlinear effect for the order distance.\n\nWe can initialize a recipe using a simple formula method:\n\nspline_rec &lt;- recipe(time_to_delivery ~ ., data = delivery_train)\n\nThere are a few things that this function call does:\n\nThe formula declares that the column time_to_delivery is the outcome (since it is on the left-hand side of the tilde). The dot on the right-hand side indicates that all of the columns in delivery_train, besides the outcome, should be treated as predictors.\nThe recipe collects information on each column’s type. For example, it understands that day is a factor and that the item_* columns are numeric.\n\nLet’s add to the recipe by converting day to indicator columns. We do this by adding a step to the recipe via:\n\nspline_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) \n\nThe first argument to step functions is the variables that should be affected by the function. We can use any dplyr selector such as everything() and/or the bare column names. Here, we want to change every factor column that was the role of “predictor”. For this purpose, recipes have an extended set of selector functions.\nOnce the recipe is processed, this step will record which columns were captured by all_factor_predictors(), retain their factor levels, then convert them to a set of 0/1 indicators for each predictor/level.\nUnlike base R’s formula method, the resulting columns are named rationally. By default, it uses the pattern {column name}_{level} for the new features. So, the column day will not exist after this step. It is replaced with columns such as day_Thursday and so on.\nThe next recipe step is probably unnecessary for this data set but automatically using it is not problematic. What happens if there is a factor level that occurs very infrequently? It is possible that this will only be observed in the validation or test set. step_dummy() will make a column for that factor level since it knows it exists but the training set will have all zeros for this column; it has zero variance. We can screen these out using step_zv() (‘zv’ = zero-variance):\n\nspline_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  step_zv(all_predictors()) \n\nNow, we can address the nonlinear effects. We’ll use a spline basis expansion (described later on the main page) that creates additional columns from some numeric predictor. We’ll use a natural spline function and create ten new columns for both hour and distance:\n\nspline_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(hour, distance, deg_free = 10)\n\nThe naming convention for these new features are hour_01 … hour_10 and so on. The original hour column is removed (same for the distance column).\nThis step allows the linear regression to have nonlinear relationships between predictors and the outcome.\nFinally, we can create interactions. In base R, an interaction between variables a and b is specified in the formula using a:b. We’ll use the same method here with step_interact(). The main difference is that the columns day and hour no longer exist at this point. To capture all of the interactions, we can use the : convention with selector functions. Using starts_wth(\"day_\") will capture the existing indicator columns and, similarly, starts_wth(\"hour_\") finds the appropriate spline terms. Our final recipe is then:\n\nspline_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(hour, distance, deg_free = 10) %&gt;% \n  step_interact(~ starts_with(\"hour_\"):starts_with(\"day_\"))\n\n\n\n\n\n\n\nLearn More About Recipes\n\n\n\nYou can learn more about recipes later and there is material in the tidymodels book as well as tidymodels.org.\n\nFeature Engineering with recipes\nDimensionality Reduction\nEncoding Categorical Data\nGet Started: Preprocess your data with recipes\nArticles with recipes\nA list of recipe steps on CRAN\n\n\n\nTo specify the linear regression model, we use one of the functions from the parsnip package called… linear_reg(). Since we are using ordinary least squares, this function defaults to stats::lm().\n\n# This creates a model specification: \nlm_spec &lt;- linear_reg()\nlm_spec\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\nThe engine mentioned here is the computational method to fit the model. R has many ways to do this and \"lm\" is the default engine.\nHow do we combine the recipe and the model specifications? The best approach is to make a pipeline-like object called a workflow:\n\nlin_reg_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_spec) %&gt;% \n  add_recipe(spline_rec)\n\nWe can use the fit() function to fit the workflow to the training set. This executes the recipe on the data set then passes the appropriate data to stats::lm():\n\nlin_reg_fit &lt;- fit(lin_reg_wflow, data = delivery_train)\n\nWe can print the results out but the results are kind of long:\n\nlin_reg_fit\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────────────\n#&gt; 4 Recipe Steps\n#&gt; \n#&gt; • step_dummy()\n#&gt; • step_zv()\n#&gt; • step_spline_natural()\n#&gt; • step_interact()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)            item_01            item_02            item_03  \n#&gt;           13.3434             1.2444             0.6461             0.7313  \n#&gt;           item_04            item_05            item_06            item_07  \n#&gt;            0.2820             0.5839             0.5250             0.5063  \n#&gt;           item_08            item_09            item_10            item_11  \n#&gt;            0.6376             0.7368             1.5341             0.5172  \n#&gt;           item_12            item_13            item_14            item_15  \n#&gt;            0.5945             0.5799             0.5080             0.6432  \n#&gt;           item_16            item_17            item_18            item_19  \n#&gt;            0.4309             0.6089             0.4028            -0.3568  \n#&gt;           item_20            item_21            item_22            item_23  \n#&gt;            0.5280             0.7743             0.5105             0.6876  \n#&gt;           item_24            item_25            item_26            item_27  \n#&gt;            0.8126             0.4768             0.5274             0.5773  \n#&gt;           day_Tue            day_Wed            day_Thu            day_Fri  \n#&gt;           -1.1200            -2.8421            -3.7059            -4.3130  \n#&gt;           day_Sat            day_Sun            hour_01            hour_02  \n#&gt;           -4.6659            -3.0919             1.8776             2.5588  \n#&gt;           hour_03            hour_04            hour_05            hour_06  \n#&gt;            2.5826             3.8464             2.5826             4.0220  \n#&gt;           hour_07            hour_08            hour_09            hour_10  \n#&gt;            4.0112             5.9056            -0.8782            11.1905  \n#&gt;       distance_01        distance_02        distance_03        distance_04  \n#&gt;           -0.0523             0.5777             0.6157             0.7524  \n#&gt;       distance_05        distance_06        distance_07        distance_08  \n#&gt;            1.6695             1.7837             2.9039             3.3200  \n#&gt;       distance_09        distance_10  hour_01_x_day_Tue  hour_01_x_day_Wed  \n#&gt;          -19.3835            75.2868             1.3304             1.2875  \n#&gt; hour_01_x_day_Thu  hour_01_x_day_Fri  hour_01_x_day_Sat  hour_01_x_day_Sun  \n#&gt;            2.4448             1.3560             2.4253             1.5058  \n#&gt; hour_02_x_day_Tue  hour_02_x_day_Wed  hour_02_x_day_Thu  hour_02_x_day_Fri  \n#&gt;            0.2268             3.1441             3.9958             4.8608  \n#&gt; hour_02_x_day_Sat  hour_02_x_day_Sun  hour_03_x_day_Tue  hour_03_x_day_Wed  \n#&gt;            4.8380             3.7002             2.9359             6.4783  \n#&gt; hour_03_x_day_Thu  hour_03_x_day_Fri  hour_03_x_day_Sat  hour_03_x_day_Sun  \n#&gt;            7.4590             9.9423            10.9673             5.7549  \n#&gt; hour_04_x_day_Tue  hour_04_x_day_Wed  hour_04_x_day_Thu  hour_04_x_day_Fri  \n#&gt;            1.6701             6.2609             9.7183            12.1586  \n#&gt; hour_04_x_day_Sat  hour_04_x_day_Sun  hour_05_x_day_Tue  hour_05_x_day_Wed  \n#&gt;           13.9715             7.9502             3.9981             9.4129  \n#&gt; hour_05_x_day_Thu  hour_05_x_day_Fri  hour_05_x_day_Sat  hour_05_x_day_Sun  \n#&gt;           12.1748            16.5402            17.5038             9.3518  \n#&gt; hour_06_x_day_Tue  hour_06_x_day_Wed  hour_06_x_day_Thu  hour_06_x_day_Fri  \n#&gt;            2.5202             8.2079            11.9678            15.7150  \n#&gt; hour_06_x_day_Sat  hour_06_x_day_Sun  hour_07_x_day_Tue  hour_07_x_day_Wed  \n#&gt; \n#&gt; ...\n#&gt; and 14 more lines.\n\nOne helpful function is tidy(). It is designed to return the object results rationally, helpfully. In our case, the tidy() method for an lm object gives us a nice data frame back with information on the fitted coefficients:\n\ntidy(lin_reg_fit)\n#&gt; # A tibble: 114 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)  13.34     1.585       8.420 4.664e-17\n#&gt; 2 item_01       1.244    0.1031     12.08  3.498e-33\n#&gt; 3 item_02       0.6461   0.06865     9.412 6.852e-21\n#&gt; 4 item_03       0.7313   0.06914    10.58  6.463e-26\n#&gt; 5 item_04       0.2820   0.06260     4.505 6.776e- 6\n#&gt; 6 item_05       0.5839   0.07871     7.418 1.360e-13\n#&gt; # ℹ 108 more rows\n\nUnlike the summary() method for lm objects, this object can immediately be used in plots or tables.\nAnother valuable supporting function is augment(). It can take a model object and data set and attach the prediction columns to the data frame. Essentially, this is an upgraded version of predict(). Let’s predict the validation set:\n\nlm_reg_val_pred &lt;- augment(lin_reg_fit, new_data = delivery_val)\nnames(lm_reg_val_pred)\n#&gt;  [1] \".pred\"            \".resid\"           \"time_to_delivery\" \"hour\"            \n#&gt;  [5] \"day\"              \"distance\"         \"item_01\"          \"item_02\"         \n#&gt;  [9] \"item_03\"          \"item_04\"          \"item_05\"          \"item_06\"         \n#&gt; [13] \"item_07\"          \"item_08\"          \"item_09\"          \"item_10\"         \n#&gt; [17] \"item_11\"          \"item_12\"          \"item_13\"          \"item_14\"         \n#&gt; [21] \"item_15\"          \"item_16\"          \"item_17\"          \"item_18\"         \n#&gt; [25] \"item_19\"          \"item_20\"          \"item_21\"          \"item_22\"         \n#&gt; [29] \"item_23\"          \"item_24\"          \"item_25\"          \"item_26\"         \n#&gt; [33] \"item_27\"\n\nWhat is our MAE? This is where we use our metric set reg_metrics. Note that there is a column in the results called .pred. For regression models, this is the predicted delivery time for each order in the validation set. We can use that and the original observed outcome column to estimate the MAE6:\n\nlm_reg_val_pred %&gt;% \n  reg_metrics(truth = time_to_delivery, estimate = .pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       1.609\n\nThe units are fractional minutes.\nAt this point, we can make diagnostic plots of our data and so on.\nLet’s take a minor distraction that will pay off a bit later. The main page mentions that we can treat the validation set as a single resample of the data. If we were to do that, our code wouldn’t have to change much when we get into more complex scenarios such as cross-validation or model tuning. To do this, we can convert the initial split object into a resampling set (a.k.a. an rset):\n\ndelivery_rs &lt;- validation_set(delivery_split)\n\nclass(delivery_rs)\n#&gt; [1] \"validation_set\" \"rset\"           \"tbl_df\"         \"tbl\"           \n#&gt; [5] \"data.frame\"\n\ndelivery_rs\n#&gt; # A tibble: 1 × 2\n#&gt;   splits              id        \n#&gt;   &lt;list&gt;              &lt;chr&gt;     \n#&gt; 1 &lt;split [6004/2004]&gt; validation\n\nThis packages the training and validation sets together in a way that it knows when to use each data set appropriately.\nSince we are treating this as if it were resampling, we can use the fit_resamples() function to do much of the manual work we just showed. We’ll add a control object to the mix to specify that we want to retain the validation set predictions (and our original workflow).\n\nctrl_rs &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\nlin_reg_res &lt;-\n  fit_resamples(lin_reg_wflow,\n                resamples = delivery_rs,\n                control = ctrl_rs,\n                metrics = reg_metrics)\n\nThe benefit here is that there are a lot of helper functions to simplify your code. For example, to get the validation set MAE and predictions:\n\ncollect_metrics(lin_reg_res)\n#&gt; # A tibble: 1 × 6\n#&gt;   .metric .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   1.609     1      NA Preprocessor1_Model1\n\ncollect_predictions(lin_reg_res)\n#&gt; # A tibble: 2,004 × 5\n#&gt;   .pred id          .row time_to_delivery .config             \n#&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 30.12 validation  6005            27.23 Preprocessor1_Model1\n#&gt; 2 23.02 validation  6006            22.10 Preprocessor1_Model1\n#&gt; 3 28.38 validation  6007            26.63 Preprocessor1_Model1\n#&gt; 4 31.04 validation  6008            30.84 Preprocessor1_Model1\n#&gt; 5 38.57 validation  6009            41.17 Preprocessor1_Model1\n#&gt; 6 27.04 validation  6010            27.04 Preprocessor1_Model1\n#&gt; # ℹ 1,998 more rows\n\nThe probably package also has a nice helper to check the calibration of the model via a plot:\n\ncal_plot_regression(lin_reg_res)\n\n\n\n\n\n\n\nRule-Based Ensemble\nTo fit the Cubist model, we need to load one of the tidymodels extension packages called rules. It has the tools to fit this model and will automatically (and silently) use the Cubist package when the model fit occurs.\nWe’ll create a model specification that has an enselmble size of 100:\n\nlibrary(rules)\n\n# A model specification: \ncb_spec &lt;- cubist_rules(committees = 100)\n\nOne advantage of rule-based models is that very little preprocessing is required (i.e., no dummy variables or spline terms). For that reason, we’ll use a simple R formula instead of a recipe:\n\ncb_wflow &lt;- \n  workflow() %&gt;% \n  add_model(cb_spec) %&gt;% \n  add_formula(time_to_delivery ~ .)\n\nLet’s go straight to fit_resamples():\n\ncb_res &lt;-\n  fit_resamples(\n    cb_wflow, \n    resamples = delivery_rs, \n    control = ctrl_rs, \n    metrics = reg_metrics\n  )\n\ncollect_metrics(cb_res)\n#&gt; # A tibble: 1 × 6\n#&gt;   .metric .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   1.416     1      NA Preprocessor1_Model1\n\nThe calibration plot:\n\ncal_plot_regression(cb_res)\n\n\n\n\n\n\n\nThis is pretty simple and demonstrates that, after an initial investment in learning tidymodels syntax, the process of fitting different models does not require huge changes to your scripts.\nTo get the model fit, we previously used fit(). With resampling objects (and the tuning objects that we are about to see), there is another helper function called fit_best() that will create the model from the entire training set using the resampling results7:\n\ncb_fit &lt;- fit_best(cb_res)\n\ncb_fit\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: cubist_rules()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────────────\n#&gt; time_to_delivery ~ .\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; cubist.default(x = x, y = y, committees = 100)\n#&gt; \n#&gt; Number of samples: 6004 \n#&gt; Number of predictors: 30 \n#&gt; \n#&gt; Number of committees: 100 \n#&gt; Number of rules per committee: 31, 22, 23, 26, 21, 24, 23, 23, 22, 23, 21, 22, 18, 24, 20, 22, 16, 26, 25, 26 ...\n\nThe tidy() method is also helpful here. It contains all of the rules and corresponding regression models. Let’s get these values for the second rule in the fourth ensemble:\n\nrule_details &lt;- tidy(cb_fit)\n\nrule_details %&gt;% \n  filter(committee == 4 & rule_num == 2) %&gt;% \n  pluck(\"rule\")\n#&gt; [1] \"( hour &lt;= 14.946 ) & ( day  %in% c( 'Mon','Tue','Wed','Thu','Sun' ) ) & ( distance &lt;= 4.52 )\"\n\nrule_details %&gt;% \n  filter(committee == 4 & rule_num == 2) %&gt;% \n  select(estimate) %&gt;% \n  pluck(1) %&gt;% \n  pluck(1)\n#&gt; # A tibble: 15 × 2\n#&gt;   term        estimate\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 (Intercept)   -11.24\n#&gt; 2 hour            2.05\n#&gt; 3 distance        0.78\n#&gt; 4 item_01        -0.5 \n#&gt; 5 item_02         0.3 \n#&gt; 6 item_05         0.8 \n#&gt; # ℹ 9 more rows\n\nNeural Network\nThe model function for this type of model is parsnip::mlp() (MLP is short for “multi-layer perceptron”). There are quite a few packages for neural networks in R. tidymodels has interfaces to several engines:\n\nshow_engines(\"mlp\")\n#&gt; # A tibble: 6 × 2\n#&gt;   engine mode          \n#&gt;   &lt;chr&gt;  &lt;chr&gt;         \n#&gt; 1 keras  classification\n#&gt; 2 keras  regression    \n#&gt; 3 nnet   classification\n#&gt; 4 nnet   regression    \n#&gt; 5 brulee classification\n#&gt; 6 brulee regression\n\nWe’ll use the brulee package. This uses the torch software to fit the model. We’ll only tune the number of hidden units (for now, see later chapters). To mark any parameter for tuning, we pass the tune() function to an argument:\n\nnnet_spec &lt;- \n  mlp(\n    hidden_units = tune(),\n    # Some specific argument values that we chose:\n    penalty = 0.01,\n    learn_rate = 0.1,\n    epochs = 5000\n  ) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"brulee\", stop_iter = 10, rate_schedule = \"cyclic\")\n\nA few notes on this:\n\nThe arguments to mlp() are called the main arguments since they are used by several engines.\nThe default engine uses the nnet package; set_engine() specifies that we want to use the brulee package instead.\nTwo arguments (stop_iter and rate_schedule) are specific to our engine. We set them here (and can also pass a tune() value to them).\nNeural networks can fit both classification and regression models. We must state what model type (called a “mode”) to create.\n\nThis model requires the conversion to dummy variables but does not require features to handle nonlinear trends and interactions. One additional preprocessing step that is required is to put the predictors in the same units (e.g., not miles or hours). There are a few ways to do this. We will center and scale the predictors using recipes::step_normalize():\n\nnorm_rec &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\nnnet_wflow &lt;- \n  workflow() %&gt;% \n  add_model(nnet_spec) %&gt;% \n  add_recipe(norm_rec)\n\nUnlike the previous models, we are tuning one of the hyper-parameters. Instead of fit_resamples(), we’ll use tune_grid() to search over a predefined set of values for the number of hidden units. We can set how many values we should try or directly declare the candidate values. We’ll do the latter and, for simplicity, use a smaller range of values.\nFinally, we’ll use another control function:\n\n# The main materials used 2:100\nnnet_grid &lt;- tibble(hidden_units = 2:10)\n\nctrl_grid &lt;- control_grid(save_pred = TRUE, save_workflow = TRUE)\n\n# The model initializes the parameters with random numbers so set the seed:\nset.seed(388)\nnnet_res &lt;-\n  tune_grid(nnet_wflow,\n            resamples = delivery_rs,\n            control = ctrl_grid,\n            grid = nnet_grid,\n            metrics = reg_metrics)\n\nThere are some additional helper functions for model tuning. For example, we can rank the models based on a metric:\n\nshow_best(nnet_res, metric = \"mae\")\n#&gt; # A tibble: 5 × 7\n#&gt;   hidden_units .metric .estimator  mean     n std_err .config             \n#&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1            9 mae     standard   1.499     1      NA Preprocessor1_Model8\n#&gt; 2            5 mae     standard   1.514     1      NA Preprocessor1_Model4\n#&gt; 3            6 mae     standard   1.518     1      NA Preprocessor1_Model5\n#&gt; 4           10 mae     standard   1.528     1      NA Preprocessor1_Model9\n#&gt; 5            7 mae     standard   1.532     1      NA Preprocessor1_Model6\n\nWe can also return the parameter with the numerically best results8:\n\nbest_hidden_units &lt;- select_best(nnet_res, metric = \"mae\")\nbest_hidden_units\n#&gt; # A tibble: 1 × 2\n#&gt;   hidden_units .config             \n#&gt;          &lt;int&gt; &lt;chr&gt;               \n#&gt; 1            9 Preprocessor1_Model8\n\nThe autoplot() method can visualize the relationship between the tuning parameter(s) and the performance metric(s).\n\nautoplot(nnet_res, metric = \"mae\")\n\n\n\n\n\n\n\ntune::collect_predictions() will automatically return the out-of-sample predictions for every candidate model (e.g., every tuning parameter value.). We might not want them all; it has an argument called parameters that can be used to filter the results.\nprobably::cal_plot_regression() automatically shows the results for each tuning parameter combination. For example:\n\ncal_plot_regression(nnet_res)\n\n\n\n\n\n\n\nThere are two options if we need a model fit on the training set. If the numerically best parameter is best (i.e., smallest MAE), then tune::fit_best() is the easiest approach. Alternatively, you can choose the exact tuning parameter values you desire and splice them into the model to replace the current values of tune(). To do this, there is a finalize_workflow() function. It takes a data frame with one row and columns for each tuning parameter. Here’s an example where we decide that 10 hidden units are best:\n\nset.seed(814)\nnnet_fit &lt;- \n  nnet_wflow %&gt;% \n  finalize_workflow(tibble(hidden_units = 10)) %&gt;% \n  fit(data = delivery_train)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#aside-parallel-processing",
    "href": "chapters/whole-game.html#aside-parallel-processing",
    "title": "2  The Whole Game",
    "section": "\n2.6 Aside: Parallel Processing",
    "text": "2.6 Aside: Parallel Processing\nFor model tuning, we are fitting many models. With grid search, these models are not dependent on one another. For this reason, it is possible to compute these model fits simultaneously (i.e., in parallel).\nTo do so, tidymodels requires you to specify a parallel backend. There are several types, and we will use PSOCK clusters since they work on all operating systems. For this technology, we can run the following commands before running fit_resamples() or any of the tune_*() functions:\n\ncores &lt;- parallel::detectCores(logical = FALSE)\nlibrary(doParallel)\ncl &lt;- makePSOCKcluster(cores)\nregisterDoParallel(cl)\n\nAfter we are finished, we also close down the cluster:\n\nparallel::stopCluster(cl)\n\nThere can be significant speed-ups when running in parallel. See the section in the tidymodels book for more details.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#calibration",
    "href": "chapters/whole-game.html#calibration",
    "title": "2  The Whole Game",
    "section": "\n2.7 Calibration",
    "text": "2.7 Calibration\nThe functions to calibrate predictions are in the probably package and have names that start with cal_*. There are methods that work on the results from fit_resamples() or the tune_*() functions, but you can also just use a data frame of predicted values.\nWe must estimate the trend with the validation set. If we use our object lin_reg_res, it knows what data to use:\n\nlin_reg_cal &lt;- cal_estimate_linear(lin_reg_res)\nlin_reg_cal\n#&gt; \n#&gt; ── Regression Calibration\n#&gt; Method: Generalized additive model\n#&gt; Source class: Tune Results\n#&gt; Data points: 2,004\n#&gt; Truth variable: `time_to_delivery`\n#&gt; Estimate variable: `.pred`\n\nAs you’ll see in a minute, the function probably::cal_apply() calibrates new predictions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#test-set-results",
    "href": "chapters/whole-game.html#test-set-results",
    "title": "2  The Whole Game",
    "section": "\n2.8 Test Set Results",
    "text": "2.8 Test Set Results\nAs with best_fit(), there are two ways to predict the test set.\nThe more manual approach is to fit the model on the training set, use predict() or augment() to compute the test set predictions, calibrate them with our object, then use our metric to compute performance. If we had not already fit the model, the pre-calibration code is:\n\nlin_reg_fit &lt;- fit(lin_reg_wflow, delivery_train)\nlin_reg_test_pred &lt;- augment(lin_reg_fit, delivery_test)\n\nlin_reg_test_pred %&gt;% \n  reg_metrics(time_to_delivery, .pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       1.607\n\n# plot the uncalibrated results: \nlin_reg_test_pred %&gt;% \n  cal_plot_regression(truth = time_to_delivery, estimate = .pred)\n\n\n\n\n\n\n\nThere is a shortcut for the first three commands. tune::last_fit() takes our initial split object and automatically does the rest (but not calibration yet):\n\nlin_reg_test_res &lt;- \n  lin_reg_wflow %&gt;% \n  last_fit(delivery_split, metrics = reg_metrics)\n\nWe can pull out the elements we need from this object using some extract_*() and collect_*() functions. Here are a few:\n\n# Test set metrics:\ncollect_metrics(lin_reg_test_res)\n#&gt; # A tibble: 1 × 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard       1.607 Preprocessor1_Model1\n\n# Test set predictions: \ncollect_predictions(lin_reg_test_res)\n#&gt; # A tibble: 2,004 × 5\n#&gt;   .pred id                .row time_to_delivery .config             \n#&gt;   &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt;            &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 15.98 train/test split     7            18.02 Preprocessor1_Model1\n#&gt; 2 16.03 train/test split    14            17.57 Preprocessor1_Model1\n#&gt; 3 27.60 train/test split    16            26.71 Preprocessor1_Model1\n#&gt; 4 17.15 train/test split    29            17.64 Preprocessor1_Model1\n#&gt; 5 32.24 train/test split    33            32.19 Preprocessor1_Model1\n#&gt; 6 20.18 train/test split    34            20.31 Preprocessor1_Model1\n#&gt; # ℹ 1,998 more rows\n\n# Final model fit: \nlin_reg_fit &lt;- extract_fit_parsnip(lin_reg_test_res)\n\n# cal_plot_regression(lin_reg_test_res)\n\nNow let’s calibrate and compute performance:\n\n# apply calibration\nlin_reg_test_pred_cal &lt;- \n  lin_reg_test_pred %&gt;% \n  cal_apply(lin_reg_cal)\n\nlin_reg_test_pred_cal %&gt;% \n  reg_metrics(time_to_delivery, .pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       1.545\n\n# plot the calibrated results: \nlin_reg_test_pred_cal %&gt;% \n  cal_plot_regression(truth = time_to_delivery, estimate = .pred)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#conclusion",
    "href": "chapters/whole-game.html#conclusion",
    "title": "2  The Whole Game",
    "section": "\n2.9 Conclusion",
    "text": "2.9 Conclusion\nThis has been an abbreviated, high-level introduction to using tidymodels. Future chapters will go into much more detail on these subjects and illustrate additional features and functions as needed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#footnotes",
    "href": "chapters/whole-game.html#footnotes",
    "title": "2  The Whole Game",
    "section": "",
    "text": "We use a different ggplot theme for the main materials. We’ll use the default theme here.↩︎\nIf you are wondering how we get the seed values, I use sample.int(1000, 1) to generate random seeds on the fly.↩︎\nWhy are we doing this again? Didn’t we already “jump in the river?” Yes. If we were executing all of the code here in the exact order (with no typos or commands in between), we would have reproducible pseudo-random numbers. That’s usually not how interactive data analysis goes, though. Therefore, we (re)set the seed each time we use randomness.↩︎\nThis map() call can be made much faster by using the furrr package. It has versions of the purrr::map() functions that can run in parallel.↩︎\nYou can see another example of bootstrap intervals at tidymodels.org.↩︎\nWe could have used the yardstick::mae() directly instead of stuffing that function in a metric set. Since we often want to collect more than one type of performance statistic, we’re showing how to use a metric set.↩︎\nThis is possible since we previously used the save_workflow = TRUE option in the control function.↩︎\nAs impressive as the torch ecosystem is, it is not as optimized for reproducibility. These results may vary from run to run due to the inability to fix some of the random numbers used and their use of different numerical tolerances across operating systems.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html",
    "href": "chapters/initial-data-splitting.html",
    "title": "3  Initial Data Splitting",
    "section": "",
    "text": "3.1 Requirements\nYou’ll need 2 packages (caret and tidymodels) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"caret\", \"tidymodels\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\ntidymodels_prefer()\nThe data used here are both in R packages that are already installed. Let’s work with the primary data set: the Ames Iowa housing data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-ames-intro",
    "href": "chapters/initial-data-splitting.html#sec-ames-intro",
    "title": "3  Initial Data Splitting",
    "section": "\n3.2 The Ames Housing Data",
    "text": "3.2 The Ames Housing Data\nThese data are in the modeldata package, which is part of tidymodels. Let’s load the data, subset a few columns, and modify the sale price units. We’ll also combine the two bathroom-related columns into a single column.\n\ndata(ames, package = \"modeldata\")\n\names &lt;-\n  ames %&gt;%\n  select(Sale_Price, Bldg_Type, Neighborhood, Year_Built, Gr_Liv_Area, Full_Bath,\n         Half_Bath, Year_Sold, Lot_Area, Central_Air, Longitude, Latitude) %&gt;%\n  mutate(\n    Sale_Price = log10(Sale_Price),\n    Baths = Full_Bath  + Half_Bath/2\n  ) %&gt;%\n  select(-Half_Bath, -Full_Bath)\n\nglimpse(ames)\n#&gt; Rows: 2,930\n#&gt; Columns: 11\n#&gt; $ Sale_Price   &lt;dbl&gt; 5.332, 5.021, 5.236, 5.387, 5.279, 5.291, 5.329, 5.282, 5.374…\n#&gt; $ Bldg_Type    &lt;fct&gt; OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, TwnhsE, Twnhs…\n#&gt; $ Neighborhood &lt;fct&gt; North_Ames, North_Ames, North_Ames, North_Ames, Gilbert, Gilb…\n#&gt; $ Year_Built   &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 1995, 1999, 1…\n#&gt; $ Gr_Liv_Area  &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616, 1804, 16…\n#&gt; $ Year_Sold    &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2…\n#&gt; $ Lot_Area     &lt;int&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005, 5389, 75…\n#&gt; $ Central_Air  &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y…\n#&gt; $ Longitude    &lt;dbl&gt; -93.62, -93.62, -93.62, -93.62, -93.64, -93.64, -93.63, -93.6…\n#&gt; $ Latitude     &lt;dbl&gt; 42.05, 42.05, 42.05, 42.05, 42.06, 42.06, 42.06, 42.06, 42.06…\n#&gt; $ Baths        &lt;dbl&gt; 1.0, 1.0, 1.5, 2.5, 2.5, 2.5, 2.0, 2.0, 2.0, 2.5, 2.5, 2.0, 2…\n\ntidymodels requires that, for outcome data, any basic transformations should occur before data splitting.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-basic-splitting",
    "href": "chapters/initial-data-splitting.html#sec-basic-splitting",
    "title": "3  Initial Data Splitting",
    "section": "\n3.3 Simple Data Splitting",
    "text": "3.3 Simple Data Splitting\nThere are a few main functions for an initial split:\n\n\nrsample::initial_split(): completely random splits and stratified splits.\n\nrsample::initial_time_split(): non-random splits for times series; the most recent data are used for testing.\n\nrsample::initial_validation_split() and rsample::initial_validation_time_split(): an initial split into three partitions.\n\nrsample::group_initial_split(): for situations with repeated measures or other important grouping factors.\n\nMost of our applications will use the first function, where the default is to use 75% for training and 25% for testing. This is determined at random; there is no need to randomly sort the rows before splitting. By default, a simple random split is used.\nFor the Ames data, we know that the distribution of sale prices has some outlying points. To deal with this, we’ll use a stratified split (on the outcome) using 5 quantiles of the data in ames:\n\nset.seed(3024)\names_split &lt;- initial_split(ames, strata = Sale_Price, breaks = 5)\n\names_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;2196/734/2930&gt;\n\nThe output shows the size of the resulting data sets. To get the two data sets, there are simple accessor functions:\n\names_train &lt;- training(ames_split)\names_test  &lt;- testing(ames_split)\n\nConsistent with the printed output, there are 2,196 data points in the training set and 734 reserved for testing.\nWe won’t touch on initial_time_split() here but only mention that it takes the fraction of the data specified for testing from the bottom/tail of the data frame. Unlike the previous function, the order of the rows matters.\ngroup_initial_split() and initial_validation_split() are discussed in more detail below.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-split-with-predictors",
    "href": "chapters/initial-data-splitting.html#sec-split-with-predictors",
    "title": "3  Initial Data Splitting",
    "section": "\n3.4 Using the Predictors",
    "text": "3.4 Using the Predictors\nInstead of using the outcome to partition the data, other columns can be used when applicable. The text mentions using the twinning package (CRAN page). The same authors have a second approach that can be found in the SPlit package (CRAN). Both are straightforward to use.\nMaximum dissimilarity sampling can be conducted using caret::maxDissim(). It starts with an initial set of one or more or fewer data points to use as a starter. Unless there is a specific set of points of interest, picking one close to the center of the multivariate predictor distribution might make sense. Here is some code that uses the geographic coordinates as the splitting variables:\n\n# Since we will be using distances in the calculations, create centered \n# and scaled versions of the coordinates then add a row index column. \names_scaled &lt;-\n  ames %&gt;%\n  select(Longitude, Latitude) %&gt;%\n  mutate(\n    scaled_lon = scale(Longitude)[,1], \n    scaled_lat = scale(Latitude)[,1]\n  ) %&gt;%\n  select(starts_with(\"scaled\")) %&gt;% \n  add_rowindex()\n\n# Select an initial data point closest to the middle\nseed_row &lt;-\n  ames_scaled %&gt;%\n  mutate(\n    dist = (scaled_lon)^2 + (scaled_lat)^2\n  ) %&gt;%\n  slice_min(dist, n = 1) %&gt;%\n  pluck(\".row\")\n\n# Partition the data\names_scaled_seed &lt;- ames_scaled %&gt;% slice( seed_row)\names_scaled_pool &lt;- ames_scaled %&gt;% slice(-seed_row)\n\n# Conduct the selection process\nselection_path &lt;- \n  caret::maxDissim(\n    # Only give the function the predictor columns for each data set\n    ames_scaled_seed %&gt;% select(-.row), \n    ames_scaled_pool %&gt;% select(-.row), \n    n = 24\n  )\n\n# Get the selected row numbers that correspond to the 'ames' data frame.\nselected_rows &lt;- c(seed_row, ames_scaled_pool$.row[selection_path])\n\nselected_data &lt;- ames %&gt;% slice(selected_rows)\n\n# A non-map plot of the values: \nselected_data %&gt;%\n  mutate(xend = lead(Longitude), yend = lead(Latitude)) %&gt;%\n  ggplot(aes(Longitude, Latitude)) +\n  geom_point() +\n  geom_segment(aes(xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.1, \"inches\"), type = \"closed\"),\n               col = \"blue\", alpha = 1 / 5) +\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-multilevel-splitting",
    "href": "chapters/initial-data-splitting.html#sec-multilevel-splitting",
    "title": "3  Initial Data Splitting",
    "section": "\n3.5 Multi-Level Data",
    "text": "3.5 Multi-Level Data\nThis section will focus on data with a rational grouping of data. For example, medical data might follow patient over time so that there are multiple rows per patient. The patient is the independent experimental unit (IEU), meaning that the data between patients are thought to be independent, and those within a patient are (statistically) related. We want to partition the data so that all of the data for each IEU end up in either the training or test sets but not both. We want to sample the data by the group – where the group in this example is the patient.\nThere are other applications of grouped data but the example data that we’ll use fits into the description above: 27 patients were followed and had data collected at four time points. The data are in the nlme package:\n\ndata(Orthodont, package = \"nlme\")\nglimpse(Orthodont)\n#&gt; Rows: 108\n#&gt; Columns: 4\n#&gt; $ distance &lt;dbl&gt; 26.0, 25.0, 29.0, 31.0, 21.5, 22.5, 23.0, 26.5, 23.0, 22.5, 24.0,…\n#&gt; $ age      &lt;dbl&gt; 8, 10, 12, 14, 8, 10, 12, 14, 8, 10, 12, 14, 8, 10, 12, 14, 8, 10…\n#&gt; $ Subject  &lt;ord&gt; M01, M01, M01, M01, M02, M02, M02, M02, M03, M03, M03, M03, M04, …\n#&gt; $ Sex      &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Male, Male,…\n\nTo use rsample::group_initial_split(), we must supply a group argument that corresponds to one of the columns in the data. There is also a prop argument that specifies the proportion of the groups that should go into the training set.\n\nset.seed(93)\north_split &lt;- group_initial_split(Orthodont, group = Subject, prop = 2 / 3)\n\n# The numbers in this output are individual rows (not numbers of groups)\north_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;72/36/108&gt;\n\nFrom here, the code to get the resulting data sets is the same as previously shown. We’ll also verify that no subjects are in both data sets:\n\north_train &lt;- training(orth_split)\north_test  &lt;- testing(orth_split)\n\n# Is there any overlap in the subjects? \nsubjects_train &lt;- unique(orth_train$Subject)\nsubjects_test  &lt;- unique(orth_test$Subject)\n\nintersect(subjects_train, subjects_test)\n#&gt; character(0)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-three-way-split",
    "href": "chapters/initial-data-splitting.html#sec-three-way-split",
    "title": "3  Initial Data Splitting",
    "section": "\n3.6 Validation Sets",
    "text": "3.6 Validation Sets\nTo add a validation set at the outset, initial_validation_split() works the same as initial_split(). The prop argument requires two values now: the first is the training set proportion, and the second is for the validation set. In this example below, we add 80% to training, 10% to validation, and the remaining 10% to the testing set:\n\nset.seed(4)\names_val_split &lt;- initial_validation_split(ames, strata = Sale_Price, prop = c(0.8, 0.1))\n\names_val_split\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;2342/293/295/2930&gt;\n\nAgain, the acquisition of data is the same but has the additional use of the validation() function:\n\names_train &lt;- training(ames_val_split)\names_val   &lt;- validation(ames_val_split)\names_test  &lt;- testing(ames_val_split)\n\nrsample::initial_validation_time_split() does the same thing but based on the ordering of the data (as opposed to random selection).\nSuppose a data frame had 100 rows. Using prop = c(0.8, 0.1) would place the first 80 rows into training, the next 10 into validation, and the last 10 into testing. Keeping the data appropriately ordered is important when using validation sets in tidymodels.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html",
    "href": "chapters/numeric-predictors.html",
    "title": "4  Transforming Numeric Predictors",
    "section": "",
    "text": "4.1 Requirements\nYou’ll need 4 packages (bestNormalize, embed, splines2, and tidymodels) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"bestNormalize\", \"embed\", \"tidymodels\", \"splines2\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-hotel-data-intro",
    "href": "chapters/numeric-predictors.html#sec-hotel-data-intro",
    "title": "4  Transforming Numeric Predictors",
    "section": "\n4.2 Data Sets",
    "text": "4.2 Data Sets\nThe data sets used here are both in R packages that have already been installed. Let’s work with the primary data set: the Ames Iowa housing data.\nIn the last chapter, our manipulation and splitting code was:\n\ndata(ames, package = \"modeldata\")\n\names &lt;-\n  ames %&gt;%\n  select(Sale_Price, Bldg_Type, Neighborhood, Year_Built, Gr_Liv_Area, Full_Bath,\n         Half_Bath, Year_Sold, Lot_Area, Central_Air, Longitude, Latitude) %&gt;%\n  mutate(\n    Sale_Price = log10(Sale_Price),\n    Baths = Full_Bath  + Half_Bath/2\n  ) %&gt;%\n  select(-Half_Bath, -Full_Bath)\n\nset.seed(3024)\names_split &lt;- initial_split(ames, strata = Sale_Price, breaks = 5)\names_train &lt;- training(ames_split)\names_test  &lt;- testing(ames_split)\n\nWe’ll work with ames_train almost exclusively here.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-r-formulas",
    "href": "chapters/numeric-predictors.html#sec-r-formulas",
    "title": "4  Transforming Numeric Predictors",
    "section": "\n4.3 Standard R Formulas",
    "text": "4.3 Standard R Formulas\nModel formulas in R are identical to those in S, which Chambers and Hastie introduced in Statistical Models in S (1991). A broader discussion can be found in two blog posts (one and two).\nThe formula has a few basic operators:\n\nThe tilde (~) separates the outcome columns from the predictor columns. Anything to the left is considered an outcome, and the right-hand side defines predictors (e.g., outcome ~ predictor\n\nA dot is a wildcard for any columns in the data set that are not outcomes (e.g., y ~ .).\nPlus signs signify the symbolic addition of columns to the formula (typically predictors). For example, y ~ x1 + x2 indicates one outcome and two predictor columns. To indicate arithmetic addition (or any other computations), you can wrap the items in the identity function I() such as y ~ I(x1 + x2).\nYou can use the minus sign to remove columns. This may not be implemented in some modeling functions.\nThe colon indicates interaction terms (described in a future chapter).\n\nThere is further syntax described below.\nHere’s an example of a basic formula that creates two predictor columns by specifying a symbolic formula comprised of two numeric predictors.\n\nf_01 &lt;- Sale_Price ~  Baths + Year_Built\n\nHere’s a short function to show basic results:\n\nshow_columns &lt;- function(f) {\n  model.matrix(f, data = ames_train) %&gt;% \n  tibble::as_tibble() %&gt;% \n  dplyr::slice(c(1, 3, 9))\n}\nshow_columns(f_01)\n#&gt; # A tibble: 3 × 3\n#&gt;   `(Intercept)` Baths Year_Built\n#&gt;           &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1             1   1         1961\n#&gt; 2             1   1.5       1971\n#&gt; 3             1   2         1962\n\nIt does not use row-wise arithmetic additions of the two columns. To do that, you can use the identify function:\n\n# One slope term, not two\nf_02 &lt;- Sale_Price ~  I(Full_Bath + Half_Bath)\n\nSymbolic addition creates separate columns of the data set. In chapter TODO, we’ll discuss main effects and interactions. The main effects are features composed of a single predictor (as in f_01 above). Interaction effects are one or more model terms that combine the information of all the predictors in a multiplicative way. There are a few ways to specify them. Here are three methods for specifying two-factor interactions between predictors:\n\n# `:` is used for specific interactions\nf_03 &lt;- Sale_Price ~  Baths + Year_Built + Baths:Year_Built\n\n# `*` is used to make all interactions of two or more terms\nf_04 &lt;- Sale_Price ~  Baths * Year_Built\n\n# `()^D` makes interactions up to order D of all of the columns\n# within the parenthesis\nf_05 &lt;- Sale_Price ~  (Baths + Year_Built)^2\n\nshow_columns(f_05)\n#&gt; # A tibble: 3 × 4\n#&gt;   `(Intercept)` Baths Year_Built `Baths:Year_Built`\n#&gt;           &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;              &lt;dbl&gt;\n#&gt; 1             1   1         1961              1961 \n#&gt; 2             1   1.5       1971              2956.\n#&gt; 3             1   2         1962              3924\n\nFor this data set, the right-hand side of f_05 could be shortened to (.)^2.\nSince Baths and Year_Built are both numeric, their interactions are created by simply multiplying their values, i.e., I(Baths * Year_Built).\nBy default, the model formula creates an intercept column where the value of each row is 1.0. To prevent the intercept from being added, there are two syntaxes:\n\nf_06 &lt;- Sale_Price ~  Baths - 1 \nf_07 &lt;- Sale_Price ~  Baths + 0\n\nshow_columns(f_07)\n#&gt; # A tibble: 3 × 1\n#&gt;   Baths\n#&gt;   &lt;dbl&gt;\n#&gt; 1   1  \n#&gt; 2   1.5\n#&gt; 3   2\n\nWhat happens with factor predictors? Their specification is the same:\n\nf_08 &lt;- Sale_Price ~  Bldg_Type \n\nHowever, most of the time1, the formula method creates columns of binary 0/1 to replace the original factor column. Since there are 5 possible values of Bldg_Type, the formula creates 4 columns of indicator variables, each corresponding to a specific level. The first factor level is excluded by default. This is discussed more in Working with Categorical Predictors.\n\n# Note that the resulting column names smash the original column\n# name an its factor level together with no delimiter. \nshow_columns(f_08)\n#&gt; # A tibble: 3 × 5\n#&gt;   `(Intercept)` Bldg_TypeTwoFmCon Bldg_TypeDuplex Bldg_TypeTwnhs Bldg_TypeTwnhsE\n#&gt;           &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1             1                 0               0              0               0\n#&gt; 2             1                 0               0              1               0\n#&gt; 3             1                 0               1              0               0\n\nFor interaction terms, the syntax is the same as the one shown above. In the case of categorical predictors, all combinations of the predictors are created. In the following case, Central_Air has two levels. A two-way interaction of these two predictors creates 4 \\(\\times\\) 1 = 4 interaction columns.\n\nf_09 &lt;- Sale_Price ~  (Bldg_Type + Central_Air)^2\n\nshow_columns(f_09)\n#&gt; # A tibble: 3 × 10\n#&gt;   `(Intercept)` Bldg_TypeTwoFmCon Bldg_TypeDuplex Bldg_TypeTwnhs Bldg_TypeTwnhsE\n#&gt;           &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1             1                 0               0              0               0\n#&gt; 2             1                 0               0              1               0\n#&gt; 3             1                 0               1              0               0\n#&gt; # ℹ 5 more variables: Central_AirY &lt;dbl&gt;, `Bldg_TypeTwoFmCon:Central_AirY` &lt;dbl&gt;,\n#&gt; #   `Bldg_TypeDuplex:Central_AirY` &lt;dbl&gt;, `Bldg_TypeTwnhs:Central_AirY` &lt;dbl&gt;,\n#&gt; #   `Bldg_TypeTwnhsE:Central_AirY` &lt;dbl&gt;\n\nWhat happens when you exclude the intercept? All factor levels receive a binary indicator column for a single categorical predictor.\n\nf_10 &lt;- Sale_Price ~  Bldg_Type + 0\n\nshow_columns(f_10)\n#&gt; # A tibble: 3 × 5\n#&gt;   Bldg_TypeOneFam Bldg_TypeTwoFmCon Bldg_TypeDuplex Bldg_TypeTwnhs Bldg_TypeTwnhsE\n#&gt;             &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1               1                 0               0              0               0\n#&gt; 2               0                 0               0              1               0\n#&gt; 3               0                 0               1              0               0\n\nHowever, this may produce unexpected results when multiple factor predictors exist. The first factor in the formula creates all possible indicators (e.g., 5 for Bldg_Type) while the others have all but one factor level created. For example, these two formulas would have different columns:\n\nf_11 &lt;- Sale_Price ~ Bldg_Type + Central_Air + 0\nf_12 &lt;- Sale_Price ~ Central_Air + Bldg_Type + 0\n\nshow_columns(f_11) %&gt;% names() %&gt;% sort()\n#&gt; [1] \"Bldg_TypeDuplex\"   \"Bldg_TypeOneFam\"   \"Bldg_TypeTwnhs\"    \"Bldg_TypeTwnhsE\"  \n#&gt; [5] \"Bldg_TypeTwoFmCon\" \"Central_AirY\"\nshow_columns(f_12) %&gt;% names() %&gt;% sort()\n#&gt; [1] \"Bldg_TypeDuplex\"   \"Bldg_TypeTwnhs\"    \"Bldg_TypeTwnhsE\"   \"Bldg_TypeTwoFmCon\"\n#&gt; [5] \"Central_AirN\"      \"Central_AirY\"\n\nThere model predictions and anova() results will be the same but the interpretation of their coefficients will be very different.\nYou can use in-line functions within a recipe. For example:\n\nlibrary(splines2)\n#&gt; Warning: package 'splines2' was built under R version 4.3.3\nf_13 &lt;- Sale_Price ~  log(Gr_Liv_Area) + scale(Lot_Area) + naturalSpline(Latitude, df = 3)\n\nshow_columns(f_13)\n#&gt; # A tibble: 3 × 6\n#&gt;   `(Intercept)` `log(Gr_Liv_Area)` `scale(Lot_Area)`\n#&gt;           &lt;dbl&gt;              &lt;dbl&gt;             &lt;dbl&gt;\n#&gt; 1             1              6.798            0.1994\n#&gt; 2             1              6.895           -1.103 \n#&gt; 3             1              7.455            0.4139\n#&gt; # ℹ 3 more variables: `naturalSpline(Latitude, df = 3)1` &lt;dbl&gt;,\n#&gt; #   `naturalSpline(Latitude, df = 3)2` &lt;dbl&gt;,\n#&gt; #   `naturalSpline(Latitude, df = 3)3` &lt;dbl&gt;\n\nuses three in-line functions:\n\nThe first is a simple log transformation of the gross living area.\nThe use of scale() will compute the mean and standard deviation of Lot_Area and use those to center and scale that column.\nThe function splines2::naturalSpline() will create a set of basis functions (described in chapter TODO) that will replace the original Latitude column.\n\nIn the second and third cases, R’s machinery will estimate the relevant statistics and embed them as attributes in the corresponding columns. For each in-line function, the exact same operations are conducted on new data (say when predict() is called).\nFinally, be aware that each formula captures the environment in which it was created. For example:\n\nenvironment(f_12)\n#&gt; &lt;environment: R_GlobalEnv&gt;\n\n# The number of objects in the session used to create this web page (up to now):\nlength(ls(envir = environment(f_12)))\n#&gt; [1] 22\n\nIf an object that used f_12 is saved to disk, it will also contain the 22 objects in the global environment. If any of these objects are large, it can unintentionally make the saved data object large. Note that using the base function object.size() will not take into account anything stored in the environment (so the binary file size is underestimated). lobstr::obj_size() will give a more accurate estimate.\nThe butcher package has tools to strip off these unneeded objects from formulas (or objects that contain formulas). Also, butcher::weigh() returns a tibble with the size of each element contained in the object (if any).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-recipe-intro",
    "href": "chapters/numeric-predictors.html#sec-recipe-intro",
    "title": "4  Transforming Numeric Predictors",
    "section": "\n4.4 What is a Recipe?",
    "text": "4.4 What is a Recipe?\nA recipe is a set of sequential steps that specify what operations should be conducted on a set of predictors. Operations could include:\n\nModifying a predictor’s encoding (e.g., date to month/day/year columns)\nAdding new features, such as basis expansions.\nStandardizing or transforming individual predictors.\nFeature extraction or embeddings on multiple predictors.\n\nRemoving features.\n\nRecipes can be used by themselves or as part of a modeling pipeline. For illustration, we’ll show how to use them directly. The process is to\nspecify -&gt; estimate -&gt; apply\nthe recipe. In terms of syntax, the analogous functions are:\nrecipe() -&gt; prep() -&gt; bake()\nWe’ll start simply by trying to “unskew” a predictor’s distribution.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-recipe-skewness",
    "href": "chapters/numeric-predictors.html#sec-recipe-skewness",
    "title": "4  Transforming Numeric Predictors",
    "section": "\n4.5 Resolving Skewness",
    "text": "4.5 Resolving Skewness\nThe main text mentions that the distribution of the Lot_Area variable is skewed. Let’s see what that looks like.\n\names_train %&gt;% \n  ggplot(aes(Lot_Area)) + \n  geom_histogram(bins = 30, col = \"white\", fill = \"#8E195C\", alpha = 1 / 2) +\n  geom_rug(alpha = 1 / 2, length = unit(0.03, \"npc\"), linewidth = 1) +\n  labs(x = \"Lot Area\")\n\n\n\n\n\n\n\nTo get started, we initialize a recipe with the recipe() function and a data set:\n\nunskew_rec &lt;- recipe(Sale_Price ~ ., data = ames_train)\n\nThe formula method doesn’t do much here: it records the outcome (columns to the left of ~), which are predictors (to the right of ~ ), and their data types. Note that the . in the formula means that all columns, except those to the left, should be considered predictors. When using a formula to start a recipe, keep it simple. It won’t accept any in-line functions (like sqrt() or log()); it wants you to change the variables inside of recipe steps.\nRegarding the data argument: any data set with the appropriate columns could be used. The initial recipe work is just cataloging the columns. You could even use a “zero row slice” such as ames_train[0,] and get the same results. You might want to do something like this if you have a very large training set (to reduce the in-memory footprint). The main advantage of using ames_train is convenience (as we’ll see later).\nWe’ll add different recipe step functions from this initial object to declare what we want to do. Let’s say we will transform the lot area column using the Yeo-Johnsom transformation. To do this:\n\nunskew_rec &lt;- \n  recipe(Sale_Price ~ ., data = ames_train) %&gt;% \n  step_YeoJohnson(Lot_Area)\n\n# or use a dplyr selector:\nunskew_rec &lt;- \n  recipe(Sale_Price ~ ., data = ames_train) %&gt;% \n  step_YeoJohnson(any_of(\"Lot_Area\"))\n\nunskew_rec\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:    1\n#&gt; predictor: 10\n#&gt; \n#&gt; ── Operations\n#&gt; • Yeo-Johnson transformation on: any_of(\"Lot_Area\")\n\nor starts_with(\"Lot_\") and so on.\nThis only specifies what we want to do. Recall that the Yeo-Johnson transformation estimates a transformation parameter from the data. To estimate the recipe, use prep():\n\nunskew_rec &lt;- prep(unskew_rec)\n\n# or, to use a different data set: \nunskew_rec &lt;- prep(unskew_rec, training = ames_train)\n#&gt; Warning in prep(unskew_rec, training = ames_train): ! The previous data will be used by `prep()`.\n#&gt; ℹ The data passed using `training` will be ignored.\nunskew_rec\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:    1\n#&gt; predictor: 10\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 2196 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Yeo-Johnson transformation on: Lot_Area | Trained\n\nNote that the printed recipe shows that Lot_Area was resolved from the original request for any_of(\"Lot_Area\").\nWhat was the estimate of the transformation parameter? The tidy() method can tell us:\n\n# Get the list of steps: \ntidy(unskew_rec)\n#&gt; # A tibble: 1 × 6\n#&gt;   number operation type       trained skip  id              \n#&gt;    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n#&gt; 1      1 step      YeoJohnson TRUE    FALSE YeoJohnson_VL1H3\n\n# Get information about the first step: \ntidy(unskew_rec, number = 1)\n#&gt; # A tibble: 1 × 3\n#&gt;   terms     value id              \n#&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;           \n#&gt; 1 Lot_Area 0.1503 YeoJohnson_VL1H3\n\nNow that we have a trained recipe, we can use it via bake():\n\n# Get the list of steps: \nbake(unskew_rec, new_data = head(ames_train))\n#&gt; # A tibble: 6 × 11\n#&gt;   Bldg_Type Neighborhood    Year_Built Gr_Liv_Area Year_Sold Lot_Area Central_Air\n#&gt;   &lt;fct&gt;     &lt;fct&gt;                &lt;int&gt;       &lt;int&gt;     &lt;int&gt;    &lt;dbl&gt; &lt;fct&gt;      \n#&gt; 1 OneFam    North_Ames            1961         896      2010    20.52 Y          \n#&gt; 2 OneFam    North_Ames            1971         864      2010    20.11 Y          \n#&gt; 3 Twnhs     Briardale             1971         987      2010    13.67 Y          \n#&gt; 4 Twnhs     Briardale             1971        1092      2010    13.67 Y          \n#&gt; 5 Twnhs     Northpark_Villa       1975         836      2010    14.62 Y          \n#&gt; 6 OneFam    Sawyer_West           1920        1012      2010    19.83 N          \n#&gt; # ℹ 4 more variables: Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt;, Baths &lt;dbl&gt;,\n#&gt; #   Sale_Price &lt;dbl&gt;\n\nDid it work? Let’s look at the whole training set:\n\nunskew_rec %&gt;% \n  bake(new_data = ames_train) %&gt;% \n  ggplot(aes(Lot_Area)) +\n  geom_rug(alpha = 1 / 2, length = unit(0.03, \"npc\"), linewidth = 1) + \n  geom_histogram(bins = 30, col = \"white\", fill = \"#8E195C\", alpha = 1 / 2) +\n  labs(x = \"Lot Area\")\n\n\n\n\n\n\n\nOne shortcut we can take: the recipe has to apply each step to the training data after it estimates the step. By default, the recipe object saves the processed version of the data set. This can be turned off using the retain = FALSE option to prep(). Since the training set is already in the recipe, we can get it with no additional computations using\nbake(unskew_rec, new_data = NULL) \nThe main site mentions a few other methods that could be used besides Yeo-Johnson:\n\nBox-Cox: step_BoxCox()\n\nPercentile: step_percentile()\n\norderNorm: step_orderNorm()\n\n\nNote that the last method has its step function in the bestNormalize package; various recipe extension packages can be used. A full set of recipe steps for CRAN packages is available on tidymodels.org.\nThere is also a general step for simple computations that do not need to be estimated. If we were to log transform the data, we would use:\nrecipe(Sale_Price ~ ., data = ames_train) %&gt;% \n  step_mutate(Lot_Area = log10(Lot_Area))\nOther single variable transformations can be found in the following R packages: car, trafo, and Transform.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-recipe-selectors",
    "href": "chapters/numeric-predictors.html#sec-recipe-selectors",
    "title": "4  Transforming Numeric Predictors",
    "section": "\n4.6 More on Recipe Selectors",
    "text": "4.6 More on Recipe Selectors\nThe previous section showed a recipe step that operated on a single column. You can select one or more predictors in a variety of different ways within a recipe:\n\nBare, unquoted column names such as Lot_Area.\n\ndplyr package selectors, including starts_with(), contained(), and so on.\nSpecial, recipe-only selectors:\n\nRole-based: all_predictors(), all_outcomes(), and so on.\nType-based: all_numeric(), all_factor(), …\nCombinations: all_numeric_predictors() etc.\n\n\n\nTwo important dplyr selectors are all_of() and any_of(). These take character vectors of column names as inputs. all_of() will select all of the columns in the vector and will fail if they are not all present when the recipe step is executed. any_of() will select any of the columns that are given and won’t fail, even if none are available.\nThis is important for a few reasons. Some steps can combine or eliminate columns. A recipe should be fault tolerant; if the previous step removed column A and the next step strictly requires it, it will fail. However, if any_of(c(\"A\")) is used, it will not 2.\nThere is a documentation page for recipe selectors as well as the reference page.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-recipe-standardize",
    "href": "chapters/numeric-predictors.html#sec-recipe-standardize",
    "title": "4  Transforming Numeric Predictors",
    "section": "\n4.7 Standardizing to a common scale",
    "text": "4.7 Standardizing to a common scale\nThe two main steps for standardizing columns to have the same units are step_normalize() and step_range(). A common pattern for the former is:\n\nnorm_rec &lt;- \n  unskew_rec %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\nnorm_rec\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:    1\n#&gt; predictor: 10\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 2196 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Yeo-Johnson transformation on: Lot_Area | Trained\n#&gt; • Zero variance filter on: all_predictors()\n#&gt; • Centering and scaling for: all_numeric_predictors()\n\nstep_zv() is for removing “zero-variance” (zv) predictors. These are columns with a single unique value. Since step_normalize() will try to divide by a column’s standard deviation, this will fail if there is no variation in the column. step_zv() will remove such columns that exist in the training set.\nWe recycled the previous recipe, which has already been trained. Note that in the output above, only the first step is labeled as “Trained”. When we run prep() on this recipe, it only estimates the remaining two steps.\nAgain, once we prep(are) the recipe, we can use bake() to get the normalized data.\nAnother important point is that recipes are designed to utilize different data sets appropriately. The training set is used with prep() and ensures that all the estimations are based on it. There is, as is appropriate, no re-estimation of quantities when new data are processed.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-recipe-spatialp-sign",
    "href": "chapters/numeric-predictors.html#sec-recipe-spatialp-sign",
    "title": "4  Transforming Numeric Predictors",
    "section": "\n4.8 Spatial Sign",
    "text": "4.8 Spatial Sign\nUnsurprisingly, the step to compute the spatial sign is step_spatialsign(). It projects two or more numeric columns onto a multidimensional hypersphere. The resulting data has columns the same name as the input:\n\nlibrary(bestNormalize)\n\nsp_sign_rec &lt;- \n  recipe(Sale_Price ~ Lot_Area + Gr_Liv_Area, data = ames_train) %&gt;% \n  step_YeoJohnson(any_of(c(\"Lot_Area\", \"Gr_Liv_Area\"))) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_orderNorm(all_numeric_predictors()) %&gt;% \n  step_spatialsign(all_numeric_predictors()) %&gt;% \n  prep()\n\nsp_sign_data &lt;- bake(sp_sign_rec, new_data = NULL)\nsp_sign_data\n#&gt; # A tibble: 2,196 × 3\n#&gt;   Lot_Area Gr_Liv_Area Sale_Price\n#&gt;      &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1   0.4496     -0.8932      5.021\n#&gt; 2   0.2349     -0.9720      5.061\n#&gt; 3  -0.9012     -0.4334      4.982\n#&gt; 4  -0.9474     -0.3199      4.944\n#&gt; 5  -0.7424     -0.6700      5.079\n#&gt; 6   0.1537     -0.9881      4.829\n#&gt; # ℹ 2,190 more rows\n\n\nsp_sign_data %&gt;% \n  ggplot(aes(Lot_Area, Gr_Liv_Area)) +\n  geom_point(cex =  2, alpha = 1 / 10, pch = 1) +\n  coord_equal()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-recipe-resources",
    "href": "chapters/numeric-predictors.html#sec-recipe-resources",
    "title": "4  Transforming Numeric Predictors",
    "section": "\n4.9 Other resources for learning about recipes",
    "text": "4.9 Other resources for learning about recipes\n\n\ntidymodels.org: Preprocess your data with recipes\n\n\nTMwR chapter: Feature Engineering with recipes\n\n\nTMwR chapter: Dimensionality Reduction\n\n2023 Posit conference workshop slides: Intro: Using recipes\n\n2023 Posit conference workshop slides: Feature engineering using recipes\n\nRoles in recipes\nOrdering of steps\nStackoverflow Questions tagged [r-recipes]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#footnotes",
    "href": "chapters/numeric-predictors.html#footnotes",
    "title": "4  Transforming Numeric Predictors",
    "section": "",
    "text": "Some model functions require these binary indicators, and others do not. You should assume they convert factor predictors to binary indicators; we will alter you when a specific function does not.↩︎\nMore accurately, it will probably be fine. Most steps are permissive; others are not. The previously described step_mutate() would fail if Lot_Area was previously eliminated.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html",
    "href": "chapters/categorical-predictors.html",
    "title": "5  Working with Categorical Predictors",
    "section": "",
    "text": "5.1 Requirements\nYou’ll need 5 packages (embed, rpart, text2vec, textrecipes, and tidymodels) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"embed\", \"text2vec\", \"textrecipes\", \"tidymodels\", \"rpart\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#the-hotel-rate-data",
    "href": "chapters/categorical-predictors.html#the-hotel-rate-data",
    "title": "5  Working with Categorical Predictors",
    "section": "\n5.2 The Hotel Rate Data",
    "text": "5.2 The Hotel Rate Data\nThe hotel rate data are used for most examples in the chapter. The original version is in the modeldata package. We’ll split the data in the following way:\n\ndata(hotel_rates, package = \"modeldata\")\n\n# Make the initial split\nhotel_rates &lt;- hotel_rates %&gt;% arrange(arrival_date)\nhotel_rate_split &lt;- initial_time_split(hotel_rates, prop = c(0.75))\nhotel_rate_train &lt;- training(hotel_rate_split)\nhotel_rate_test  &lt;- testing(hotel_rate_split)\n\ninitial_time_split() will reserve that most recent 25% for the test set. It assumes that the data are arranged in time and takes that last part of the data for testing (assumed to be the most recent).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-indicators",
    "href": "chapters/categorical-predictors.html#sec-indicators",
    "title": "5  Working with Categorical Predictors",
    "section": "\n5.3 Simple Indicator Variables",
    "text": "5.3 Simple Indicator Variables\nWe’ll look at two primary methods for creating indicator (a.k.a. “dummy”) variables.\n\n5.3.1 Base R formulas\nBase R’s formula method, discussed previously in Section 4.3, automatically creates indicators when the formula includes a factor predictor. For example:\n\ncustomer_types &lt;- \n  hotel_rate_train %&gt;% \n  distinct(customer_type) %&gt;% \n  arrange(customer_type)\n\ncustomer_types %&gt;% \n  model.matrix( ~ customer_type, data = .) %&gt;% \n  as_tibble() %&gt;% \n  select(-`(Intercept)`)\n#&gt; # A tibble: 4 × 3\n#&gt;   customer_typegroup customer_typetransient customer_typetransient_party\n#&gt;                &lt;dbl&gt;                  &lt;dbl&gt;                        &lt;dbl&gt;\n#&gt; 1                  0                      0                            0\n#&gt; 2                  1                      0                            0\n#&gt; 3                  0                      1                            0\n#&gt; 4                  0                      0                            1\n\nNote that the column name and the factor levels are directly concatenated.\nmodel.matrix() is part of the larger base R preprocessing framework and always returns a matrix (by default, with an intercept column). There is also model.frame(). This returns a data frame without creating indicator columns or interactions. It does execute any in-line functions and only returns the columns involved in the formula.\nWhen the factor has missing values, the default behavior is to remove the offending row:\n\nlvls &lt;- levels(hotel_rate_train$customer_type)\n\nwith_missing &lt;- \n  customer_types %&gt;% \n  bind_rows(tibble(customer_type = factor(NA, levels = lvls)))\n\nwith_missing\n#&gt; # A tibble: 5 × 1\n#&gt;   customer_type  \n#&gt;   &lt;fct&gt;          \n#&gt; 1 contract       \n#&gt; 2 group          \n#&gt; 3 transient      \n#&gt; 4 transient_party\n#&gt; 5 &lt;NA&gt;\n\nmodel.matrix( ~ customer_type, data = with_missing) %&gt;% \n  as_tibble() %&gt;% \n  select(-`(Intercept)`)\n#&gt; # A tibble: 4 × 3\n#&gt;   customer_typegroup customer_typetransient customer_typetransient_party\n#&gt;                &lt;dbl&gt;                  &lt;dbl&gt;                        &lt;dbl&gt;\n#&gt; 1                  0                      0                            0\n#&gt; 2                  1                      0                            0\n#&gt; 3                  0                      1                            0\n#&gt; 4                  0                      0                            1\n\nA family of functions can be used to dictate what should be done when missing values occur. The global R option is\n\noptions()$na.action\n#&gt; [1] \"na.omit\"\n\nTo keep the number of rows intact, you can set the global option to be na.pass:\n\norig_options &lt;- options()\noptions(na.action = 'na.pass')\n\nmodel.matrix( ~ customer_type, data = with_missing) %&gt;% \n  as_tibble() %&gt;% \n  select(-`(Intercept)`)\n#&gt; # A tibble: 5 × 3\n#&gt;   customer_typegroup customer_typetransient customer_typetransient_party\n#&gt;                &lt;dbl&gt;                  &lt;dbl&gt;                        &lt;dbl&gt;\n#&gt; 1                  0                      0                            0\n#&gt; 2                  1                      0                            0\n#&gt; 3                  0                      1                            0\n#&gt; 4                  0                      0                            1\n#&gt; 5                 NA                     NA                           NA\n\n# Now reset to original settings:\noptions(orig_options) \noptions()$na.action\n#&gt; [1] \"na.omit\"\n\nIn R, the word “contrast” refers to the algorithm used to create different types of indicators 1. Global options control this (for the most part) and the defaults are:\n\noptions()$contrast\n#&gt;         unordered           ordered \n#&gt; \"contr.treatment\"      \"contr.poly\"\n\n# with possible options: \napropos(\"contr\\\\.\")\n#&gt; [1] \"contr.helmert\"   \"contr.poly\"      \"contr.SAS\"       \"contr.sum\"      \n#&gt; [5] \"contr.treatment\"\n\nMany packages also have additional contrast functions.\n\n5.3.2 Recipes\nWe can also use a recipe to do this (with more functionality):\n\nind_rec &lt;- \n  recipe( ~ customer_type, data = customer_types) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  prep()\n\nbake(ind_rec, customer_types, starts_with(\"customer_type\"))\n#&gt; # A tibble: 4 × 3\n#&gt;   customer_type_group customer_type_transient customer_type_transient_party\n#&gt;                 &lt;dbl&gt;                   &lt;dbl&gt;                         &lt;dbl&gt;\n#&gt; 1                   0                       0                             0\n#&gt; 2                   1                       0                             0\n#&gt; 3                   0                       1                             0\n#&gt; 4                   0                       0                             1\nbake(ind_rec, with_missing, starts_with(\"customer_type\"))\n#&gt; Warning: ! There are new levels in a factor: `NA`.\n#&gt; # A tibble: 5 × 3\n#&gt;   customer_type_group customer_type_transient customer_type_transient_party\n#&gt;                 &lt;dbl&gt;                   &lt;dbl&gt;                         &lt;dbl&gt;\n#&gt; 1                   0                       0                             0\n#&gt; 2                   1                       0                             0\n#&gt; 3                   0                       1                             0\n#&gt; 4                   0                       0                             1\n#&gt; 5                  NA                      NA                            NA\n\nThere is no need to set the global option for na.action.\nAlso, the naming of features is more rational, with names and levels separated by an underscore. There is also an argument to step_dummy() that controls the naming of new features.\nThere is also an option to produce one-hot encodings called… one_hot.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-recipe-novel",
    "href": "chapters/categorical-predictors.html#sec-recipe-novel",
    "title": "5  Working with Categorical Predictors",
    "section": "\n5.4 Novel Categories",
    "text": "5.4 Novel Categories\nWhen we think the recipe or model will encounter new values of a factor predictor, we can use step_novel() to add a new factor level:\n\nrecipe(avg_price_per_room ~ customer_type, data = hotel_rate_train) %&gt;% \n  step_novel(customer_type) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL) %&gt;% \n  pluck(\"customer_type\") %&gt;% \n  levels()\n#&gt; [1] \"contract\"        \"group\"           \"transient\"       \"transient_party\"\n#&gt; [5] \"new\"\n\nFor the training set, this new level will never have any data associated with it.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-recipe-other",
    "href": "chapters/categorical-predictors.html#sec-recipe-other",
    "title": "5  Working with Categorical Predictors",
    "section": "\n5.5 “Othering”",
    "text": "5.5 “Othering”\nWe can also determine infrequently occurring categories (in the training set) and re-level the factor by converting them to an “other” category. If we chose a frequency of 0.01% as a cutoff, we have far fewer levels:\n\nlength(levels(hotel_rate_train$agent))\n#&gt; [1] 174\n\nrecipe(avg_price_per_room ~ agent, data = hotel_rate_train) %&gt;% \n  step_other(agent, threshold = 0.0001) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL) %&gt;% \n  pluck(\"agent\") %&gt;% \n  levels() %&gt;% \n  length()\n#&gt; [1] 101\n\nIf you are interested in which levels are combined, the tidy() method on the recipe can tell you.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-recipe-hashing",
    "href": "chapters/categorical-predictors.html#sec-recipe-hashing",
    "title": "5  Working with Categorical Predictors",
    "section": "\n5.6 Feature Hashing",
    "text": "5.6 Feature Hashing\nFeature hashing converts qualitative predictors to a set of binary indicators based solely on the value of their category. It is most useful when many categories and/or novel levels might be encountered.\nThe recipe step is in the textrecipes package, so we must load it first2. The main arguments are num_terms and signed. The first sets the number of features to create, and when signed = TRUE, the indicators will have an appropriate sign attached to them (i.e., their values could be -1/0/1).\n\nlibrary(textrecipes)\n\nrecipe(avg_price_per_room ~ agent, data = hotel_rate_train) %&gt;% \n  step_dummy_hash(agent, num_terms = 4) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL, contains(\"agent\")) \n#&gt; # A tibble: 11,551 × 4\n#&gt;   dummyhash_agent_1 dummyhash_agent_2 dummyhash_agent_3 dummyhash_agent_4\n#&gt;               &lt;int&gt;             &lt;int&gt;             &lt;int&gt;             &lt;int&gt;\n#&gt; 1                 0                 1                 0                 0\n#&gt; 2                 0                 0                -1                 0\n#&gt; 3                 0                 1                 0                 0\n#&gt; 4                 0                 1                 0                 0\n#&gt; 5                 0                 1                 0                 0\n#&gt; 6                 0                 0                 0                 1\n#&gt; # ℹ 11,545 more rows",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-recipe-effect-encode",
    "href": "chapters/categorical-predictors.html#sec-recipe-effect-encode",
    "title": "5  Working with Categorical Predictors",
    "section": "\n5.7 Effect Encodings",
    "text": "5.7 Effect Encodings\nEffect embedding is a supervised method to convert qualitative predictors to a numeric column that contains the effect of the category on the outcome. The outcome is a numeric value (the ADR) for these data. Using effect endings here will produce a column with specialized estimates of each category’s mean ADR. Let’s look at the agent predictor again.\nThe embed package has a few recipe steps to do this. This method is sometimes called “likelihood encoding” and the recipe steps all start with step_lencode_*:\n\n\nstep_lencode_glm() produces basic, naive estimates of effect. This is a “no pooling” estimate.\n\nstep_lencode_mixed() uses a non-Bayesian hierarchical model to produce regularized effect estimates.\n\nstep_lencode_bayes() uses a Bayesian model that is more flexible than its non-Bayesian sibling but can take much longer to fit.\n\nWe’ll use the “mixed” function3, For agent:\n\nlibrary(embed)\n\nencoded_agent_rec &lt;- \n  recipe(avg_price_per_room ~ agent, data = hotel_rate_train) %&gt;% \n  step_lencode_mixed(agent, outcome = vars(avg_price_per_room), id = \"effect\") %&gt;% \n  prep() \nencoded_agent_rec\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 1\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 11551 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Linear embedding for factors via mixed effects for: agent | Trained\n\nTo see the actual effect estimates, use the tidy() method:\n\ntidy(encoded_agent_rec, id = \"effect\")\n#&gt; # A tibble: 123 × 4\n#&gt;   level            value terms id    \n#&gt;   &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n#&gt; 1 aaron_marquez    92.22 agent effect\n#&gt; 2 alexander_drake 120.1  agent effect\n#&gt; 3 allen_her        73.70 agent effect\n#&gt; 4 anas_el_bashir   80.41 agent effect\n#&gt; 5 araseli_billy    66.91 agent effect\n#&gt; 6 arhab_al_islam   51.15 agent effect\n#&gt; # ℹ 117 more rows\n\n# The estimate for new agents:\ntidy(encoded_agent_rec, id = \"effect\") %&gt;% \n  slice_tail(n = 1)\n#&gt; # A tibble: 1 × 4\n#&gt;   level value terms id    \n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n#&gt; 1 ..new 75.36 agent effect\n\nWhen the recipe is applied to new data, the agent column is converted to a numeric column with these values:\n\nbake(encoded_agent_rec, hotel_rate_test) %&gt;% \n  bind_cols(hotel_rate_test %&gt;% select(original_col = agent))\n#&gt; # A tibble: 3,851 × 3\n#&gt;    agent avg_price_per_room original_col        \n#&gt;    &lt;dbl&gt;              &lt;dbl&gt; &lt;fct&gt;               \n#&gt; 1 106.9                 100 devin_rivera_borrego\n#&gt; 2 106.9                  60 devin_rivera_borrego\n#&gt; 3  76.38                 45 not_applicable      \n#&gt; 4 106.9                  82 devin_rivera_borrego\n#&gt; 5  76.38                 47 not_applicable      \n#&gt; 6  76.38                 60 not_applicable      \n#&gt; # ℹ 3,845 more rows\n\nFor categorical outcomes, the effect estimate is the log-odds of an event (the first factor level).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-recipe-collapse",
    "href": "chapters/categorical-predictors.html#sec-recipe-collapse",
    "title": "5  Working with Categorical Predictors",
    "section": "\n5.8 Supervised Combining of Categories",
    "text": "5.8 Supervised Combining of Categories\nTo collapse a large number of factor levels to a smaller set using a supervised model, we can use step_collapse_cart() in the embed package.\nFor example:\n\n# Also needs the embed package loaded (and rpart installed)\nlibrary(embed)\n\ncollapse_agent_rec &lt;- \n  recipe(avg_price_per_room ~ agent, data = hotel_rate_train) %&gt;% \n  step_collapse_cart(agent, outcome = vars(avg_price_per_room), id = \"collapse\") %&gt;% \n  prep() \ncollapse_agent_rec\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 1\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 11551 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Collapsing factor levels using CART: agent | Trained\n\nThe step converts 122 unique values of agent in the training set to a smaller set of 12 categories. To see the conversion key, use the tidy() methods:\n\ntidy(collapse_agent_rec, id = \"collapse\")\n#&gt; # A tibble: 122 × 4\n#&gt;   terms old               new      id      \n#&gt;   &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 1 agent araseli_billy     agent_01 collapse\n#&gt; 2 agent arhab_al_islam    agent_01 collapse\n#&gt; 3 agent brayan_guerrero   agent_01 collapse\n#&gt; 4 agent daifallah_el_sami agent_01 collapse\n#&gt; 5 agent dante_merritt     agent_01 collapse\n#&gt; 6 agent derrick_barger    agent_01 collapse\n#&gt; # ℹ 116 more rows\n\ntidy(collapse_agent_rec, id = \"collapse\") %&gt;% \n  count(new)\n#&gt; # A tibble: 12 × 2\n#&gt;   new          n\n#&gt;   &lt;chr&gt;    &lt;int&gt;\n#&gt; 1 agent_01    17\n#&gt; 2 agent_02    14\n#&gt; 3 agent_03    14\n#&gt; 4 agent_04    10\n#&gt; 5 agent_05     7\n#&gt; 6 agent_06    13\n#&gt; # ℹ 6 more rows\n\nThere are two main tuning parameters (described later in section TODO):\n\ncost complexity (a.k.a. \\(C_p\\)): smaller values result in more groups. Values typically range between zero and 0.1.\nminimum n: the minimum number of rows in a group to enable it to keep splitting. Smaller values should result in more groupings.\n\nThese values can be tuned.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-recipe-ordinal",
    "href": "chapters/categorical-predictors.html#sec-recipe-ordinal",
    "title": "5  Working with Categorical Predictors",
    "section": "\n5.9 Working with Ordinal Predictors",
    "text": "5.9 Working with Ordinal Predictors\nAs reported in the section on ordinal data, the default for R is to encode ordinal values with \\(p\\) values is to create a set of \\(p - 1\\) orthogonal polynomial features. That is what step_dummy() does by default.\n\nquality_vals &lt;- c('excellent', 'fair', 'good', 'typical', 'poor')\nquality &lt;- tibble(quality = ordered(quality_vals, levels = quality_vals))\nstr(quality)\n#&gt; tibble [5 × 1] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ quality: Ord.factor w/ 5 levels \"excellent\"&lt;\"fair\"&lt;..: 1 2 3 4 5\n\nrecipe(~ quality, data = quality) %&gt;% \n  step_dummy(quality) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\n#&gt; # A tibble: 5 × 4\n#&gt;    quality_1 quality_2  quality_3 quality_4\n#&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 -6.325e- 1    0.5345 -3.162e- 1    0.1195\n#&gt; 2 -3.162e- 1   -0.2673  6.325e- 1   -0.4781\n#&gt; 3 -3.288e-17   -0.5345  9.637e-17    0.7171\n#&gt; 4  3.162e- 1   -0.2673 -6.325e- 1   -0.4781\n#&gt; 5  6.325e- 1    0.5345  3.162e- 1    0.1195\n\nWe can convert the ordered factor to an unordered factor:\n\nrecipe(~ quality, data = quality) %&gt;% \n  step_unorder(quality) %&gt;% \n  step_dummy(quality) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\n#&gt; # A tibble: 5 × 4\n#&gt;   quality_fair quality_good quality_typical quality_poor\n#&gt;          &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1            0            0               0            0\n#&gt; 2            1            0               0            0\n#&gt; 3            0            1               0            0\n#&gt; 4            0            0               1            0\n#&gt; 5            0            0               0            1\n\nAnother strategy is mapping the ordinal factor levels to a set of numeric values that make sense for their modeling problem. step_ordinalscore() can do that with a user-supplied conversion function:\n\nconvert_to_prime &lt;-  function(x) {\n  primes &lt;- c(2, 3, 7, 11, 13)\n  primes[as.numeric(x)]\n}\n\nrecipe(~ quality, data = quality) %&gt;% \n  step_ordinalscore(quality, convert = convert_to_prime) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\n#&gt; # A tibble: 5 × 1\n#&gt;   quality\n#&gt;     &lt;int&gt;\n#&gt; 1       2\n#&gt; 2       3\n#&gt; 3       7\n#&gt; 4      11\n#&gt; 5      13\n\nstep_integer() does the same for either type of factor but converts them to consecutive one-based integers.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#other-relevant-recipe-steps",
    "href": "chapters/categorical-predictors.html#other-relevant-recipe-steps",
    "title": "5  Working with Categorical Predictors",
    "section": "\n5.10 Other relevant recipe steps",
    "text": "5.10 Other relevant recipe steps\nThere are a variety of other steps that can be used with qualitative predictors (a list of relevant recipe steps in recipes)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#footnotes",
    "href": "chapters/categorical-predictors.html#footnotes",
    "title": "5  Working with Categorical Predictors",
    "section": "",
    "text": "In statistics, “contrasts” typically indicates a combination of parameters. For example, if you wanted to test that the average of two parameters was equal to a third, the contrast would be \\(\\beta_1 + \\beta_2 - 2\\beta_3\\) and the contrast coefficients would be c(1, 1, -2).↩︎\nIt also requires another package (text2vec) to be installed but not loaded.↩︎\nFor this function, we all need the lme4 package to be installed.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html",
    "href": "chapters/embeddings.html",
    "title": "6  Embeddings",
    "section": "",
    "text": "6.1 Requirements\nYou’ll need 13 packages (bestNormalize, dimRed, embed, fastICA, igraph, mixOmics, modeldatatoo, patchwork, RANN, RSpectra, tidymodels, uwot, viridis) for this chapter. The mixOmics is a Bioconductor package and is not on CRAN. For the others, we can install them as usual but we’ll get mixOmics from GitHub:\nreq_pkg &lt;- c(\"bestNormalize\", \"dimRed\", \"embed\", \"fastICA\", \"igraph\", \n             \"mixOmics\", \"modeldatatoo\", \"patchwork\", \"RANN\", \"RSpectra\", \n             \"tidymodels\", \"uwot\", \"viridis\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  \n  # mixOmics is not on CRAN\n  cran_install_list &lt;- install_list[install_list != \"mixOmics\"]\n  if ( length(cran_install_list) &gt; 0 ) {\n    pak::pak(cran_install_list)\n  }\n  \n  # Get mixOmics from github\n  if ( \"mixOmics\" %in% install_list ) {\n    pak::pak(\"mixOmicsTeam/mixOmics\")\n  }\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\nlibrary(viridis)\nlibrary(embed) # for umap\nlibrary(patchwork)\n\ntidymodels_prefer()\ntheme_set(theme_bw())",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-barley",
    "href": "chapters/embeddings.html#sec-barley",
    "title": "6  Embeddings",
    "section": "\n6.2 Example: Predicting Barley Amounts",
    "text": "6.2 Example: Predicting Barley Amounts\nThe data are in the modeldatatoo package. Let’s load the data, remove two outcome columns that will not be analyzed here, and conduct a three-way split of the data:\n\nlibrary(modeldatatoo)\n\nchimiometrie_2019 &lt;-\n  data_chimiometrie_2019()  %&gt;%\n  select(-soy_oil, -lucerne)\n\nbarley_breaks &lt;- (0:27) * 2\n\nset.seed(101)\nbarley_split &lt;- initial_validation_split(chimiometrie_2019, prop = c(0.7, 0.15), strata = barley)\nbarley_train &lt;- training(barley_split)\nbarley_val &lt;- validation(barley_split)\nbarley_test &lt;- testing(barley_split)\nbarley_rs &lt;- validation_set(barley_split)\n\nwave &lt;- tibble(index = 1:550, wavelength = seq(1300, 2398, by = 2))\nwave_corr &lt;- \n  barley_train %&gt;% \n  select(starts_with(\"wv\")) %&gt;% \n  cor()\nwave_corr &lt;- wave_corr[upper.tri(wave_corr)]\n\nchimiometrie_2019$barley_bin &lt;-\n  cut(chimiometrie_2019$barley,\n      breaks = barley_breaks,\n      include.lowest = TRUE) \n\nThe column names for the predictors are wvlgth_001 through wvlgth_550.\nThe primary recipe used for almost all of the embedding methods is:\n\nlibrary(bestNormalize) # for ORD transformation\n\nbarley_rec &lt;-\n  recipe(barley ~ ., data = barley_train) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;%\n  # Pre-compute to save time later\n  prep()\n\nbarley_rec\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:     1\n#&gt; predictor: 550\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 4839 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • orderNorm transformation on: wvlgth_001, wvlgth_002, wvlgth_003, ... | Trained\n\nIf you use a recipe, most of the embedding methods can be computed with a common interface. The recipe step functions are mostly in the recipes package, although some live in “side packages,” such as the embed package. We’ll be clear about which package is needed for each.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-linear-embed",
    "href": "chapters/embeddings.html#sec-linear-embed",
    "title": "6  Embeddings",
    "section": "\n6.3 Linear Transformations",
    "text": "6.3 Linear Transformations\nWe’ll look at the three linear methods described in the text.\n\n6.3.1 Principal Component Analysis\nUnsurprisingly, the recipe step needed here is called step_pca(). We’ll add an id argument to more easily reference the step of interest.\n\nbarley_pca_rec &lt;-\n  barley_rec %&gt;%\n  step_pca(all_numeric_predictors(), num_comp = 2, id = \"pca\") %&gt;% \n  prep()\n\nbarley_pca_rec\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:     1\n#&gt; predictor: 550\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 4839 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • orderNorm transformation on: wvlgth_001, wvlgth_002, wvlgth_003, ... | Trained\n#&gt; • PCA extraction with: wvlgth_001, wvlgth_002, wvlgth_003, ... | Trained\n\nTo further investigate the results, the tidy() method can extract elements of the computations. For example, you can return how variance each component captures using the argument type = \"variance\". Note that when the PCA recipe step was added, we used the option id = “pca”. This is not required, but it makes it easier to specify what step the tidy() method should consider:\n\npca_scree &lt;- tidy(barley_pca_rec, id = \"pca\", type = \"variance\")\npca_scree\n#&gt; # A tibble: 2,200 × 4\n#&gt;   terms       value component id   \n#&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;\n#&gt; 1 variance 507.5            1 pca  \n#&gt; 2 variance  35.84           2 pca  \n#&gt; 3 variance   3.395          3 pca  \n#&gt; 4 variance   1.511          4 pca  \n#&gt; 5 variance   0.6940         5 pca  \n#&gt; 6 variance   0.4265         6 pca  \n#&gt; # ℹ 2,194 more rows\n\npca_scree %&gt;% count(terms)\n#&gt; # A tibble: 4 × 2\n#&gt;   terms                           n\n#&gt;   &lt;chr&gt;                       &lt;int&gt;\n#&gt; 1 cumulative percent variance   550\n#&gt; 2 cumulative variance           550\n#&gt; 3 percent variance              550\n#&gt; 4 variance                      550\n\nNote that there are 550 entries for each since there are 550 predictor columns.\nThe default option for the tidy() method with PCA is to return the estimated loadings. This can help untangle which predictors influence the PCA components the most (or least).\n\npca_loadings &lt;- tidy(barley_pca_rec, id = \"pca\")\npca_loadings\n#&gt; # A tibble: 302,500 × 4\n#&gt;   terms         value component id   \n#&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;\n#&gt; 1 wvlgth_001 -0.01696 PC1       pca  \n#&gt; 2 wvlgth_002 -0.01704 PC1       pca  \n#&gt; 3 wvlgth_003 -0.01713 PC1       pca  \n#&gt; 4 wvlgth_004 -0.01723 PC1       pca  \n#&gt; 5 wvlgth_005 -0.01734 PC1       pca  \n#&gt; 6 wvlgth_006 -0.01748 PC1       pca  \n#&gt; # ℹ 302,494 more rows\n\nThere are 550^2 = 302500 possible loadings.\nTo get the component values for new data, such as the validation set, the bake() method can be used. Using new_data = NULL returns the training set points:\n\nbarley_pca_rec %&gt;% \n  bake(new_data = NULL, starts_with(\"PC\"))\n#&gt; # A tibble: 4,839 × 2\n#&gt;       PC1   PC2\n#&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2.546  6.650\n#&gt; 2  0.1477 6.821\n#&gt; 3 -3.638  5.551\n#&gt; 4 -5.344  5.264\n#&gt; 5 -5.064  4.263\n#&gt; 6  8.857  8.426\n#&gt; # ℹ 4,833 more rows\n\nSince we used num_comp = 2, two new features were generated.\nWe can also pass new data in, such as the validation set:\n\npca_score_plot &lt;- \n  barley_pca_rec %&gt;% \n  bake(new_data = barley_val) %&gt;% \n  ggplot(aes(PC1, PC2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\")\n\npca_score_plot\n\n\n\n\n\n\n\nNote the difference in the axis ranges. If we are considering how much the PCA components explain the original predictors (i.e., not the outcome), it can be very helpful to keep the axis scales common:\n\npca_score_plot + coord_obs_pred()\n\n\n\n\n\n\n\nThis helps avoid over-interpreting proportionally small patterns in the later components.\nThe functions embed::step_pca_sparse() and embed::step_pca_sparse_bayes() have sparse/regularized estimation methods for PCA. Each has an argument called predictor_prop() that attempts to control how much sparsity should be used. predictor_prop = 0 should approximate regular PCA, and values near 1.0 would produce very few non-zero loadings.\n\n6.3.2 Independent Component Analysis\nAn ICA recipe step can also be found in the recipes package. The syntax is virtually identical:\n\nset.seed(538)\nbarley_ica_rec &lt;-\n  recipe(barley ~ ., data = barley_train) %&gt;% \n  step_ica(all_numeric_predictors(), num_comp = 2, id = \"ica\") %&gt;% \n  prep()\n\nSimilarly, the tidy() method returns the ICA loadings:\n\ntidy(barley_ica_rec, id = \"ica\")\n#&gt; # A tibble: 1,100 × 4\n#&gt;   terms      component    value id   \n#&gt;   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1 wvlgth_001 IC1       -0.02197 ica  \n#&gt; 2 wvlgth_001 IC2        0.9340  ica  \n#&gt; 3 wvlgth_002 IC1       -0.02196 ica  \n#&gt; 4 wvlgth_002 IC2        0.9307  ica  \n#&gt; 5 wvlgth_003 IC1       -0.02196 ica  \n#&gt; 6 wvlgth_003 IC2        0.9270  ica  \n#&gt; # ℹ 1,094 more rows\n\nMost other dimension reduction techniques (but not PCA and PLS) depend on random numbers. We’ll set them when needed, but it is worth pointing out that you will likely get different results each time you run them.\nFor example, when two ICA components are used, the results are not the same but close when using a different random number seed.\n\nset.seed(955)\nica_redo &lt;- \n  recipe(barley ~ ., data = barley_train) %&gt;% \n  step_ica(all_numeric_predictors(), num_comp = 2, id = \"ica\") %&gt;% \n  prep()\n\nica_redo %&gt;% tidy(id = \"ica\")\n#&gt; # A tibble: 1,100 × 4\n#&gt;   terms      component    value id   \n#&gt;   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1 wvlgth_001 IC1        0.9341  ica  \n#&gt; 2 wvlgth_001 IC2       -0.01989 ica  \n#&gt; 3 wvlgth_002 IC1        0.9307  ica  \n#&gt; 4 wvlgth_002 IC2       -0.01989 ica  \n#&gt; 5 wvlgth_003 IC1        0.9270  ica  \n#&gt; 6 wvlgth_003 IC2       -0.01990 ica  \n#&gt; # ℹ 1,094 more rows\n\nThe individual loading values are different between runs, and components one and two are swapped between invocations with different seeds:\n\nica_1 &lt;- \n  barley_ica_rec %&gt;% \n  bake(new_data = barley_val) %&gt;% \n  ggplot(aes(IC1, IC2, col = barley)) + \n  geom_point(alpha = 1 / 4, show.legend = FALSE) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred() +\n  labs(title = \"seed = 538\")\n\nica_2 &lt;- \n  ica_redo %&gt;% \n  bake(new_data = barley_val) %&gt;% \n  ggplot(aes(IC1, IC2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred() +\n  labs(title = \"seed = 955\")\n\nica_1 + ica_2\n\n\n\n\n\n\n\nThis might not cause a difference in performance when the features are used in a predictive model, but if the model uses slopes and intercepts, the parameter estimates will be different each time it is run.\n\n6.3.3 Partial Least Squares\nThe syntax for PLS is also very similar. However, it is a supervised method, so we need to specify the column containing the outcome (the outcome column is not needed after model training). The code below uses dplyr::vars() to declare the column name, but a simple character string can also be used.\n\nbarley_pls_rec &lt;-\n  barley_rec %&gt;%\n  step_pls(all_numeric_predictors(), outcome = vars(barley), num_comp = 2,\n           id = \"pls\") %&gt;% \n  prep()\n\n# Loadings: \ntidy(barley_pls_rec, id = \"pls\")\n#&gt; # A tibble: 1,100 × 4\n#&gt;   terms         value component id   \n#&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;\n#&gt; 1 wvlgth_001 -0.05632 PLS1      pls  \n#&gt; 2 wvlgth_001 -0.1572  PLS2      pls  \n#&gt; 3 wvlgth_002 -0.05637 PLS1      pls  \n#&gt; 4 wvlgth_002 -0.1571  PLS2      pls  \n#&gt; 5 wvlgth_003 -0.05642 PLS1      pls  \n#&gt; 6 wvlgth_003 -0.1570  PLS2      pls  \n#&gt; # ℹ 1,094 more rows",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-mds",
    "href": "chapters/embeddings.html#sec-mds",
    "title": "6  Embeddings",
    "section": "\n6.4 Multidimensional Scaling",
    "text": "6.4 Multidimensional Scaling\ntidymodels contains recipe steps for Isomap and UMAP. The latter is accessible via the embed package.\n\n6.4.1 Isomap\nAgain, the syntax is very similar to the previous unsupervised methods. The main two tuning parameters are num_terms and neighbors. We should also set the seed before execution.\n\nset.seed(221)\nbarley_isomap_rec &lt;-\n  barley_rec %&gt;%\n  step_isomap(all_numeric_predictors(), neighbors = 10, num_terms = 2) %&gt;% \n  prep()\n\nWe can project this preprocessing model onto new data:\n\nbarley_isomap_rec %&gt;% \n  bake(new_data = barley_val) %&gt;% \n  ggplot(aes(Isomap1, Isomap2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred()\n\n\n\n\n\n\n\n\n6.4.2 UMAP\nstep_umap(), in the embed package, has a number of tuning parameters: neighbors, num_comp, min_dist, learn_rate, epochs, initial (initialization method, e.g. “pca”), and the optional target_weight.\nFor an unsupervised embedding:\n\nset.seed(724)\nbarley_umap_rec &lt;-\n  barley_rec %&gt;%\n  step_umap(all_numeric_predictors(), neighbors = 10, num_comp = 2) %&gt;% \n  prep()\n\nProjection on new data has the same syntax:\n\nbarley_umap_rec %&gt;% \n  bake(new_data = barley_val) %&gt;% \n  ggplot(aes(UMAP1, UMAP2, col = barley)) + \n  geom_point(alpha = 1 / 4) + \n  scale_color_viridis(option = \"viridis\") +\n  coord_obs_pred()\n\n\n\n\n\n\n\nFor a supervised embedding, the target_weight argument is used. A value of zero is unsupervised, and values near 1.0 are completely supervised. As with PLS, the argument for the outcome column is called outcome and can be a string of an unquoted name wrapped in vars().",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-centroids",
    "href": "chapters/embeddings.html#sec-centroids",
    "title": "6  Embeddings",
    "section": "\n6.5 Centroid-Based Methods",
    "text": "6.5 Centroid-Based Methods\nThere are two steps in recipes for this:\n\n\nstep_classdist(): basic “distance to centroid” calculations and,\n\nstep_classdist_shrunken(): nearest shrunken centroids\n\nThese steps are for classification data, so we’ll use some example data from the modeldata package:\n\ntwo_class_dat %&gt;% \n  ggplot(aes(A, B, col = Class)) + \n  geom_point(alpha = 1 / 2) +\n  coord_obs_pred()\n\n\n\n\n\n\n\nHere’s an example of creating a recipe with the basic class distance computations:\n\ncentroid_rec &lt;-\n  recipe(Class ~ ., data = two_class_dat) %&gt;%\n  step_classdist(all_numeric_predictors(), class = \"Class\") %&gt;% \n  prep()\n\nThe outcome argument is called \"class\" and takes a string value for the column name.\nThe processed data has a default naming convention of \"classdist_{class level}\" and you get one column per class:\n\nbake(centroid_rec, new_data = NULL)\n#&gt; # A tibble: 791 × 5\n#&gt;       A     B Class  classdist_Class1 classdist_Class2\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;             &lt;dbl&gt;            &lt;dbl&gt;\n#&gt; 1 2.070 1.632 Class1         -0.05795          -0.5526\n#&gt; 2 2.016 1.037 Class1         -1.026             1.647 \n#&gt; 3 1.689 1.367 Class2         -0.8454            0.2437\n#&gt; 4 3.435 1.980 Class2          1.367             1.678 \n#&gt; 5 2.885 1.976 Class1          0.9208            0.3913\n#&gt; 6 3.314 2.406 Class2          1.708             0.4739\n#&gt; # ℹ 785 more rows\n\nThe shrunken version of this step has an additional argument that is the fraction of the complete solutions. The argument name is threshold:\n\ncentroid_shrunk_rec &lt;-\n  recipe(Class ~ ., data = two_class_dat) %&gt;%\n  step_classdist_shrunken(all_numeric_predictors(), threshold = 1 / 6, class = \"Class\") %&gt;% \n  prep()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html",
    "href": "chapters/interactions-nonlinear.html",
    "title": "7  Interactions and Nonlinear Features",
    "section": "",
    "text": "7.1 Requirements\nYou’ll need 5 packages (aorsf, embed, gt, hstats, and tidymodels) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"aorsf\", \"embed\", \"gt\", \"hstats\", \"tidymodels\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\nlibrary(embed)\ntidymodels_prefer()\ntheme_set(theme_bw())",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-interactions",
    "href": "chapters/interactions-nonlinear.html#sec-interactions",
    "title": "7  Interactions and Nonlinear Features",
    "section": "\n7.2 Interactions",
    "text": "7.2 Interactions\nAs the text mentions, interactions involve two or more predictors (of any data type). An interaction means that the relationship between the outcome and the predictors involved cannot be articulated by looking at one predictor at a time; they act in concert.\nTo get started, let’s once again load the food delivery data and use the same split:\n\ndata(deliveries, package = \"modeldata\")\n\nset.seed(991)\ndelivery_split &lt;- initial_validation_split(deliveries, prop = c(0.6, 0.2), strata = time_to_delivery)\ndelivery_train &lt;- training(delivery_split)\ndelivery_test  &lt;- testing(delivery_split)\ndelivery_val   &lt;- validation(delivery_split)\ndelivery_rs    &lt;- validation_set(delivery_split)\n\n## Some functions to faciltate the content\n\ndec_to_time &lt;- function(x) {\n  mins &lt;- floor(x)\n  dec &lt;- floor((x - mins) * 60)\n  res &lt;- cli::pluralize(\"{mins} minutes{?s} and {dec} second{?s}\")\n  res &lt;- as.character(res)\n  res &lt;- gsub(\" and 0 seconds\", \"\", res)\n  res &lt;- gsub(\"1 minutes\", \"1 minute\", res) \n  res\n}\n\ndec_to_time_rs &lt;- function(x) {\n  collect_metrics(x) %&gt;% \n    filter(.metric == \"mae\") %&gt;% \n    pluck(\"mean\") %&gt;% \n    dec_to_time\n}\n\nWe’ll consider two mechanisms to encode interaction columns: via the base R formula method and with recipes.\n\n7.2.1 Interactions with Model Formulas\nThis was briefly discussed in Section 4.3.\nThe main operator that creates interactions is the colon. Using a:b within a formula will create the appropriate columns that encode the interactions. The specifics depend on what type of data are in columns a and b:\n\nIf both are numeric, an additional column, which is their product, is added to the model matrix. By default, the base R formula method gives it the name \"a:b\".\nIf one is numeric and the other is categorical, R first converts the categorical (i.e., factor) column into binary indicator variables, then makes columns that are the produce of each indicator column and the original numeric column.\nIf both are categorical, indicators are made for both and then their corresponding product pairs are created (binary times binary columns).\n\nLet’s make a small example to demonstrate:\n\nlibrary(gt)\n\ninteraction_example &lt;- \n  delivery_test %&gt;% \n  slice(1, .by = day) %&gt;% \n  select(day, hour, distance) %&gt;% \n  arrange(day)\n\ninteraction_example %&gt;% gt()\n\n\n\n\n\n\nday\nhour\ndistance\n\n\n\nMon\n19.38\n2.59\n\n\nTue\n12.14\n2.40\n\n\nWed\n18.03\n2.96\n\n\nThu\n16.22\n3.25\n\n\nFri\n12.88\n2.88\n\n\nSat\n12.94\n2.47\n\n\nSun\n12.96\n3.80\n\n\n\n\n\n\n\nFor two numeric predictors:\n\nmodel.matrix(~ hour + distance + hour:distance, interaction_example) %&gt;% \n  as_tibble() %&gt;% \n  select(-`(Intercept)`) %&gt;% \n  gt()\n\n\n\n\n\n\nhour\ndistance\nhour:distance\n\n\n\n19.38\n2.59\n50.19\n\n\n12.14\n2.40\n29.14\n\n\n18.03\n2.96\n53.36\n\n\n16.22\n3.25\n52.71\n\n\n12.88\n2.88\n37.09\n\n\n12.94\n2.47\n31.97\n\n\n12.96\n3.80\n49.26\n\n\n\n\n\n\n\nOne numeric and one factor predictor:\n\nmodel.matrix(~ day + distance + day:distance, interaction_example) %&gt;% \n  as_tibble() %&gt;% \n  select(-`(Intercept)`) %&gt;% \n  gt()\n\n\n\n\n\n\ndayTue\ndayWed\ndayThu\ndayFri\ndaySat\ndaySun\ndistance\ndayTue:distance\ndayWed:distance\ndayThu:distance\ndayFri:distance\ndaySat:distance\ndaySun:distance\n\n\n\n0\n0\n0\n0\n0\n0\n2.59\n0.0\n0.00\n0.00\n0.00\n0.00\n0.0\n\n\n1\n0\n0\n0\n0\n0\n2.40\n2.4\n0.00\n0.00\n0.00\n0.00\n0.0\n\n\n0\n1\n0\n0\n0\n0\n2.96\n0.0\n2.96\n0.00\n0.00\n0.00\n0.0\n\n\n0\n0\n1\n0\n0\n0\n3.25\n0.0\n0.00\n3.25\n0.00\n0.00\n0.0\n\n\n0\n0\n0\n1\n0\n0\n2.88\n0.0\n0.00\n0.00\n2.88\n0.00\n0.0\n\n\n0\n0\n0\n0\n1\n0\n2.47\n0.0\n0.00\n0.00\n0.00\n2.47\n0.0\n\n\n0\n0\n0\n0\n0\n1\n3.80\n0.0\n0.00\n0.00\n0.00\n0.00\n3.8\n\n\n\n\n\n\n\nIf you want to make all possible interactions, you can use the dot and exponent operator:\n\n# All possible two-way interactions:\nmodel.matrix(~ (.)^2, interaction_example)\n\n# All possible two- and three-way interactions, etc:\nmodel.matrix(~ (.)^3, interaction_example)\n\n\n7.2.2 Interactions from Recipes\nThe recipes has step_interact(). This step is very atypical since it uses a formula to specify the inputs (rather than a set of dplyr selectors). It also requires all columns used to be already converted to indicators (perhaps using step_dummy()).\nThe formula passed to step_interact() also uses colons to declare interactions, but it has two special differences from base R formulas:\n\nyou can use dplyr selectors to select the columns to interact with and\nthe resulting interaction columns use _x_ as the default seperator in the names.\n\nFor continuous/continuous interactions:\n\nrecipe(~ hour + distance, data = interaction_example) %&gt;% \n  step_interact(~ hour:distance) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL) %&gt;% \n  gt()\n\n\n\n\n\n\nhour\ndistance\nhour_x_distance\n\n\n\n19.38\n2.59\n50.19\n\n\n12.14\n2.40\n29.14\n\n\n18.03\n2.96\n53.36\n\n\n16.22\n3.25\n52.71\n\n\n12.88\n2.88\n37.09\n\n\n12.94\n2.47\n31.97\n\n\n12.96\n3.80\n49.26\n\n\n\n\n\n\n\nFor categorical/continuous combinations, we use step_dummy() first and then a selector to make the interactions\n\nrecipe(~ day + hour, data = interaction_example) %&gt;% \n  step_dummy(all_factor_predictors()) %&gt;% \n  step_interact(~ starts_with(\"day_\"):hour) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL) %&gt;% \n  gt()\n\n\n\n\n\n\nhour\nday_Tue\nday_Wed\nday_Thu\nday_Fri\nday_Sat\nday_Sun\nday_Tue_x_hour\nday_Wed_x_hour\nday_Thu_x_hour\nday_Fri_x_hour\nday_Sat_x_hour\nday_Sun_x_hour\n\n\n\n19.38\n0\n0\n0\n0\n0\n0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n12.14\n1\n0\n0\n0\n0\n0\n12.14\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n18.03\n0\n1\n0\n0\n0\n0\n0.00\n18.03\n0.00\n0.00\n0.00\n0.00\n\n\n16.22\n0\n0\n1\n0\n0\n0\n0.00\n0.00\n16.22\n0.00\n0.00\n0.00\n\n\n12.88\n0\n0\n0\n1\n0\n0\n0.00\n0.00\n0.00\n12.88\n0.00\n0.00\n\n\n12.94\n0\n0\n0\n0\n1\n0\n0.00\n0.00\n0.00\n0.00\n12.94\n0.00\n\n\n12.96\n0\n0\n0\n0\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n12.96\n\n\n\n\n\n\n\n\n7.2.3 Detecting Interactions\nA few different packages can compute \\(H\\)-statistics (and similar quantities):\n\nhstats\n\npre, specifically the interact() and bsnullinteract() functions\n\nbartMachine has interaction_investigator()\n\n\naorsf has orsf_vint()\n\n\nand so on. We’ll focus on the first since the other functions are tied to specific models.\nFor this chapter, we fit an oblique random forest model to compute the \\(H\\)-statistics. We’ll defer the details of that model fit until a later chapter but the code was:\n\nlibrary(aorsf)\n#&gt; Warning: package 'aorsf' was built under R version 4.3.3\n\np &lt;- ncol(delivery_train) - 1\n\nset.seed(1)\norf_fit &lt;- orsf(time_to_delivery ~ ., data = delivery_train, mtry = p, n_tree = 50)\n\nAny ML model can be used with the hstats package. We’ll use the validation set to compute the values; this lends more validity to the values (although we could have used the training set). There are a lot of computations here; this may take a bit to compute:\n\nlibrary(hstats)\n\nset.seed(218)\norf_hstats &lt;-\n  hstats(orf_fit,\n         X = delivery_val %&gt;% dplyr::select(-time_to_delivery),\n         # Prioritize the top 10 individual predictors for computing potential\n         # pairwise interactions.\n         pairwise_m = 10,\n         # We can run a little faster by using quantiles to approximate the \n         # predictor distributions. \n         approx = TRUE,\n         # How many random data points are used for the computations.\n         n_max = 1000,\n         verbose = FALSE)\n\norf_two_way_int_obj &lt;- h2_pairwise(orf_hstats, zero = TRUE)\norf_two_way_int_obj\n#&gt; Pairwise H^2 (normalized)\n#&gt;                       [,1]\n#&gt; hour:day         6.623e-02\n#&gt; item_01:item_10  2.606e-02\n#&gt; day:distance     1.554e-02\n#&gt; hour:distance    4.647e-03\n#&gt; item_10:item_24  4.074e-03\n#&gt; day:item_10      4.043e-03\n#&gt; day:item_24      1.897e-03\n#&gt; day:item_01      1.792e-03\n#&gt; item_07:item_24  1.187e-03\n#&gt; item_01:item_24  1.149e-03\n#&gt; distance:item_24 1.022e-03\n#&gt; item_01:item_07  1.004e-03\n#&gt; item_08:item_10  9.924e-04\n#&gt; item_02:item_03  9.040e-04\n#&gt; distance:item_10 7.654e-04\n#&gt; day:item_02      7.413e-04\n#&gt; item_03:item_07  6.627e-04\n#&gt; item_01:item_08  6.616e-04\n#&gt; day:item_03      6.051e-04\n#&gt; day:item_07      5.829e-04\n#&gt; item_02:item_24  5.552e-04\n#&gt; item_08:item_24  5.536e-04\n#&gt; distance:item_02 5.458e-04\n#&gt; item_07:item_10  5.246e-04\n#&gt; distance:item_03 5.144e-04\n#&gt; distance:item_01 4.824e-04\n#&gt; distance:item_07 4.761e-04\n#&gt; item_03:item_24  4.467e-04\n#&gt; item_03:item_10  3.985e-04\n#&gt; hour:item_10     3.515e-04\n#&gt; hour:item_01     2.623e-04\n#&gt; hour:item_24     2.140e-04\n#&gt; item_07:item_08  2.086e-04\n#&gt; day:item_08      1.894e-04\n#&gt; item_02:item_10  1.800e-04\n#&gt; distance:item_08 1.757e-04\n#&gt; hour:item_08     1.711e-04\n#&gt; item_02:item_07  1.532e-04\n#&gt; hour:item_02     1.489e-04\n#&gt; hour:item_03     1.368e-04\n#&gt; item_03:item_08  1.292e-04\n#&gt; item_01:item_03  9.561e-05\n#&gt; item_01:item_02  8.399e-05\n#&gt; item_02:item_08  5.081e-05\n#&gt; hour:item_07     3.284e-05\n\nFrom these results, we could add more terms by adding another layer of step_interact() to our recipe.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-polynomials",
    "href": "chapters/interactions-nonlinear.html#sec-polynomials",
    "title": "7  Interactions and Nonlinear Features",
    "section": "\n7.3 Polynomial Basis Expansions",
    "text": "7.3 Polynomial Basis Expansions\nThere are two main functions that produce different types of orthogonal polynomials:\n\n\nstep_poly() wrapping stats::poly()\n\n\nstep_poly_bernstein() wrapping splines2::bernsteinPoly().\n\nWe can pass the columns of interest to each step:\n\nrecipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_poly(hour, degree = 4) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL, starts_with(\"hour\"))\n#&gt; # A tibble: 6,004 × 4\n#&gt;   hour_poly_1 hour_poly_2 hour_poly_3 hour_poly_4\n#&gt;         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1   -0.02318    0.02240     -0.01758     0.008000\n#&gt; 2    0.01558    0.01027     -0.003123   -0.01343 \n#&gt; 3    0.01105   -0.0003742   -0.01174    -0.01039 \n#&gt; 4   -0.002365  -0.01319      0.004819    0.01182 \n#&gt; 5   -0.01761    0.006275     0.009360   -0.01725 \n#&gt; 6   -0.02321    0.02249     -0.01777     0.008254\n#&gt; # ℹ 5,998 more rows\n\n# or\n\nrecipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_poly_bernstein(hour, degree = 4) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL, starts_with(\"hour\"))\n#&gt; # A tibble: 6,004 × 4\n#&gt;    hour_1  hour_2   hour_3     hour_4\n#&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 0.2593  0.03588 0.002207 0.00005090\n#&gt; 2 0.01665 0.1209  0.3899   0.4717    \n#&gt; 3 0.05106 0.2201  0.4216   0.3028    \n#&gt; 4 0.2658  0.3742  0.2342   0.05495   \n#&gt; 5 0.4047  0.1437  0.02267  0.001341  \n#&gt; 6 0.2582  0.03549 0.002169 0.00004969\n#&gt; # ℹ 5,998 more rows\n\nIf we would like different polynomial degrees for different columns, we would call the step multiple times.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-splines",
    "href": "chapters/interactions-nonlinear.html#sec-splines",
    "title": "7  Interactions and Nonlinear Features",
    "section": "\n7.4 Spline Functions",
    "text": "7.4 Spline Functions\nrecipes has several spline steps:\n\napropos(\"step_spline\")\n#&gt; [1] \"step_spline_b\"           \"step_spline_convex\"      \"step_spline_monotone\"   \n#&gt; [4] \"step_spline_natural\"     \"step_spline_nonnegative\"\n\nThe syntax is almost identical to the polynomial steps. For example, for natural splines:\n\nrecipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_spline_natural(hour, deg_free = 4) %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL, starts_with(\"hour\"))\n#&gt; # A tibble: 6,004 × 4\n#&gt;   hour_1   hour_2  hour_3  hour_4\n#&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 0.1970 0.004302 0       0      \n#&gt; 2 0      0.06326  0.4307  0.3122 \n#&gt; 3 0      0.2167   0.4816  0.2611 \n#&gt; 4 0.2030 0.6655   0.03618 0.01668\n#&gt; 5 0.3939 0.05003  0       0      \n#&gt; 6 0.1959 0.004225 0       0      \n#&gt; # ℹ 5,998 more rows\n\nSome steps also have arguments for the spline degree called degree. Also, most basis expansion function steps have an option called options to pass additional specifications, such as a vector of knots.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#discretization",
    "href": "chapters/interactions-nonlinear.html#discretization",
    "title": "7  Interactions and Nonlinear Features",
    "section": "\n7.5 Discretization",
    "text": "7.5 Discretization\nThere is one step in recipes that conducts unsupervised binning called step_discretize() and the embed package has two for supervised methods: step_discretize_cart() and step_discretize_xgb().\nFor unsupervised binning:\n\nbasic_binning &lt;- \n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;% \n  step_discretize(hour, num_breaks = 4) %&gt;% \n  prep()\n\nbake(basic_binning, new_data = NULL, hour)\n#&gt; # A tibble: 6,004 × 1\n#&gt;   hour \n#&gt;   &lt;fct&gt;\n#&gt; 1 bin1 \n#&gt; 2 bin4 \n#&gt; 3 bin4 \n#&gt; 4 bin2 \n#&gt; 5 bin1 \n#&gt; 6 bin1 \n#&gt; # ℹ 5,998 more rows\n\nThe hour column has been converted to a factor. There is also an argument called min_unique that defines the “line of dignity” for the binning. If this value is less than the number of data points per bin that would occur for the value of num_breaks, binning is not used.\nTo understand where the breakpoints reside, the tidy() method will show them:\n\ntidy(basic_binning, number = 1)\n#&gt; # A tibble: 5 × 3\n#&gt;   terms   value id              \n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;           \n#&gt; 1 hour  -Inf    discretize_eoInz\n#&gt; 2 hour    14.48 discretize_eoInz\n#&gt; 3 hour    16.57 discretize_eoInz\n#&gt; 4 hour    18.21 discretize_eoInz\n#&gt; 5 hour   Inf    discretize_eoInz\n\n# note: \nquantile(delivery_train$hour, probs = (0:4) / 4)\n#&gt;    0%   25%   50%   75%  100% \n#&gt; 11.07 14.48 16.57 18.21 20.92\n\nThe use of infinite bounds allows data falling outside of the range of the training set to be processed.\nIf you want to use custom breakpoints, recipes::step_cut() is also available.\nFor supervised methods, the syntax is different in two ways. First, the outcome column is specified using the outcome argument. Secondly, additional parameters control the complexity of the results (i.e., the number of bins). These depend on the binning models. for CART binning, the arguments are cost_complexity, tree_depth, and min_n. They are different for the step that uses boosted trees.\nThe format of the results is very similar though:\n\nlibrary(embed)\ncart_binning &lt;-\n  recipe(time_to_delivery ~ ., data = delivery_train) %&gt;%\n  step_discretize_cart(hour,\n                       outcome = vars(time_to_delivery),\n                       cost_complexity = 0.001) %&gt;%\n  prep()\n\nbake(cart_binning, new_data = NULL, hour)\n#&gt; # A tibble: 6,004 × 1\n#&gt;   hour         \n#&gt;   &lt;fct&gt;        \n#&gt; 1 [-Inf,12.28) \n#&gt; 2 [18.97,19.68)\n#&gt; 3 [16.63,18.97)\n#&gt; 4 [15.73,16.24)\n#&gt; 5 [12.28,13.23)\n#&gt; 6 [-Inf,12.28) \n#&gt; # ℹ 5,998 more rows\n\ntidy(cart_binning, number = 1)\n#&gt; # A tibble: 11 × 3\n#&gt;   terms value id                   \n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1 hour  12.28 discretize_cart_ChWJu\n#&gt; 2 hour  13.23 discretize_cart_ChWJu\n#&gt; 3 hour  13.96 discretize_cart_ChWJu\n#&gt; 4 hour  14.77 discretize_cart_ChWJu\n#&gt; 5 hour  15.73 discretize_cart_ChWJu\n#&gt; 6 hour  16.24 discretize_cart_ChWJu\n#&gt; # ℹ 5 more rows\n\nAs you can see, the breakpoints are built into the factor levels.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html",
    "href": "chapters/overfitting.html",
    "title": "8  Overfitting",
    "section": "",
    "text": "8.1 Requirements\nYou’ll need 2 packages (bestNormalize and tidymodels) for this chapter. You can install them via:\nreq_pkg &lt;- c(\"bestNormalize\", \"tidymodels\")\n\n# Check to see if they are installed: \npkg_installed &lt;- vapply(req_pkg, rlang::is_installed, logical(1))\n\n# Install missing packages: \nif ( any(!pkg_installed) ) {\n  install_list &lt;- names(pkg_installed)[!pkg_installed]\n  pak::pak(install_list)\n}\nLet’s load the meta package and manage some between-package function conflicts.\nlibrary(tidymodels)\nlibrary(bestNormalize)\ntidymodels_prefer()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#tuning-paramters",
    "href": "chapters/overfitting.html#tuning-paramters",
    "title": "8  Overfitting",
    "section": "\n8.2 Tuning Paramters",
    "text": "8.2 Tuning Paramters\nThere are currently two main components in a model pipeline:\n\na preprocessing method\na supervised model fit.\n\nIn tidymodels, the type of object that can hold these two components is called a workflow. Each has arguments, many of which are for tuning parameters.\nThere are standardized arguments for most model parameters. For example, regularization in glmnet models and neural networks use the argument name penalty even though the latter model would refer to this as weight decay.\ntidymodels differentiates between two varieties of tuning parameters:\n\nmain arguments are used by most engines for a model type.\nengine arguments represent more niche values specific to a few engines.\n\nLet’s look at an example. In the embeddings chapter, the barley data had a high degree of correlation between the predictors. We discussed PCA, PLS, and other methods to deal with this (via recipe steps). We might try a neural network (say, using the brulee engine) for a model. The code to specify this pipeline would be:\n\npls_rec &lt;-\n  recipe(barley ~ ., data = barley_train) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;%\n  step_pls(all_numeric_predictors(),\n           outcome = \"barley\",\n           num_comp = 20) %&gt;%\n  step_normalize(all_predictors())\n\nnnet_spec &lt;-\n  mlp(\n    hidden_units = 10,\n    activation = \"relu\",\n    penalty = 0.01,\n    epochs = 1000,\n    learn_rate = 0.1\n  ) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"brulee\")\n\npls_nnet_wflow &lt;- workflow(pls_rec, nnet_spec)\n\nWe’ve filled in specific values for each of these arguments, although we don’t know if these are best that we can do.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#marking-parameters-for-optimization",
    "href": "chapters/overfitting.html#marking-parameters-for-optimization",
    "title": "8  Overfitting",
    "section": "\n8.3 Marking Parameters for Optimization",
    "text": "8.3 Marking Parameters for Optimization\nTo tune these parameters, we can give them a value of the function tune(). This special function just returns an expression with the value “tune()”. For example:\n\npls_rec &lt;-\n  recipe(barley ~ ., data = barley_train) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;%\n  step_pls(all_numeric_predictors(),\n           outcome = \"barley\",\n           # For demonstration, we'll use a label\n           num_comp = tune(\"pca comps\")) %&gt;%\n  step_normalize(all_predictors())\n\nnnet_spec &lt;-\n  mlp(\n    hidden_units = tune(),\n    activation = tune(),\n    penalty = tune(),\n    epochs = tune(),\n    learn_rate = tune()\n  ) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"brulee\")\n\npls_nnet_wflow &lt;- workflow(pls_rec, nnet_spec)\n\nOptionally, we can give a label as an argument to the function:\n\nstr(tune(\"#PCA components\"))\n#&gt;  language tune(\"#PCA components\")\n\nThis is useful when the pipeline has two arguments with the same name. For example, if you wanted to use splines for two predictors but allow them to have different degrees of freedom, the resulting set of parameters would not be unique since both of them would have the default label of deg_free. In this case, one recipe step could use tune(“predictor 1 deg free”) and another could be tune(“predictor 2 deg free”).\nEngine arguments are set by set_engine(). For example:\n\nnnet_spec &lt;-\n  mlp(\n    hidden_units = tune(),\n    activation = tune(),\n    penalty = tune(),\n    epochs = tune(),\n    learn_rate = tune()\n  ) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"brulee\", stop_iter = 5, rate_schedule = tune()) \n\npls_nnet_wflow &lt;- workflow(pls_rec, nnet_spec)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#parameter-functions",
    "href": "chapters/overfitting.html#parameter-functions",
    "title": "8  Overfitting",
    "section": "\n8.4 Parameter Functions",
    "text": "8.4 Parameter Functions\nEach tuning parameter has a corresponding function from the dials package containing information on the parameter type, parameter ranges (or possible values), and other data.\nFor example, the function for the penalty argument is:\n\npenalty()\n#&gt; Amount of Regularization (quantitative)\n#&gt; Transformer: log-10 [1e-100, Inf]\n#&gt; Range (transformed scale): [-10, 0]\n\nThis parameter has a default range from 10-10 to 1.0. It also has a corresponding transformation function (log base 10). This means that when values are created, they are uniformly distributed on the log scale. This is common for parameters that have values that span several orders of magnitude and cannot be negative.\nWe can change these defaults via arguments:\n\npenalty(range = c(0, 1), trans = scales::transform_identity())\n#&gt; Amount of Regularization (quantitative)\n#&gt; Transformer: identity [-Inf, Inf]\n#&gt; Range (transformed scale): [0, 1]\n\nIn some cases, we can’t know the range a priori. Parameters like the number of possible PCA components or random forest’s \\(m_{try}\\) depend on the data dimensions. In the case of \\(m_{try}\\) , the default has an unknown in its range:\n\nmtry()\n#&gt; # Randomly Selected Predictors (quantitative)\n#&gt; Range: [1, ?]\n\nWe would need to set this range to use the parameter.\nIn a few situations, the argument name to a recipe step or model function will use a dials function that has a different name than the argument. For example, there are a few different types of “degrees”. There is (real-valued) polynomial exponent degree:\n\ndegree()\n#&gt; Polynomial Degree (quantitative)\n#&gt; Range: [1, 3]\n\n# Data type: \ndegree()$type\n#&gt; [1] \"double\"\n\nbut for the spline recipe steps we need an integer value:\n\n# Data type: \nspline_degree()$type\n#&gt; [1] \"integer\"\n\nIn some cases, tidymodels has methods for automatically changing the parameter function to be used, the range of values, and so on. We’ll see that in a minute.\nThere are also functions to manipulate individual parameters:\n\n# Function list:\napropos(\"^value_\")\n#&gt; [1] \"value_inverse\"   \"value_sample\"    \"value_seq\"       \"value_set\"      \n#&gt; [5] \"value_transform\" \"value_validate\"\n\nvalue_seq(hidden_units(), n = 4)\n#&gt; [1]  1  4  7 10",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sets-of-parameters",
    "href": "chapters/overfitting.html#sets-of-parameters",
    "title": "8  Overfitting",
    "section": "\n8.5 Sets of Parameters",
    "text": "8.5 Sets of Parameters\nFor our pipeline pls_nnet_wflow, we can extract a parameter set that collects all of the parameters and their suggested information. There is a function to do this:\n\npls_nnet_param &lt;- extract_parameter_set_dials(pls_nnet_wflow)\n\nclass(pls_nnet_param)\n#&gt; [1] \"parameters\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnames(pls_nnet_param)\n#&gt; [1] \"name\"         \"id\"           \"source\"       \"component\"    \"component_id\"\n#&gt; [6] \"object\"\n\npls_nnet_param\n#&gt; Collection of 7 parameters for tuning\n#&gt; \n#&gt;     identifier          type    object\n#&gt;   hidden_units  hidden_units nparam[+]\n#&gt;        penalty       penalty nparam[+]\n#&gt;         epochs        epochs nparam[+]\n#&gt;     activation    activation dparam[+]\n#&gt;     learn_rate    learn_rate nparam[+]\n#&gt;  rate_schedule rate_schedule dparam[+]\n#&gt;      pca comps      num_comp nparam[+]\n\nThe difference in the type and identifier columns only occurs when the tune() value has a label (as with the final row).\nThe output ”nparam[+]” indicates a numeric parameter, and the plus indicates that it is fully specified. If our pipeline had used \\(m_{try}\\), that value would show \"nparam[?]”. The rate schedule is a qualitative parameter and has a label of \"dparam[+]\" (“d” for discrete).\nLet’s look at the information for the learning rate parameter by viewing the parameter information set by tidymodels. It is different than the default:\n\npls_nnet_param %&gt;% \n  filter(id == \"learn_rate\") %&gt;% \n  pluck(\"object\")\n#&gt; [[1]]\n#&gt; Learning Rate (quantitative)\n#&gt; Transformer: log-10 [1e-100, Inf]\n#&gt; Range (transformed scale): [-3, -0.5]\n\n# The defaults: \nlearn_rate()\n#&gt; Learning Rate (quantitative)\n#&gt; Transformer: log-10 [1e-100, Inf]\n#&gt; Range (transformed scale): [-10, -1]\n\nWhy are they different? The main function has a wider range since it can be used by boosted trees, neural networks, UMAP, and other tools. The range is more narrow for this pipeline since we know that neural networks tend to work better with faster learning rates (so we set a different default).\nSuppose we want to change the range to be even more narrow. We can use the update() function to change defaults or to use a different dials parameter function:\n\nnew_rate &lt;- \n  pls_nnet_param %&gt;% \n  update(learn_rate = learn_rate(c(-2, -1/2)))\n\nnew_rate %&gt;% \n  filter(id == \"learn_rate\") %&gt;% \n  pluck(\"object\")\n#&gt; [[1]]\n#&gt; Learning Rate (quantitative)\n#&gt; Transformer: log-10 [1e-100, Inf]\n#&gt; Range (transformed scale): [-2, -0.5]\n\nYou don’t always have to extract or modify a parameter set; this is an optional tool in case you want to change default values.\nThe parameter set is sometimes passed as an argument to grid creation functions or to iterative optimization functions that need to simulate/sample random candidates. For example, to create a random grid with 4 candidate values:\n\nset.seed(220)\ngrid_random(pls_nnet_param, size = 4)\n#&gt; # A tibble: 4 × 7\n#&gt;   hidden_units   penalty epochs activation learn_rate rate_schedule `pca comps`\n#&gt;          &lt;int&gt;     &lt;dbl&gt;  &lt;int&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;               &lt;int&gt;\n#&gt; 1            9 2.080e-10    170 elu          0.002654 step                    2\n#&gt; 2            3 1.883e- 3    112 tanh         0.002299 decay_time              4\n#&gt; 3            6 3.417e- 2    362 elu          0.1435   none                    4\n#&gt; 4            6 1.593e- 9    444 tanh         0.001125 decay_time              4",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  }
]