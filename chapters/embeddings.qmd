---
knitr:
  opts_chunk:
    cache.path: "../_cache/embeddings/"
---

# Embeddings {#sec-embeddings}

```{r}
#| label: embeddings-knitr-setup
#| include: false

knitr::opts_chunk$set(
    comment = "#>",
    collapse = TRUE,
    fig.align = 'center',
    fig.width = 10,
    fig.height = 6,
    out.width = "95%",
    dev = 'svg',
    dev.args = list(bg = "transparent"),
    tidy = FALSE,
    echo = TRUE
  )

options(digits = 4, width = 84)
options(dplyr.print_min = 6, dplyr.print_max = 6)
options(cli.width = 85)
options(crayon.enabled = FALSE)
options(pillar.advice = FALSE, pillar.min_title_chars = Inf, pillar.sigfig = 4)

source("../R/_common.R")
req_pkg <- c("bestNormalize", "dimRed", "embed", "fastICA", "igraph", 
             "mixOmics", "modeldatatoo", "patchwork", "RANN", "RSpectra", 
             "tidymodels", "uwot", "viridis")

req_pkg_fmt <- purrr::map_chr(req_pkg, ~ pkg_chr(.x))
```

The [corresponding chapter](http://aml4td/chapters/embeddings.html) on the main site focuses on finding ways to combine or distill a set of features into a smaller set that captures important information.  Like the previous chapters, this. one will also focus on the `r pkg(recipes)` package. 

## Requirements

You’ll need `r length(req_pkg)` packages (`r req_pkg_fmt`) for this chapter. The `r pkg(mixOmics)` is a Bioconductor package and is not on CRAN. For the others, we can install them as usual but we'll get `r pkg(mixOmics)` from GitHub:

```{r}
#| label: embeddings-installs
#| eval: false
#| echo: true
req_pkg <- c("bestNormalize", "dimRed", "embed", "fastICA", "igraph", 
             "mixOmics", "modeldatatoo", "patchwork", "RANN", "RSpectra", 
             "tidymodels", "uwot", "viridis")

# Check to see if they are installed: 
pkg_installed <- vapply(req_pkg, rlang::is_installed, logical(1))

# Install missing packages: 
if ( any(!pkg_installed) ) {
  install_list <- names(pkg_installed)[!pkg_installed]
  
  # mixOmics is not on CRAN
  cran_install_list <- install_list[install_list != "mixOmics"]
  if ( length(cran_install_list) > 0 ) {
    pak::pak(cran_install_list)
  }
  
  # Get mixOmics from github
  if ( "mixOmics" %in% install_list ) {
    pak::pak("mixOmicsTeam/mixOmics")
  }
}
```

Let's load the meta package and manage some between-package function conflicts. 

```{r}
#| label: start-tidymodels
#| results: hide
#| message: false
#| warning: false
library(tidymodels)
library(viridis)
library(embed) # for umap
library(patchwork)

tidymodels_prefer()
theme_set(theme_bw())
```

## Example: Predicting Barley Amounts  {#sec-barley}

The data are in the `r pkg(modeldatatoo)` package. Let's load the data, remove two outcome columns that will not be analyzed here, and conduct a three-way split of the data: 

```{r}
#| label: barley-data-import
source("https://raw.githubusercontent.com/aml4td/website/main/R/setup_chemometrics.R")
```

The column names for the predictors are `wvlgth_001` through `wvlgth_550`. 

The primary recipe used for almost all of the embedding methods is:

```{r}
#| label: barley-recipe

library(bestNormalize) # for ORD transformation

barley_rec <-
  recipe(barley ~ ., data = barley_train) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  # Pre-compute to save time later
  prep()

barley_rec
```

If you use a recipe, most of the embedding methods can be computed with a common interface. The recipe step functions are mostly in the `r pkg(recipes)` package, although some live in "side packages," such as the `r pkg(embed)` package. We’ll be clear about which package is needed for each. 

`r back("embeddings.html#sec-barley")`

## Linear Transformations  {#sec-linear-embed}

We'll look at the three _linear_ methods described in the text. 

### Principal Component Analysis {#sec-pca}

Unsurprisingly, the recipe step needed here is called `step_pca()`. We’ll add an `id` argument to more easily reference the step of interest. 

```{r}
#| label: pca-prep
barley_pca_rec <-
  barley_rec %>%
  step_pca(all_numeric_predictors(), num_comp = 2, id = "pca") %>% 
  prep()

barley_pca_rec
```

To further investigate the results, the `tidy()` method can extract elements of the computations. For example, you can return how variance each component captures using the argument `type = "variance"`. Note that when the PCA recipe step was added, we used the option `id = "pca"`. This is not required, but it makes it easier to specify what step the `tidy()` method should consider: 

```{r}
#| label: pca-scree-data
pca_scree <- tidy(barley_pca_rec, id = "pca", type = "variance")
pca_scree

pca_scree %>% count(terms)
```

Note that there are 550 entries for each since there are 550 predictor columns. 

The default option for the `tidy()` method with PCA is to return the estimated loadings. This can help untangle which predictors influence the PCA components the most (or least). 

```{r}
#| label: pca-loadings
pca_loadings <- tidy(barley_pca_rec, id = "pca")
pca_loadings
```

There are `550^2 = 302500` possible loadings. 

To get the component values for new data, such as the validation set, the `bake()` method can be used. Using `new_data = NULL` returns the training set points:

```{r}
#| label: pca-scores
barley_pca_rec %>% 
  bake(new_data = NULL, starts_with("PC"))
```

Since we used `num_comp = 2`, two new features were generated. 

We can also pass new data in, such as the validation set: 

```{r}
#| label: pca-score-plot
#| fig-width: 5
#| fig-height: 4
#| fig-align: "center"
#| out-width: "60%"

pca_score_plot <- 
  barley_pca_rec %>% 
  bake(new_data = barley_val) %>% 
  ggplot(aes(PC1, PC2, col = barley)) + 
  geom_point(alpha = 1 / 4) + 
  scale_color_viridis(option = "viridis")

pca_score_plot
```

Note the difference in the axis ranges. If we are considering how much the PCA components explain the original predictors (i.e., not the outcome), it can be very helpful to keep the axis scales common: 

```{r}
#| label: pca-scores-equal
#| fig-width: 5
#| fig-height: 4
#| fig-align: "center"
#| out-width: "60%"

pca_score_plot + coord_obs_pred()
```

This helps avoid over-interpreting proportionally small patterns in the later components. 

::: {.callout-note}
As mentioned in the main text, PCA (and PLS) components are unique up to their sign. This means that the embedded features have the same shape, but their values may be flipped in the North/South and/or East/West directions. 
:::

The functions `embed::step_pca_sparse()` and `embed::step_pca_sparse_bayes()` have sparse/regularized estimation methods for PCA. Each has an argument called `predictor_prop()` that attempts to control how much sparsity should be used. `predictor_prop = 0` should approximate regular PCA, and values near 1.0 would produce very few non-zero loadings. 

`r back("embeddings.html#sec-pca")`

### Independent Component Analysis {#sec-ica}

An ICA recipe step can also be found in the `r pkg(recipes)` package. The syntax is virtually identical: 

```{r}
#| label: ica-prep-barley
set.seed(538)
barley_ica_rec <-
  recipe(barley ~ ., data = barley_train) %>% 
  step_ica(all_numeric_predictors(), num_comp = 2, id = "ica") %>% 
  prep()
```

Similarly, the `tidy()` method returns the ICA loadings: 

```{r}
#| label: tidy-ica-barley
tidy(barley_ica_rec, id = "ica")
```

Most other dimension reduction techniques (but not PCA and PLS) depend on random numbers. We’ll set them when needed, but it is worth pointing out that you will likely get different results each time you run them. 

For example, when two ICA components are used, the results are not the same but close when using a different random number seed. 

```{r}
#| label: ica-redo
set.seed(955)
ica_redo <- 
  recipe(barley ~ ., data = barley_train) %>% 
  step_ica(all_numeric_predictors(), num_comp = 2, id = "ica") %>% 
  prep()

ica_redo %>% tidy(id = "ica")
```

The individual loading values are different between runs, and components one and two are swapped between invocations with different seeds: 

```{r}
#| label: ica-scores
#| fig-width: 10
#| fig-height: 4
#| fig-align: "center"
#| out-width: "100%"

ica_1 <- 
  barley_ica_rec %>% 
  bake(new_data = barley_val) %>% 
  ggplot(aes(IC1, IC2, col = barley)) + 
  geom_point(alpha = 1 / 4, show.legend = FALSE) + 
  scale_color_viridis(option = "viridis") +
  coord_obs_pred() +
  labs(title = "seed = 538")

ica_2 <- 
  ica_redo %>% 
  bake(new_data = barley_val) %>% 
  ggplot(aes(IC1, IC2, col = barley)) + 
  geom_point(alpha = 1 / 4) + 
  scale_color_viridis(option = "viridis") +
  coord_obs_pred() +
  labs(title = "seed = 955")

ica_1 + ica_2
```

This might not cause a difference in performance when the features are used in a predictive model, but if the model uses slopes and intercepts, the parameter estimates will be different each time it is run. 

`r back("embeddings.html#sec-ica")`

### Partial Least Squares {#sec-pls}

The syntax for PLS is also very similar. However, it is a supervised method, so we need to specify the column containing the outcome (the outcome column is not needed after model training). The code below uses `dplyr::vars()` to declare the column name, but a simple character string can also be used. 

```{r}
#| label: pls-prep
barley_pls_rec <-
  barley_rec %>%
  step_pls(all_numeric_predictors(), outcome = vars(barley), num_comp = 2,
           id = "pls") %>% 
  prep()

# Loadings: 
tidy(barley_pls_rec, id = "pls")
```

`r back("embeddings.html#sec-pls")`

## Multidimensional Scaling {#sec-mds}

tidymodels contains recipe steps for Isomap and UMAP. The latter is accessible via the `r pkg(embed)` package. 

### Isomap  {#sec-isomap}

Again, the syntax is very similar to the previous unsupervised methods. The main two tuning parameters are `num_terms` and `neighbors `. We should also set the seed before execution. 

```{r}
#| label: isomap-prep
#| cache: true
#| message: false
#| warning: false

set.seed(221)
barley_isomap_rec <-
  barley_rec %>%
  step_isomap(all_numeric_predictors(), neighbors = 10, num_terms = 2) %>% 
  prep()
```

We can project this preprocessing model onto new data: 

```{r}
#| label: isomap-scores
#| fig-width: 5
#| fig-height: 4
#| fig-align: "center"
#| out-width: "60%"

barley_isomap_rec %>% 
  bake(new_data = barley_val) %>% 
  ggplot(aes(Isomap1, Isomap2, col = barley)) + 
  geom_point(alpha = 1 / 4) + 
  scale_color_viridis(option = "viridis") +
  coord_obs_pred()
```

`r back("embeddings.html#sec-isomap")`

### UMAP {#sec-umap}

`step_umap()`, in the `r pkg(embed)` package, has a number of tuning parameters: `neighbors`, `num_comp`, `min_dist`, `learn_rate`, `epochs`, `initial` (initialization method, e.g. "pca"), and the optional `target_weight`. 

For an unsupervised embedding: 

```{r}
#| label: umap-prep
set.seed(724)
barley_umap_rec <-
  barley_rec %>%
  step_umap(all_numeric_predictors(), neighbors = 10, num_comp = 2) %>% 
  prep()
```

Projection on new data has the same syntax: 

```{r}
#| label: umap-scores
#| fig-width: 5
#| fig-height: 4
#| fig-align: "center"
#| out-width: "60%"

barley_umap_rec %>% 
  bake(new_data = barley_val) %>% 
  ggplot(aes(UMAP1, UMAP2, col = barley)) + 
  geom_point(alpha = 1 / 4) + 
  scale_color_viridis(option = "viridis") +
  coord_obs_pred()
```

For a _supervised_ embedding, the `target_weight` argument is used. A value of zero is unsupervised, and values near 1.0 are completely supervised. As with PLS, the argument for the outcome column is called `outcome` and can be a string of an unquoted name wrapped in `vars()`. 

`r back("embeddings.html#sec-umap")`

## Centroid-Based Methods  {#sec-centroids}

There are two steps in recipes for this: 

 - `step_classdist()`: basic "distance to centroid" calculations and,
 - `step_classdist_shrunken()`: nearest shrunken centroids
 
These steps are for classification data, so we'll use some example data from the `r pkg(modeldata)` package: 

```{r}
#| label: two-class-data-plot
#| fig-width: 5
#| fig-height: 4
#| fig-align: "center"
#| out-width: "60%"
two_class_dat %>% 
  ggplot(aes(A, B, col = Class)) + 
  geom_point(alpha = 1 / 2) +
  coord_obs_pred()
```

Here's an example of creating a recipe with the basic class distance computations:

```{r}
#| label: basic-cent
centroid_rec <-
  recipe(Class ~ ., data = two_class_dat) %>%
  step_classdist(all_numeric_predictors(), class = "Class") %>% 
  prep()
```

The outcome argument is called `"class"` and takes a string value for the column name. 

The processed data has a default naming convention of `"classdist_{class level}"` and you get one column per class: 

```{r}
#| label: basic-cent-pred
bake(centroid_rec, new_data = NULL)
```

The shrunken version of this step has an additional argument that is the fraction of the complete solutions. The argument name is `threshold`: 

```{r}
#| label: shrunk-cent
centroid_shrunk_rec <-
  recipe(Class ~ ., data = two_class_dat) %>%
  step_classdist_shrunken(all_numeric_predictors(), threshold = 1 / 6, class = "Class") %>% 
  prep()
```

`r back("embeddings.html#sec-centroids")`
