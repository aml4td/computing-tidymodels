---
knitr:
  opts_chunk:
    cache.path: "../_cache/embeddings/"
---





# Embeddings {#sec-embeddings}






```{r}
#| label: embeddings-knitr-setup
#| include: false

knitr::opts_chunk$set(
    comment = "#>",
    collapse = TRUE,
    fig.align = 'center',
    fig.path = "../figures/",
    fig.width = 10,
    fig.height = 6,
    out.width = "95%",
    dev = 'svg',
    dev.args = list(bg = "transparent"),
    tidy = FALSE,
    echo = TRUE
  )

options(digits = 4, width = 84)
options(dplyr.print_min = 6, dplyr.print_max = 6)
options(cli.width = 85)
options(crayon.enabled = FALSE)
options(pillar.advice = FALSE, pillar.min_title_chars = Inf, pillar.sigfig = 4)

source("../R/_common.R")
req_pkg <- c("bestNormalize", "dimRed", "embed", "fastICA", "igraph", 
             "mixOmics", "modeldatatoo", "patchwork", "RANN", "RSpectra", 
             "tidymodels", "uwot", "viridis")


req_pkg_fmt <- purrr::map_chr(req_pkg, ~ pkg_chr(.x))
```






## Requirements

You’ll need `r length(req_pkg)` packages (`r req_pkg_fmt`) for this chapter. The `r pkg(mixOmics)` is a Bioconductor package and is not on CRAN. For the others, we can install them as usual but we'll get `r pkg(mixOmics)` from GitHub:





```{r}
#| label: embeddings-installs
#| eval: false
#| echo: true
req_pkg <- c("bestNormalize", "dimRed", "embed", "fastICA", "igraph", 
             "mixOmics", "modeldatatoo", "patchwork", "RANN", "RSpectra", 
             "tidymodels", "uwot", "viridis")

# Check to see if they are installed: 
pkg_installed <- vapply(req_pkg, rlang::is_installed, logical(1))

# Install missing packages: 
if ( any(!pkg_installed) ) {
  install_list <- names(pkg_installed)[!pkg_installed]
  
  # mixOmics is not on CRAN
  cran_install_list <- install_list[install_list != "mixOmics"]
  if ( length(cran_install_list) > 0 ) {
    pak::pak(cran_install_list)
  }
  
  # Get mixOmics from github
  if ( "mixOmics" %in% install_list ) {
    pak::pak("mixOmicsTeam/mixOmics")
  }
}
```





Let's load the meta package and manage some between-package function conflicts. 





```{r}
#| label: start-tidymodels
#| results: hide
#| message: false
#| warning: false
library(tidymodels)
library(viridis)
library(embed) # for umap
library(patchwork)

tidymodels_prefer()
theme_set(theme_bw())
```





## Example: Predicting Barley Amounts  {#sec-barley}

The data are contained in the `r pkg(modeldatatoo)` package. Let's load the data, remove two outcome columns that will not be analyzed here, and conduct a three-way split of the data: 





```{r}
library(modeldatatoo)

chimiometrie_2019 <-
  data_chimiometrie_2019()  %>%
  select(-soy_oil, -lucerne)

set.seed(101)
barley_split <-
  initial_validation_split(chimiometrie_2019,
                           prop = c(0.7, 0.15),
                           strata = barley)
barley_train <- training(barley_split)
barley_val   <- validation(barley_split)
barley_test  <- testing(barley_split)
```





The column names for the predictors are `wvlgth_001` through `wvlgth_550`. 

The primary recipe used for almost all of the embedding methods is:





```{r}
#| label: barley-recipe

library(bestNormalize) # for ORD transformation

barley_rec <-
  recipe(barley ~ ., data = barley_train) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  # Pre-compute to save time later
  prep()

barley_rec
```






Most of the embedding methods can be computed with a common interface if you use a recipe. The recipe step functions are mostly in the `r pkg(recipes)` package although some live in “side packages” such as the `r pkg(embed)` package. We’ll be clear about which package is needed for each. 

## Linear Transformations  {#sec-linear-embed}

We'll look at the three _linear_ methods described in the text. 

### Principal Component Analysis






```{r}
#| label: pca-prep
barley_pca_rec <-
  barley_rec %>%
  step_pca(all_numeric_predictors(), num_comp = 2, id = "pca") %>% 
  prep()

barley_pca_rec
```





To interrogate the results more, the `tidy()` method can extract elements of the computations. For example, you can return how variance each component captures using the argument `type = "variance"`. Note that, when the PCA recipe step was added, we used the option `id = “pca”`. This is not required, but it makes it easier to specify what step that the `tidy()` method should consider: 





```{r}
#| label: pca-scree-data
pca_scree <- tidy(barley_pca_rec, id = "pca", type = "variance")
pca_scree

pca_scree %>% count(terms)
```





Note that there are 550 entries for each since there are 550 predictor columns. 

The default option for the `tidy()` method with PCA is to return the estimated loadings. This can help untangle which predictors are influencing the PCA components the most (or least). 





```{r}
#| label: pca-loadings
pca_loadings <- tidy(barley_pca_rec, id = "pca")
pca_loadings
```





There are `550^2 = 302500` possible loadings. 

To get the component values for new data, such as the validation set, the `bake()` method can be used. Using `new_data = NULL` returns the training set points:





```{r}
#| label: pca-scores
barley_pca_rec %>% 
  bake(new_data = NULL, starts_with("PC"))
```





Since we used `num_comp = 2`, there are 2 new features that are generated. 

We can also pass new data in, such as the validation set: 





```{r}
#| label: fig-pca-scores
#| fig-width: 5
#| fig-height: 4
#| fig-align: "center"
#| out-width: "60%"

pca_score_plot <- 
  barley_pca_rec %>% 
  bake(new_data = barley_val) %>% 
  ggplot(aes(PC1, PC2, col = barley)) + 
  geom_point(alpha = 1 / 4) + 
  scale_color_viridis(option = "viridis")

pca_score_plot
```





Note the difference in the axis ranges. If we are considering how much the PCA components explain the original predictors (i.e., not the outcome), it can be very helpful to keep the axis scales common: 





```{r}
#| label: fig-pca-scores-equal
#| fig-width: 5
#| fig-height: 4
#| fig-align: "center"
#| out-width: "60%"

pca_score_plot + coord_obs_pred()
```





This helps avoid over-interpreting proportionally small patterns in the later components. 

The functions `embed::step_pca_sparse()` and `embed::step_pca_sparse_bayes()` have sparse/regularized estimation methods for PCA. Each has an argument called `predictor_prop()` that attempts to control how much sparsity should be used. `predictor_prop = 0` should approximate regular PCA and values near 1.0 would produce very few non-zero loadings. 

### Independent Component Analysis

An ICA recipe step can also be found in the `r pkg(recipes)` package. The syntax is virtually identical: 





```{r}
set.seed(538)
barley_ica_rec <-
  recipe(barley ~ ., data = barley_train) %>% 
  step_ica(all_numeric_predictors(), num_comp = 2, id = "ica") %>% 
  prep()
```





Similarly, the `tidy()` method returns the ICA loadings: 





```{r}
tidy(barley_ica_rec, id = "ica")
```





Most other dimension reduction techniques (but not PCA and PLS) depend on random numbers. We’ll set them when needed, but it is worth pointing out that you are likely to get different results each time you run them. 

For example, when two ICA components are used, the results are not the same but close when using a different random number seed. 





```{r}
set.seed(955)
ica_redo <- 
  recipe(barley ~ ., data = barley_train) %>% 
  step_ica(all_numeric_predictors(), num_comp = 2, id = "ica") %>% 
  prep()

ica_redo %>% tidy(id = "ica")
```





The individual loads are different, and components one and two are swapped between invokations with different seeds: 





```{r}
#| label: fig-ica-scores
#| fig-width: 10
#| fig-height: 4
#| fig-align: "center"
#| out-width: "100%"

ica_1 <- 
  barley_ica_rec %>% 
  bake(new_data = barley_val) %>% 
  ggplot(aes(IC1, IC2, col = barley)) + 
  geom_point(alpha = 1 / 4, show.legend = FALSE) + 
  scale_color_viridis(option = "viridis") +
  coord_obs_pred() +
  labs(title = "seed = 538")

ica_2 <- 
  ica_redo %>% 
  bake(new_data = barley_val) %>% 
  ggplot(aes(IC1, IC2, col = barley)) + 
  geom_point(alpha = 1 / 4) + 
  scale_color_viridis(option = "viridis") +
  coord_obs_pred() +
  labs(title = "seed = 955")

ica_1 + ica_2
```





This might not cause a difference in performance when the features are used in a predictive model but if the model uses slopes and intercepts, the values would not be different each time it is run. 

### Partial Least Squares {#numeric-pls}

The syntax for PLS is also very similar. However, it is a supervised method so we need to specify the column containing the outcome (the outcome column is not needed after model training). The code below uses `dplyr::vars()` to declare the column name but a simple character string can also be used. 





```{r}
#| label: pls-prep
barley_pls_rec <-
  barley_rec %>%
  step_pls(all_numeric_predictors(), outcome = vars(barley), num_comp = 2,
           id = "pls") %>% 
  prep()

# Loadings: 
tidy(barley_pls_rec, id = "pls")
```





## Multidimensional Scaling {#sec-mds}

tidymodels contains recipe steps for Isomap and UMAP. The latter is accecable via the `r pkg(embed)` package. 

### Isomap  {#sec-isomap}

Again, the syntax is very similar to the previous unsupervised methods. The main two tuning parameters are `num_terms` and `neighbors `. We should also set the seed before execution. 





```{r}
#| label: isomap-prep
#| cache: true
#| message: false
#| warning: false

set.seed(221)
barley_isomap_rec <-
  barley_rec %>%
  step_isomap(all_numeric_predictors(), neighbors = 10, num_terms = 2) %>% 
  prep()
```





We can project this preprocessing model onto new data: 





```{r}
#| label: fig-isomap-scores
#| fig-width: 5
#| fig-height: 4
#| fig-align: "center"
#| out-width: "60%"

barley_isomap_rec %>% 
  bake(new_data = barley_val) %>% 
  ggplot(aes(Isomap1, Isomap2, col = barley)) + 
  geom_point(alpha = 1 / 4) + 
  scale_color_viridis(option = "viridis") +
  coord_obs_pred()
```





### UMAP {#sec-umap}

`step_umap()`, in the `r pkg(embed)` package, has a number of tuning parameters: `neighbors`, `num_comp`, `min_dist`, `learn_rate`, `epochs`, `initial` (initialization method, e.g. “pca”), and the optional `target_weight`. 

For an unsupervised embedding: 





```{r}
#| label: umap-prep
set.seed(724)
barley_umap_rec <-
  barley_rec %>%
  step_umap(all_numeric_predictors(), neighbors = 10, num_comp = 2) %>% 
  prep()
```





Projection on new data is the same: 





```{r}
#| label: fig-umap-scores
#| fig-width: 5
#| fig-height: 4
#| fig-align: "center"
#| out-width: "60%"

barley_umap_rec %>% 
  bake(new_data = barley_val) %>% 
  ggplot(aes(UMAP1, UMAP2, col = barley)) + 
  geom_point(alpha = 1 / 4) + 
  scale_color_viridis(option = "viridis") +
  coord_obs_pred()
```





For a _supervised_ embedding, the `target_weight` argument is used. A value of zero is unsupervised and values near 1.0 are completely supervised. As with PLS, the argument for the outcome column is called `outcome` and can be a string of an unquoted name wrapped in `vars()`. 

## Centroid-Based Methods  {#sec-centroids}


