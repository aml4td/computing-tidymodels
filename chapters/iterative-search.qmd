---
knitr:
  opts_chunk:
    cache.path: "../_cache/iterative/"
---

# Iterative Search {#sec-iterative-search}

```{r}
#| label: iterative-knitr-setup
#| include: false

knitr::opts_chunk$set(
    comment = "#>",
    collapse = TRUE,
    fig.align = 'center',
    fig.path = "../figures/",
    fig.width = 10,
    fig.height = 6,
    out.width = "95%",
    dev = 'svg',
    dev.args = list(bg = "transparent"),
    tidy = FALSE,
    echo = TRUE
  )

options(digits = 4, width = 84)
options(dplyr.print_min = 6, dplyr.print_max = 6)
options(cli.width = 85)
options(crayon.enabled = FALSE)
options(pillar.advice = FALSE, pillar.min_title_chars = Inf, pillar.sigfig = 4)

source("../R/_common.R")
req_pkg <- c("finetune", "future", "GA", "GauPro", "probably", "tidymodels", "xgboost")

req_pkg_fmt <- purrr::map_chr(req_pkg, ~ pkg_chr(.x))
```

## Requirements

You’ll need `r length(req_pkg)` packages (`r req_pkg_fmt`) for this chapter. The `r pkg(mixOmics)` is a Bioconductor package and is not on CRAN. For the others, we can install them as usual but we'll get `r pkg(mixOmics)` from GitHub:

```{r}
#| label: iterative-installs
#| eval: false
#| echo: true
req_pkg <- c("finetune", "future", "GA", "GauPro", "probably", "tidymodels", "xgboost")

# Check to see if they are installed: 
pkg_installed <- vapply(req_pkg, rlang::is_installed, logical(1))

# Install missing packages: 
if ( any(!pkg_installed) ) {
  install_list <- names(pkg_installed)[!pkg_installed]
  pak::pak(install_list)
}
```


```{r}
#| label: sshhh-startup
#| include: false

library(GA)
library(tidymodels)
library(finetune)
library(future)
library(probably)
```

```{r}
#| label: startup

library(GA)
library(tidymodels)
library(finetune)
library(probably)
library(future)


tidymodels_prefer()
theme_set(theme_bw())
plan("multisession")
```

To reduce the complexity of the example, we’ll use a simulated classification data set containing numeric predictors. We’ll simulate 1,000 samples using a simulation system, the details of which can be found in the [`r pkg(modeldata)` documentation](https://modeldata.tidymodels.org/reference/sim_classification.html#details). There are linear, nonlinear, and interacting features in the data set, and the classes are fairly balanced. We’ll use a 3:1 split for training and testing as well as 10-fold cross-validation:

```{r}
#| label: get-sim-data

set.seed(2783)
sim_dat <- sim_classification(1000)

set.seed(101)
sim_split <- initial_split(sim_dat)
sim_train <- training(sim_split)
sim_test <- testing(sim_split)
sim_rs <- vfold_cv(sim_train)
```

We’ll tune a boosted classification model using the `r pkg(xgboost)` package, described in a later chapter. We tune multiple parameters and set an additional parameter, `validation`, to be zero. This is used when early stopping, which we will not use:


```{r}
#| label: sim-spec

bst_spec <-
  boost_tree(
    mtry = tune(),
    tree_depth = tune(),
    trees = tune(),
    learn_rate = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune()
  ) %>%
  set_engine('xgboost', validation = 0) %>%
  set_mode('classification')
```

Tree-based models require little to no preprocessing so we will use a simple R formula to define the roles of variables: 

```{r}
#| label: sim-worflow
bst_wflow <- workflow(class ~ ., bst_spec)
```

From the workflow, we create a _parameters_ object and set the ranges for two parameters. `mtry` requires an upper bound to be set since it is dependent on the number of model terms in the data set. We’ll need parameter information since most iterative methods need to know the possible ranges as well as the type of parameter (e.g., integer, character, etc.) and/or any transformations of the values. 

```{r}
#| label: sim-param
bst_param <-
  bst_wflow %>%
  extract_parameter_set_dials() %>%
  update(
    mtry = mtry(c(3, 15)),
    trees = trees(c(10, 500))
  )
```

We can now fit and/or tune, models. We’ll declare what metrics should be collected and then create a small space-filling design that is used as the starting point for simulated annealing and Bayesian optimization. We _could_ let the system make these initial values for us, but we’ll create them now so that we can reuse the results and have a common starting place. 

```{r}
#| label: initial
#| cache: true

cls_mtr <- metric_set(brier_class, roc_auc)

init_grid <- grid_space_filling(bst_param, size = 6)

set.seed(21)
initial_res <-
  bst_wflow %>%
  tune_grid(
    resamples = sim_rs,
    grid = init_grid,
    metrics = cls_mtr,
    control = control_grid(save_pred = TRUE)
  )
```

From these six candidates, the smallest Brier score was `r round(show_best(initial_res, metric = "brier_class", n = 1)$mean, 3)`, a mediocre value: 

```{r}
#| label: initial-res
show_best(initial_res, metric = "brier_class") %>% 
  select(-.estimator, -.config, -.metric) %>% 
  relocate(mean)
```

We will show how to use three iterative search methods. 

## Simulated Annealing  {#sec-sim-anneal}

The finetune package contains `finetune::tune_sim_anneal()` that can incrementally search the parameter space in a non-greedy way. Its syntax is very similar to `tune_grid()` with two additional arguments of note: 

- `initial`: Either: 
	- An integer that declares how many points in a space-filling design should be created and evaluated before proceeding. 
	- An object from a previous run of `tune_grid()` or one of the other `tune_*()` functions. 
- `iter`: An integer for the maximum number of search iterations. 

Also of note is `control_sim_anneal()`, which helps save additional results and controls logging, and if restarts or early stopping should be used. 

One important note: the first metric in the metric set guides the optimization. All of the other metric values are recorded for each iteration but only one is used to improve the model fit.  

Here's some example code: 

```{r}
#| label: s-sim-anneal
#| cache: true

set.seed(381)
sa_res <-
  bst_wflow %>%
  tune_sim_anneal(
    resamples = sim_rs,
    param_info = bst_param,
    metrics = cls_mtr,
    # Additional options:
    initial = initial_res,
    iter = 50,
    # Prevent early stopping, save out-of-sample predictions, 
    # and log the process to the console: 
    control = control_sim_anneal(
      no_improve = Inf,
      verbose_iter = TRUE,
      save_pred = TRUE
    )
  )
```  

The Brier score has been reduced from the intial value of `r round(show_best(initial_res, metric = "brier_class", n = 1)$mean, 3)` to a new best of `r round(show_best(sa_res, metric = "brier_class", n = 1)$mean, 3)`. We'll estimate:

```{r}
#| label: sa-intervals
#| cache: true
show_best(sa_res, metric = "brier_class") %>% 
  select(-.estimator, -.config, -.metric) %>% 
  relocate(mean)
```

There are several ways to use `autoplot()` to investigate the results. The default methods plots the metric(s) versus the parameters. Here is it for just the Brier score: 

```{r}
#| label: sa-profile
#| fig-width: 7
#| fig-height: 6
#| out-width: 65%
autoplot(sa_res, metric = "brier_class")
```

Next, we can see how the parameter values change over the search by adding `type = "parameters"`: 

```{r}
#| label: sa-history
#| fig-width: 7
#| fig-height: 6
#| out-width: 65%
autoplot(sa_res, metric = "brier_class", type = "parameters")
```

Finally, a plot of performance metrics can be used via `type = "performance"`: 

```{r}
#| label: sa-performance
#| fig-width: 6
#| fig-height: 3.75
#| out-width: 65%
autoplot(sa_res, metric = "brier_class", type = "performance")
```

If we had used `control_sim_anneal(save_worflow = TRUE)`, we could use `fit_best()` to determine the candidate with the best metric value then fit that model to the training set. 

## Genetic Algorithms  {#sec-genetic-algo}

tidymodels has no API or function for optimizing models using genetic algorithms. However, there is unsupported code below to do this as long as the tuning parameters are all numeric. We’ll using the `r pkg(GA)` package for the computations, and this will require: 

- The upper and lower bounds of the parameters
- Code to transform the parameter values (if needed)
- A means to resample/evaluate a model on an out-of-sample data set. 
- A method to compute a single performance metric such that larger values are more desirable.

To get started, let’s work with the parameter object named `bst_param`. We can use `purrr::map_dbl()` to get vectors of the minimum and maximum values. These should be in the transformed space (if needed):


```{r}
#| label: ga-prep
min_vals <- map_dbl(bst_param$object, ~ .x$range[[1]])
max_vals <- map_dbl(bst_param$object, ~ .x$range[[2]])
```

The remainder of the tasks should occur within the GA's processing. This function shows code with comments to help understand: 

```{r}
#| label: ga-fitness

yardstick_fitness <- function(values, wflow, param_info, metrics, ...) {

  # load reqired packages if run in parallel
	require(tidymodels)

	info <- as_tibble(metrics)
	
	# Check to see if there are any qualitative parameters and stop if so.
	qual_check <- map_lgl(param_info$object, ~ inherits(.x, "qual_param"))
	if (any(qual_check)) {
	  cli::cli_abort("The function only works for quantitative tuning parameters.")
	}

	# Back-transform parameters if they use a transformation (inputs are in 
	# transformed scales)
	values <- purrr::map2_dbl(
		values,
		param_info$object,
		~ dials::value_inverse(.y, .x)
	)
	
	# Convert integer parameters to integers
	is_int <- map_lgl(param_info$object, ~ .x$type == "integer")
	int_param <- param_info$id[is_int]
	for (i in int_param) {
	  ind <- which(param_info$id == i)
	  values[[ind]] <- floor(values[[ind]])
	}
	
	# Convert from vector to a tibble
	values <- matrix(values, nrow = 1)
	colnames(values) <- param_info$id
	values <- as_tibble(values)
	
	# We'll run _populations_ within a generation in parallel. Let's make
	# sure to turn off parallelization of resamples here: 
	ctrl <- control_grid(allow_par = FALSE)

	# Resample / validate metrics
	res <- tune_grid(
		wflow,
		metrics = metrics,
		param_info = param_info,
		grid = values,
		control = ctrl,
		...
	)

	# Fitness is to be maximized so change direction if needed
	best_res <- show_best(res, metric = info$metric[1])
	if (info$direction[1] == "minimize") {
		obj_value <- -best_res$mean
	} else {
		obj_value <- best_res$mean
	}
	obj_value
}
```

Now, let's initialize the search using a space-filling design (with 10 candidates per population): 

```{r}
#| label: ga-initial-pop
pop_size <- 10
grid_ga <- grid_space_filling(bst_param, size = pop_size, original = FALSE)
grid_ga$learn_rate <- log10(grid_ga$learn_rate)
grid_ga$loss_reduction <- log10(grid_ga$loss_reduction)
```

Now we can run `GA::ga()` to begin the process: 

```{r}
#| label: ga-execute
#| cache: true

set.seed(158)
ga_res <-
  ga(
    # GA options:
    type = "real-valued",
    fitness = yardstick_fitness,
    lower = min_vals,
    upper = max_vals,
    popSize = pop_size,
    suggestions = as.matrix(grid_ga),
    maxiter = 25,
    # Save the best solutions at each iteration
    keepBest = TRUE,
    seed = 39,
    # Run _populations_ within a generation in parallel
    parallel = "multicore",
    # Now options to pass to the `...` in yardstick_fitness()
    wflow = bst_wflow,
    param_info = bst_param,
    metrics = cls_mtr,
    resamples = sim_rs
  )
```

Here is a plot of the best results per population and the mean result (both are Brier scores): 

```{r}
#| label: ga-profile
#| fig-width: 6
#| fig-height: 3.75
#| out-width: 65%

# Negate the fitness value since Brier score should be minimized.
-attr(ga_res,"summary") %>% 
  as_tibble() %>% 
  mutate(iteration = row_number()) %>% 
  select(best = max, mean = mean, iteration) %>% 
  pivot_longer(c(best, mean), names_to = "summary", values_to = "fitness") %>% 
  ggplot(aes(iteration, fitness, col = summary, pch = summary)) + 
  geom_point() + 
  labs(x = "Iteration", y = "Brier Score (CV)")
```

The best results are in a slot called `solution`. Let's remap that to the original parameter values: 

```{r}
#| label: ga-best

ga_best <- 
  # There could be multiple solutions for the same fitness; we take the first. 
  ga_res@solution[1,] %>% 
  # Back-transform
  map2(bst_param$object, ~ value_inverse(.y, .x)) %>% 
  as_tibble() %>% 
  set_names(bst_param$id) %>% 
  # Attach fitness and coerce to integer if needed.
  mutate(
    mtry = floor(mtry),
    trees = floor(trees),
    min_n = floor(min_n),
    tree_depth = floor(tree_depth),
    brier = -ga_res@fitnessValue
  ) %>% 
  relocate(brier)

ga_best
```


## Sidebar: Bayesian Models {#sec-bayes}

Bayes rules, rstanarm. brms


## Bayesian Optimization {#sec-bayes-opt}


There are other packages that use Bayesian optimization: 

- [`r pkg(BayesGP)`](https://cran.r-project.org/package=BayesGP)
- [`r pkg(BayesGPfit)`](https://cran.r-project.org/package=BayesGPfit)
- [`r pkg(FastGP)`](https://cran.r-project.org/package=FastGP)
- [`r pkg(GauPro)`](https://github.com/CollinErickson/GauPro)
- [`r pkg(GPBayes)`](https://github.com/pulongma/GPBayes)
- [`r pkg(GPfit)`](https://cran.r-project.org/package=GPfit)

and many others. The book [_Gaussian process modeling, design and optimization for the applied sciences_](https://bookdown.org/rbg/surrogates/) also contains descriptions of many other GO packages. 

Currently, tidymodels uses `r pkg(GPfit)` but is moving to `r pkg(GauPro)`.

The `r pkg(tune)` package contains `tune_bayes()` for Bayesian optimization. The syntax is identical to what we've already seen with `tune_sim_anneal()`. 


```{r}
#| label: sa-bayes
#| cache: true
set.seed(221)
bo_res <- bst_wflow %>%
  tune_bayes(
    resamples = sim_rs,
    param_info = bst_param,
    metrics = cls_mtr,
    # These options work as before: 
    initial = initial_res,
    iter = 50,
    control = control_bayes(
      no_improve = Inf,
      verbose_iter = TRUE,
      save_pred = TRUE,
    )
  )
```

The same helper functions are used to interogate the results and to create diagnostic plots: 

```{r}
#| label: bo-intervals
#| cache: true
show_best(bo_res, metric = "brier_class") %>% 
  select(-.estimator, -.config, -.metric) %>% 
  relocate(mean)
```

```{r}
#| label: bo-profile
#| fig-width: 7
#| fig-height: 6
#| out-width: 65%
autoplot(bo_res, metric = "brier_class")
```

```{r}
#| label: bo-history
#| fig-width: 7
#| fig-height: 6
#| out-width: 65%
autoplot(bo_res, metric = "brier_class", type = "parameters")
```

```{r}
#| label: bo-performance
#| fig-width: 6
#| fig-height: 3.75
#| out-width: 65%
autoplot(bo_res, metric = "brier_class", type = "performance")
```

There are other packages that use Bayesian optimization: 

- [`r pkg(mlr3mbo)`](https://mlr3mbo.mlr-org.com/)
- [`r pkg(mlrMBO)`](https://mlrmbo.mlr-org.com/)
- [`r pkg(ParBayesianOptimization)`](https://github.com/AnotherSamWilson/ParBayesianOptimization)
- [`r pkg(rBayesianOptimization)`](https://github.com/yanyachen/rBayesianOptimization)
